{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CMU-Multimodal SDK\n",
    "\n",
    "Credits: https://github.com/Justin1904/CMU-MultimodalSDK-Tutorials\n",
    "\n",
    "This file uses ***CMU-Multimodal SDK*** to load and process multimodal time-series dataset CMU-MOSEI.\n",
    "\n",
    "We specify some constants in `./constans/paths.py`. Please first take a look and modify the paths to point to the correct folders.\n",
    "\n",
    "## Downloading the data\n",
    "\n",
    "We start off by (down)loading the datasets. In the SDK each dataset has three sets of content: `highlevel`, `raw` and `labels`. `highlevel` contains the extracted features for each modality (e.g OpenFace facial landmarks, openSMILE acoustic features) while `raw` contains the raw transctripts, phonemes. `labels` are self-explanatory. Note that some datasets have more than just one set of annotations so `labels` could also give you multiple files.\n",
    "\n",
    "Currently there's a caveat that the SDK will not automatically detect if you have downloaded the data already. In event of that it will throw a `RuntimeError`. We work around that by `try/except`. This is not ideal but it will work for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from constants import SDK_PATH, DATA_PATH_HIGH_LEVEL, DATA_PATH_LABELS, ALIGNED_DATA_PATH_HIGH_LEVEL, ALIGNED_DATA_PATH_LABELS, WORD_EMB_PATH, CACHE_PATH\n",
    "import sys\n",
    "\n",
    "if SDK_PATH is None:\n",
    "    print(\"SDK path is not specified! Please specify first in constants/paths.py\")\n",
    "    exit(0)\n",
    "else:\n",
    "    sys.path.append(SDK_PATH)\n",
    "\n",
    "if not os.path.exists(SDK_PATH):\n",
    "    print(\"Check the relative address of SDK in the constants/paths.py, current address is: \", SDK_PATH)\n",
    "\n",
    "if not os.path.exists(DATA_PATH_HIGH_LEVEL):\n",
    "    print(\"Check the relative address of high level features of the data in the constants/paths.py, current address is: \", DATA_PATH_HIGH_LEVEL)\n",
    "\n",
    "if not os.path.exists(DATA_PATH_LABELS):\n",
    "    print(\"Check the relative address of labels of the high level features in the constants/paths.py, current address is: \", DATA_PATH_LABELS)\n",
    "\n",
    "if not os.path.exists(ALIGNED_DATA_PATH_HIGH_LEVEL):\n",
    "    print(\"Check the relative address of aligned high level features of the data in the constants/paths.py, current address is: \", ALIGNED_DATA_PATH_HIGH_LEVEL)\n",
    "\n",
    "if not os.path.exists(ALIGNED_DATA_PATH_LABELS):\n",
    "    print(\"Check the relative address of aligned labels of the high level features in the constants/paths.py, current address is: \", ALIGNED_DATA_PATH_LABELS)\n",
    "\n",
    "import mmsdk\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from mmsdk import mmdatasdk as md\n",
    "from subprocess import check_call, CalledProcessError\n",
    "\n",
    "# create folders for storing the data\n",
    "#if not os.path.exists(DATA_PATH):\n",
    "#    check_call(' '.join(['mkdir', '-p', DATA_PATH]), shell=True)\n",
    "\n",
    "# download highlevel features, low-level (raw) data and labels for the dataset MOSEI\n",
    "# if the files are already present, instead of downloading it you just load it yourself.\n",
    "# here we use CMU_MOSEI dataset as example.\n",
    "\n",
    "DATASET = md.cmu_mosei\n",
    "\n",
    "#try:\n",
    "#    md.mmdataset(DATASET.highlevel, DATA_PATH_HIGH_LEVEL)\n",
    "#except RuntimeError:\n",
    "#    print(\"High-level features have been downloaded previously.\")\n",
    "\n",
    "#try:\n",
    "#    md.mmdataset(DATASET.raw, DATA_PATH)\n",
    "#except RuntimeError:\n",
    "#    print(\"Raw data have been downloaded previously.\")\n",
    "    \n",
    "#try:\n",
    "#    md.mmdataset(DATASET.labels, DATA_PATH_LABELS)\n",
    "#except RuntimeError:\n",
    "#    print(\"Labels have been downloaded previously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the downloaded files\n",
    "\n",
    "We can print the files in the target data folder to see what files are there.\n",
    "\n",
    "We can observe a bunch of files ending with `.csd` extension. This stands for ***computational sequences***, which is the underlying data structure for all features in the SDK. We will come back to that later when we load the data. For now we just print out what computational sequences we have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "COAVAREP.csd\nFACET 4.2.csd\nglove_vectors.csd\nOpenFace_2.0.csd\nOpenSMILE.csd\nAll Labels.csd\n"
    }
   ],
   "source": [
    "# list the directory contents... let's see what features there are\n",
    "data_files = os.listdir(ALIGNED_DATA_PATH_HIGH_LEVEL)\n",
    "print('\\n'.join(data_files))\n",
    "data_files = os.listdir(ALIGNED_DATA_PATH_LABELS)\n",
    "print('\\n'.join(data_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a multimodal dataset\n",
    "\n",
    "Loading the dataset is as simple as telling the SDK what are the features you need and where are their computational sequences. You can construct a dictionary with format `{feature_name: csd_path}` and feed it to `mmdataset` object in the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2020-10-01 11:38:21.449] | Success | Computational sequence read from file ..\\Dataset\\AlignedData\\high_level\\glove_vectors.csd ...\n[2020-10-01 11:38:57.050] | Status  | Checking the integrity of the &lt;glove_vectors&gt; computational sequence ...\n[2020-10-01 11:38:57.052] | Status  | Checking the format of the data in &lt;glove_vectors&gt; computational sequence ...\n[2020-10-01 11:39:41.689] | Success | &lt;glove_vectors&gt; computational sequence data in correct format.\n[2020-10-01 11:39:41.694] | Status  | Checking the format of the metadata in &lt;glove_vectors&gt; computational sequence ...\n[2020-10-01 11:39:41.701] | Success | &lt;glove_vectors&gt; computational sequence metadata in correct format.\n[2020-10-01 11:39:41.707] | Success | &lt;glove_vectors&gt; computational sequence is valid!\n[2020-10-01 11:39:41.754] | Success | Computational sequence read from file ..\\Dataset\\AlignedData\\high_level\\FACET 4.2.csd ...\n[2020-10-01 11:40:04.730] | Status  | Checking the integrity of the &lt;FACET 4.2&gt; computational sequence ...\n[2020-10-01 11:40:04.733] | Status  | Checking the format of the data in &lt;FACET 4.2&gt; computational sequence ...\n[2020-10-01 11:40:43.578] | Success | &lt;FACET 4.2&gt; computational sequence data in correct format.\n[2020-10-01 11:40:43.587] | Status  | Checking the format of the metadata in &lt;FACET 4.2&gt; computational sequence ...\n[2020-10-01 11:40:43.593] | Success | &lt;FACET 4.2&gt; computational sequence metadata in correct format.\n[2020-10-01 11:40:43.599] | Success | &lt;FACET 4.2&gt; computational sequence is valid!\n[2020-10-01 11:40:43.633] | Success | Computational sequence read from file ..\\Dataset\\AlignedData\\high_level\\COAVAREP.csd ...\n[2020-10-01 11:42:48.404] | Status  | Checking the integrity of the &lt;COAVAREP&gt; computational sequence ...\n[2020-10-01 11:42:48.406] | Status  | Checking the format of the data in &lt;COAVAREP&gt; computational sequence ...\n[2020-10-01 11:45:14.775] | Success | &lt;COAVAREP&gt; computational sequence data in correct format.\n[2020-10-01 11:45:14.786] | Status  | Checking the format of the metadata in &lt;COAVAREP&gt; computational sequence ...\n[2020-10-01 11:45:14.793] | Success | &lt;COAVAREP&gt; computational sequence metadata in correct format.\n[2020-10-01 11:45:14.797] | Success | &lt;COAVAREP&gt; computational sequence is valid!\n[2020-10-01 11:45:14.809] | Success | Dataset initialized successfully ... \n"
    }
   ],
   "source": [
    "# define your different modalities - refer to the filenames of the CSD files\n",
    "#visual_field = 'CMU_MOSEI_VisualFacet42'\n",
    "#acoustic_field = 'CMU_MOSEI_COVAREP'\n",
    "#text_field = 'CMU_MOSEI_TimestampedWordVectors'\n",
    "\n",
    "# We have use different names for the files, make sure the names are same for your version of the data\n",
    "visual_field = 'FACET 4.2'\n",
    "acoustic_field = 'COAVAREP'\n",
    "text_field = 'glove_vectors'\n",
    "\n",
    "features = [\n",
    "    text_field, \n",
    "    visual_field, \n",
    "    acoustic_field\n",
    "]\n",
    "\n",
    "# Use the line below if you have just downloaded the high level unaligned features and comment out the line below that\n",
    "# recipe = {feat: os.path.join(DATA_PATH_HIGH_LEVEL, feat) + '.csd' for feat in features}\n",
    "\n",
    "recipe = {feat: os.path.join(ALIGNED_DATA_PATH_HIGH_LEVEL, feat) + '.csd' for feat in features}\n",
    "dataset = md.mmdataset(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A peek into the dataset\n",
    "\n",
    "The multimodal dataset, after loaded, has the following hierarchy:\n",
    "\n",
    "\n",
    "```\n",
    "            computational_sequence_1 ---...\n",
    "           /                                   ...\n",
    "          /                                    /\n",
    "         /                          first_video     features -- T X N array\n",
    "        /                          /               /\n",
    "dataset ---computational_sequence_2 -- second_video\n",
    "        \\                          \\               \\\n",
    "         \\                          third_video     intervals -- T X 2 array\n",
    "          \\                                    \\...\n",
    "           \\\n",
    "            computational_sequence_3 ---...\n",
    "```\n",
    "\n",
    "It looks like a nested dictionary and can be indexed as if it is a nested dictionary. A dataset contains multiple computational sequences whose key is the `text_field`, `visual_field`, `acoustic_field` as defined above. Each computational sequence, however, has multiple video IDs in it, and different computational sequences are supposed to have the same set of video IDs. Within each video, there are two arrays: `features` and `intervals`, denoting the feature values at each time step and the start and end timestamp for each step. We can take a look at its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[&#39;glove_vectors&#39;, &#39;FACET 4.2&#39;, &#39;COAVAREP&#39;]\n================================================================================\n[&#39;--qXJuDtHPw[0]&#39;, &#39;-3g5yACwYnA[0]&#39;, &#39;-3g5yACwYnA[1]&#39;, &#39;-3g5yACwYnA[2]&#39;, &#39;-3g5yACwYnA[3]&#39;, &#39;-3g5yACwYnA[4]&#39;, &#39;-3g5yACwYnA[5]&#39;, &#39;-3nNcZdcdvU[0]&#39;, &#39;-3nNcZdcdvU[1]&#39;, &#39;-3nNcZdcdvU[2]&#39;]\n================================================================================\n[&#39;features&#39;, &#39;intervals&#39;]\n================================================================================\nIntervals\n[455, 2]\n[40, 2]\n[1425, 2]\n================================================================================\nFeatures\n[455, 35]\n[40, 300]\n[1425, 74]\nDifferent modalities have different number of time steps!\n"
    }
   ],
   "source": [
    "print(list(dataset.keys()))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(list(dataset[visual_field].keys())[:10])\n",
    "print(\"=\" * 80)\n",
    "\n",
    "some_id = list(dataset[visual_field].keys())[15]\n",
    "print(list(dataset[visual_field][some_id].keys()))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print('Intervals')\n",
    "print(list(dataset[visual_field][some_id]['intervals'].shape))\n",
    "print(list(dataset[text_field][some_id]['intervals'].shape))\n",
    "print(list(dataset[acoustic_field][some_id]['intervals'].shape))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print('Features')\n",
    "print(list(dataset[visual_field][some_id]['features'].shape))\n",
    "print(list(dataset[text_field][some_id]['features'].shape))\n",
    "print(list(dataset[acoustic_field][some_id]['features'].shape))\n",
    "print(\"Different modalities have different number of time steps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment of multimodal time series\n",
    "\n",
    "To work with multimodal time series that contains multiple views of data with different frequencies, we have to first align them to a ***pivot*** modality. The convention is to align to ***words***. Alignment groups feature vectors from other modalities into bins denoted by the timestamps of the pivot modality, and apply a certain processing function to each bin. We call this function ***collapse function***, because usually it is a pooling function that collapses multiple feature vectors from another modality into one single vector. This will give you sequences of same lengths in each modality (as the length of the pivot modality) for all videos.\n",
    "\n",
    "Here we define our collapse funtion as simple averaging. We feed the function to the SDK when we invoke `align` method. Note that the SDK always expect collapse functions with two arguments: `intervals` and `features`. Even if you don't use intervals (as is in the case below) you still need to define your function in the following way.\n",
    "\n",
    "***Note: Currently the SDK applies the collapse function to all modalities including the pivot, and obviously text modality cannot be \"averaged\", causing some errors. My solution is to define the avg function such that it averages the features when it can, and return the content as is when it cannot average.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-425c77c905ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# we define a simple averaging function that does not depend on intervals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintervals\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Don't run this and the next cell if you have already aligned the data\n",
    "\n",
    "# we define a simple averaging function that does not depend on intervals\n",
    "def avg(intervals: np.array, features: np.array) -> np.array:\n",
    "    try:\n",
    "        return np.average(features, axis=0)\n",
    "    except:\n",
    "        return features\n",
    "\n",
    "# first we align to words with averaging, collapse_function receives a list of functions\n",
    "dataset.align(text_field, collapse_functions=[avg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_files={x:x for x in dataset.keys()}\n",
    "dataset.deploy(\"hl1\",deploy_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append annotations to the dataset and get the data points\n",
    "\n",
    "Now that we have a preprocessed dataset, all we need to do is to apply annotations to the data. Annotations are also computational sequences, since they are also just some values distributed on different time spans (e.g 1-3s is 'angry', 12-26s is 'neutral'). Hence, we just add the label computational sequence to the dataset and then align to the labels. Since we (may) want to preserve the whole sequences, this time we don't specify any collapse functions when aligning. \n",
    "\n",
    "Note that after alignment, the keys in the dataset changes from `video_id` to `video_id[segment_no]`, because alignment will segment each datapoint based on the segmentation of the pivot modality (in this case, it is segmented based on labels, which is what we need, and yes, one code block ago they are segmented to word level, which I didn't show you).\n",
    "\n",
    "***Important: DO NOT add the labels together at the beginning, the labels will be segmented during the first alignment to words. This also holds for any situation where you want to do multiple levels of alignment.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2020-10-01 11:52:51.752] | Success | Computational sequence read from file ..\\Dataset\\AlignedData\\labels\\All Labels.csd ...\n[2020-10-01 11:52:57.090] | Status  | Checking the integrity of the &lt;All Labels&gt; computational sequence ...\n[2020-10-01 11:52:57.093] | Status  | Checking the format of the data in &lt;All Labels&gt; computational sequence ...\n[2020-10-01 11:53:20.466] | Success | &lt;All Labels&gt; computational sequence data in correct format.\n[2020-10-01 11:53:20.470] | Status  | Checking the format of the metadata in &lt;All Labels&gt; computational sequence ...\n[2020-10-01 11:53:20.474] | Success | &lt;All Labels&gt; computational sequence metadata in correct format.\n[2020-10-01 11:53:20.484] | Success | &lt;All Labels&gt; computational sequence is valid!\n"
    }
   ],
   "source": [
    "#label_field = 'CMU_MOSEI_Labels'\n",
    "label_field = 'All Labels'\n",
    "\n",
    "# we add and align to lables to obtain labeled segments\n",
    "# this time we don't apply collapse functions so that the temporal sequences are preserved\n",
    "label_recipe = {label_field: os.path.join(ALIGNED_DATA_PATH_LABELS, label_field + '.csd')}\n",
    "dataset.add_computational_sequences(label_recipe, destination=None)\n",
    "\n",
    "# Uncomment the line below if you haven't aligned the labels \n",
    "#dataset.align(label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you just aligned the data with the labels and now you want to store the alined data\n",
    "\n",
    "deploy_files={x:x for x in dataset.keys()}\n",
    "dataset.deploy(\"fl1\",deploy_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-HwX2H8Z4hY[4]\n"
    }
   ],
   "source": [
    "# check out what the keys look like now\n",
    "print(list(dataset[text_field].keys())[55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset\n",
    "\n",
    "Now it comes to our final step: splitting the dataset into train/dev/test splits. This code block is a bit long in itself, so be patience and step through carefully with the explanatory comments.\n",
    "\n",
    "The SDK provides the splits in terms of video IDs (which video belong to which split), however, after alignment our dataset keys already changed from `video_id` to `video_id[segment_no]`. Hence, we need to extract the video ID when looping through the data to determine which split each data point belongs to.\n",
    "\n",
    "In the following data processing, I also include instance-wise Z-normalization (subtract by mean and divide by standard dev) and converted words to unique IDs.\n",
    "\n",
    "This example is based on PyTorch so I am using PyTorch related utils, but the same procedure should be easy to adapt to other frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[&#39;7l3BNtSE0xc&#39;, &#39;dZFV0lyedX4&#39;, &#39;286943&#39;, &#39;126872&#39;, &#39;qgC8_emxSIU&#39;, &#39;kld9r0iFkWM&#39;, &#39;rC29Qub0U7A&#39;, &#39;4YfyP0uIqw0&#39;, &#39;FMenDv3y8jc&#39;, &#39;4wLP4elp1uM&#39;, &#39;KYQTwFVBzME&#39;, &#39;27v7Blr0vjw&#39;, &#39;DnBHq5I52LM&#39;, &#39;HR18U0yAlTc&#39;, &#39;x266rUJQC_8&#39;, &#39;d1CDP6sMuLA&#39;, &#39;xSCvspXYU9k&#39;, &#39;4EDblUpJieU&#39;, &#39;4o4ilPK9rl8&#39;, &#39;53609&#39;, &#39;SZ7HK5ns6mE&#39;, &#39;243981&#39;, &#39;ySblgk7T7eQ&#39;, &#39;MYEyQUpMe3k&#39;, &#39;EujJ0SwiCRE&#39;, &#39;3HyAaqre_Fk&#39;, &#39;iQDB_OkAQWs&#39;, &#39;gE7kUqMqQ9g&#39;, &#39;eFV7iFPYZB4&#39;, &#39;IRSxo_XXArg&#39;, &#39;3hOlJf_JQDs&#39;, &#39;BRSyH6yfDLk&#39;, &#39;1jogeKX0wGw&#39;, &#39;3At-BKm9eYk&#39;, &#39;NVLPURuAVLU&#39;, &#39;pZye4zFzk3o&#39;, &#39;l1jW3OMXUzs&#39;, &#39;XKyumlBmix8&#39;, &#39;eKQKEi2-0Ws&#39;, &#39;WgI8IbJtXHw&#39;, &#39;tnWmVXZ87h0&#39;, &#39;YCEllKyaCrc&#39;, &#39;W1CWpktWtTs&#39;, &#39;8wQhzezNcUY&#39;, &#39;0bxhZ-LIfZY&#39;, &#39;lrjm6F3JJgg&#39;, &#39;Vdf1McvE9ao&#39;, &#39;eQc5uI7FKCU&#39;, &#39;2QXHdu2zlQY&#39;, &#39;YCI-ZzclIPQ&#39;, &#39;2Ky9DBSl49w&#39;, &#39;SKTyBOhDX6U&#39;, &#39;b86B3hP8ARM&#39;, &#39;23656&#39;, &#39;kpS4BXif_Sw&#39;, &#39;dR68gbeOWOc&#39;, &#39;tC2KicUHB9Q&#39;, &#39;absh1hsZeF0&#39;, &#39;c5zxqITn3ZM&#39;, &#39;uogwnZGb-iE&#39;, &#39;46495&#39;, &#39;Sq6DIhFxPqQ&#39;, &#39;PexNiFbPTYM&#39;, &#39;z441aDJvAcU&#39;, &#39;OORklkFql3k&#39;, &#39;WbtsuXkaGeg&#39;, &#39;grsV1YN1z5s&#39;, &#39;Gc_zIjqqUys&#39;, &#39;424SXFTCFsA&#39;, &#39;P17tYiqMGRU&#39;, &#39;UweZVaFqruU&#39;, &#39;mzAu5gxjE-w&#39;, &#39;8TDAP0KNIIw&#39;, &#39;u9ZV8jb_-U0&#39;, &#39;iPPp6MCythU&#39;, &#39;lwL4hjhkid4&#39;, &#39;102389&#39;, &#39;frCWtiam4tE&#39;, &#39;pSxte-ms0t8&#39;, &#39;c9hE1ghElrM&#39;, &#39;WfNiQBXmPw8&#39;, &#39;_q7DM8WkzAQ&#39;, &#39;257534&#39;, &#39;fU5AYkq0m9k&#39;, &#39;q17gSr9kNww&#39;, &#39;AgH84SNRx5s&#39;, &#39;206585&#39;, &#39;yzCHa2qchpg&#39;, &#39;GmpDbIstUdc&#39;, &#39;eREud0qYR3s&#39;, &#39;NoOt0oU843M&#39;, &#39;svsbGQn389o&#39;, &#39;ZsLrKF7_Oos&#39;, &#39;Kyz32PTyP4I&#39;, &#39;7idU7rR77Ss&#39;, &#39;8lfS97s2AKc&#39;, &#39;X2Hs89fZ2-c&#39;, &#39;5vwXp27bCLw&#39;, &#39;tZDNinnrGf8&#39;, &#39;KB5hSnV1emg&#39;, &#39;TxRtSItpGMo&#39;, &#39;eJfT7-dDqzA&#39;, &#39;x2n19Cn96aw&#39;, &#39;XDVit9ASVUg&#39;, &#39;6brtMLkjjYk&#39;, &#39;-rxZxtG0xmY&#39;, &#39;JATMzuV6sUE&#39;, &#39;LueQT0vY1zI&#39;, &#39;267466&#39;, &#39;m-7yRWZLwLY&#39;, &#39;OWWHjP3pX9o&#39;, &#39;QnYlpSeVOYo&#39;, &#39;V7OFSHYcQD0&#39;, &#39;GK-Pprzh0t0&#39;, &#39;yLo-Jl8nBXU&#39;, &#39;200941&#39;, &#39;61531&#39;, &#39;ezuWKsxPRSM&#39;, &#39;ehZrOdw6PhA&#39;, &#39;-6rXp3zJ3kc&#39;, &#39;Z4iYSMMMycQ&#39;, &#39;MtIklGnIMGo&#39;, &#39;116213&#39;, &#39;3XShFTBsp_Q&#39;, &#39;YQZPnHZRp1w&#39;, &#39;fsd1qPLA3kY&#39;, &#39;208322&#39;, &#39;uVM4JWjfGgQ&#39;, &#39;QWll4lS1qqI&#39;, &#39;Vlas-mPydcg&#39;, &#39;teQqaAmqqx0&#39;, &#39;AQ4Pktv4-Gc&#39;, &#39;yCpHmPSshKY&#39;, &#39;pDRdCSIyjkA&#39;, &#39;sIusv36VoBY&#39;, &#39;8jY_RhOS89o&#39;, &#39;GKsjv42t284&#39;, &#39;HA2AiTz-qxc&#39;, &#39;GXIfrEUJ5d4&#39;, &#39;0Fqav67TDEw&#39;, &#39;10219&#39;, &#39;wHeZHLv9wGI&#39;, &#39;qDfSYz0PX9g&#39;, &#39;180971&#39;, &#39;qBanrqkzobg&#39;, &#39;NgENhhXf0LE&#39;, &#39;SH0OYx3fR7s&#39;, &#39;lc5bSoGlQwY&#39;, &#39;XrNaL-MTjXg&#39;, &#39;8i7u3fl-hP8&#39;, &#39;N-NnCI6U52c&#39;, &#39;r46amqjpWgg&#39;, &#39;QVyxySAaehE&#39;, &#39;JKueLneBoik&#39;, &#39;110565&#39;, &#39;TqQgIxR7thU&#39;, &#39;267694&#39;, &#39;ZKErPftd--w&#39;, &#39;GMa0cIAltnw&#39;, &#39;MSHBEntSDjU&#39;, &#39;AB1PbMaW03s&#39;, &#39;oBS-IW-BO00&#39;, &#39;5fKPJPFqPho&#39;, &#39;8NPaDkOiXw4&#39;, &#39;104741&#39;, &#39;2ItiGjefTRA&#39;, &#39;LFOwCSiGOvw&#39;, &#39;YLK58srjhNI&#39;, &#39;233939&#39;, &#39;5xa0Ac2krGs&#39;, &#39;CbRexsp1HKw&#39;, &#39;112425&#39;, &#39;bCBMKwafZKY&#39;, &#39;_0efYOjQYRc&#39;, &#39;O2ShYliS3CU&#39;, &#39;Oa2xVjzAMFc&#39;, &#39;bnzVR2ETQQ8&#39;, &#39;yBtMwyQFXwA&#39;, &#39;TtAyUQtmTLk&#39;, &#39;yXE_XZUb8qE&#39;, &#39;3wHE78v9zr4&#39;, &#39;cml9rShionM&#39;, &#39;rePYTlT5_AY&#39;, &#39;9TAGpMywQyE&#39;, &#39;ryE9VBiR3p8&#39;, &#39;238063&#39;, &#39;NlrCjfHELLE&#39;, &#39;oGFDE-6nd7Q&#39;, &#39;bWmrrWQOVCM&#39;, &#39;29751&#39;, &#39;11UtTaDYgII&#39;, &#39;jZe-2w7pkd8&#39;, &#39;275267&#39;, &#39;tymso_pAxhk&#39;, &#39;PcqKFt1p3UQ&#39;, &#39;sfaWfZ2-4c0&#39;, &#39;kLAXmTx2xOA&#39;, &#39;3OYY5Tsz_2k&#39;, &#39;wk5lFk5kFjY&#39;, &#39;hE-sA5umuCk&#39;, &#39;3IUVpwx23cY&#39;, &#39;92291&#39;, &#39;102213&#39;, &#39;236442&#39;, &#39;nbru7qLot04&#39;, &#39;zhNksSReaQk&#39;, &#39;8VhVf0TbjDA&#39;, &#39;35694&#39;, &#39;20LfN8ENbhM&#39;, &#39;257277&#39;, &#39;VsXGwSZazwA&#39;, &#39;EyoMU2yoJPY&#39;, &#39;E1r0FrFyNTw&#39;, &#39;CO2YoTZbUr0&#39;, &#39;wC_1M7KIv9s&#39;, &#39;24196&#39;, &#39;194299&#39;, &#39;R9xTBw3MCWI&#39;, &#39;cY8DcaHXNNs&#39;, &#39;SwT0gh0V8fI&#39;, &#39;UiurP5k-f1A&#39;, &#39;N0d2JL7JC1s&#39;, &#39;208592&#39;, &#39;GAVpYuhMZAw&#39;, &#39;pvIQWWiT4-0&#39;, &#39;namehdJxRIM&#39;, &#39;tNd3--lvSXE&#39;, &#39;NaWmaHwjElo&#39;, &#39;mfpR4CN9LZo&#39;, &#39;U6IqFbpM-VM&#39;, &#39;XLjpZUsFEXo&#39;, &#39;YUNxD04EvfE&#39;, &#39;hI7ObFqn9Bg&#39;, &#39;CO6n-skQJss&#39;, &#39;RsE2gYghZ2s&#39;, &#39;2ze94yo2aPo&#39;, &#39;254427&#39;, &#39;MHyW857u_X8&#39;, &#39;Xa086gxLJ3Y&#39;, &#39;Uu_XyXyKHAk&#39;, &#39;TsfyeZ8hgwE&#39;, &#39;vI5JH0daNsM&#39;, &#39;mmg_eTDHjkk&#39;, &#39;lD4xtQ6NpDY&#39;, &#39;XWIp0zH3qDM&#39;, &#39;259470&#39;, &#39;0eTibWQdO5M&#39;, &#39;fcxbB7ybUfs&#39;, &#39;5pxFqJ5hKMU&#39;, &#39;245582&#39;, &#39;WQFRctNL8AA&#39;, &#39;2m58ShI1QSI&#39;, &#39;cn0WZ8-0Z1Y&#39;, &#39;25640&#39;, &#39;huzEsVEJPaY&#39;, &#39;UTErYLpdAY0&#39;, &#39;F2hc2FLOdhI&#39;, &#39;vGxqVh_kJdo&#39;, &#39;F_YaG_pvZrA&#39;, &#39;UNLD7eYPzfQ&#39;, &#39;0K7dCp80n9c&#39;, &#39;xBE9YWYGjtk&#39;, &#39;nTZSH0EwpnY&#39;, &#39;mZ_8em_-CGc&#39;, &#39;fdc7iyzKvFQ&#39;, &#39;221137&#39;, &#39;QBc7X5jj8oA&#39;, &#39;pnpFPX34Agk&#39;, &#39;63951&#39;, &#39;veA6ECsGFxI&#39;, &#39;XbkYA-iXUwA&#39;, &#39;1LkYxsqRPZM&#39;, &#39;qAip3lZRj-g&#39;, &#39;gR3igiwaeyc&#39;, &#39;pIaEcqnzI-s&#39;, &#39;oHff2W51wZ8&#39;, &#39;XlTYSOaZ_vM&#39;, &#39;3WZ6R9B0PcU&#39;, &#39;IOpWjKAHG8Q&#39;, &#39;53766&#39;, &#39;190743&#39;, &#39;107585&#39;, &#39;SYQ_zv8dWng&#39;, &#39;hBzw4r0kfjA&#39;, &#39;0uftSGdwo0Q&#39;, &#39;jj8aSNPHMw8&#39;, &#39;86c2OkQ3_U8&#39;, &#39;rhQB8e999-Q&#39;, &#39;qyqVc352g3Q&#39;, &#39;1zXAYdPdzy8&#39;, &#39;nZFPKP9kBkw&#39;, &#39;A1lFJXUpxZo&#39;, &#39;-cEhr0cQcDM&#39;, &#39;Kn5eKHlPD0k&#39;, &#39;255408&#39;, &#39;eD5cScqaf6c&#39;, &#39;FHDVQkU-nGI&#39;, &#39;24351&#39;, &#39;NOGhjdK-rDI&#39;, &#39;fz-MzQcOBwQ&#39;, &#39;DjcZrtcBZi4&#39;, &#39;1HS2HcN2LDo&#39;, &#39;209758&#39;, &#39;2o2ljA0QD7g&#39;, &#39;211875&#39;, &#39;5lrDS7LluCA&#39;, &#39;ybK5wRaaUyE&#39;, &#39;M6HKUzJNlsE&#39;, &#39;QIonRUsCqBs&#39;, &#39;k8yDywC4gt8&#39;, &#39;jPtaz1rN6lc&#39;, &#39;69824&#39;, &#39;kI6jzM_aLGs&#39;, &#39;x8UZQkN52o4&#39;, &#39;ZKZ8UjaQQT4&#39;, &#39;obGF3RfWQKE&#39;, &#39;221153&#39;, &#39;YgyeyooSz0g&#39;, &#39;faUvT7zfsyk&#39;, &#39;ddWHTdJz2O8&#39;, &#39;OKJPFisBoPY&#39;, &#39;HAnQVHOd3hg&#39;, &#39;EO_5o9Gup6g&#39;, &#39;F7zQPzwFToE&#39;, &#39;273250&#39;, &#39;1pl2FVdQWj0&#39;, &#39;91844&#39;, &#39;bvycs3BXtx0&#39;, &#39;hbJfSyJKBEA&#39;, &#39;ZHUFlEgKk-w&#39;, &#39;OyK86reBnJE&#39;, &#39;xwvSYhQrHoA&#39;, &#39;H-74k5vclCU&#39;, &#39;Sb6ftNgzz9M&#39;, &#39;Hq3cHc6X8BM&#39;, &#39;jscKL5jS-SQ&#39;, &#39;2vsgDSlJ9pU&#39;, &#39;DtbT85s3i94&#39;, &#39;LcfubBagG6Q&#39;, &#39;f-VdKweez2U&#39;, &#39;a8UMRrUjavI&#39;, &#39;MvEw24PU2Ac&#39;, &#39;MZUr1DfYNNw&#39;, &#39;UcV-bpksJi0&#39;, &#39;2W-U94hXuK0&#39;, &#39;OctOcfI4KSs&#39;, &#39;NocexkPXja8&#39;, &#39;eUwbRLhV1vs&#39;, &#39;bdFCD-3BCRg&#39;, &#39;TXiOSZdaLJ8&#39;, &#39;XadAy93f1P8&#39;, &#39;136196&#39;, &#39;gJjkCPO7iXg&#39;, &#39;210433&#39;, &#39;oH9fMma8jiQ&#39;, &#39;d-Uw_uZyUys&#39;, &#39;oQizLbmte0c&#39;, &#39;X6N7UEFJLNY&#39;, &#39;0PlQc98SccA&#39;, &#39;3REPHw7oLWo&#39;, &#39;vB_kZocHtYo&#39;, &#39;2BuFtglEcaY&#39;, &#39;HMRqR-P68Ws&#39;, &#39;V27mmEkN80g&#39;, &#39;Y2F51I-dzAg&#39;, &#39;dTcz1am1eUw&#39;, &#39;gL8h7lOPv1Q&#39;, &#39;WoL4fCxGd8Q&#39;, &#39;135623&#39;, &#39;41381&#39;, &#39;IHp8hd1jm6k&#39;, &#39;dHk--ExZbHs&#39;, &#39;o2XbNJDpOlc&#39;, &#39;V2X1NU5RkwY&#39;, &#39;9orb0lQnVW4&#39;, &#39;fsBzpr4k3rY&#39;, &#39;2fbBrB1nJEQ&#39;, &#39;um8WVjZMLUc&#39;, &#39;eE8Qr9fOvVA&#39;, &#39;fVCDn6SdtVM&#39;, &#39;83400&#39;, &#39;an_GzG40hcE&#39;, &#39;xkEK17UUyi4&#39;, &#39;y3r2kk8zvl0&#39;, &#39;KanWhGY33Hk&#39;, &#39;210259&#39;, &#39;DR65no1jCbg&#39;, &#39;lkIe41StoGI&#39;, &#39;RB3HA-ZMtFw&#39;, &#39;qEuJj4uW93E&#39;, &#39;ydzNAuqUAnc&#39;, &#39;GO0V4ZGSF28&#39;, &#39;9PzZSheh10U&#39;, &#39;6RFVsOWK1m4&#39;, &#39;-s9qJ7ATP7w&#39;, &#39;ey1lr8wFFDc&#39;, &#39;oZxMx8e0x2U&#39;, &#39;UjqA6KVW2m8&#39;, &#39;OaWYjsS02fk&#39;, &#39;79356&#39;, &#39;34cU3HO_hEA&#39;, &#39;KZzFCrEyKF0&#39;, &#39;c5VEvmutmVg&#39;, &#39;O4UkHVJlIs8&#39;, &#39;22373&#39;, &#39;v_8QeoNc4QY&#39;, &#39;BR2pkk3TK-0&#39;, &#39;EMS14J0odIE&#39;, &#39;221274&#39;, &#39;92496&#39;, &#39;DMtFQjrY7Hg&#39;, &#39;h1ZZHUU4j0k&#39;, &#39;gpn71-aKWwQ&#39;, &#39;tW5xAWDnbGU&#39;, &#39;88791&#39;, &#39;vJDDEuE-FlY&#39;, &#39;kaudsLIvYC8&#39;, &#39;x0rLwBIocuI&#39;, &#39;wnL3ld9bM2o&#39;, &#39;8wNr-NQImFg&#39;, &#39;37NMbnd7r20&#39;, &#39;56989&#39;, &#39;ctAZf4sMBUQ&#39;, &#39;7npCA0zoQ8Q&#39;, &#39;u9I5WD3Nglk&#39;, &#39;IIPYcCii7Sg&#39;, &#39;JNhqI4JtPXA&#39;, &#39;Bpy61RdLAvo&#39;, &#39;C5-cY1nPQ20&#39;, &#39;ihPjsliqz5o&#39;, &#39;4t5k_yILGJM&#39;, &#39;mgsvwAVQAQo&#39;, &#39;Ie6sDptjAsU&#39;, &#39;lkeVfgI0eEk&#39;, &#39;O-b3DQg0QmA&#39;, &#39;PzI6yY9Y2xQ&#39;, &#39;lYwgLa4R5XQ&#39;, &#39;NIpuKcoJhGM&#39;, &#39;LpTbjgELALo&#39;, &#39;0YiAKqU36KE&#39;, &#39;ZznoGQVwTtw&#39;, &#39;QCR7uyowjhM&#39;, &#39;ossKC1VrusE&#39;, &#39;p4WmcxrXkc4&#39;, &#39;ZS1Nb0OWYNE&#39;, &#39;P0UHzR4CmYg&#39;, &#39;qTkazqluJ_I&#39;, &#39;252097&#39;, &#39;0JaYazphxfM&#39;, &#39;zvZd3V5D5Ik&#39;, &#39;-RfYyzHpjk4&#39;, &#39;l0vCKpk6Aes&#39;, &#39;ktblaVOnFVE&#39;, &#39;KRje7su4I5U&#39;, &#39;FqEekswKPWE&#39;, &#39;130366&#39;, &#39;HFPGeaEPy9o&#39;, &#39;-HeZS2-Prhc&#39;, &#39;93iGT5oueTA&#39;, &#39;L-7oRnbi9-w&#39;, &#39;pQpy7RSfWzM&#39;, &#39;YsMM_1R1vtw&#39;, &#39;SBFnCNzlynQ&#39;, &#39;4dAYMzRyndc&#39;, &#39;CU6U-gS76K4&#39;, &#39;NiAjz4ciA60&#39;, &#39;-9y-fZ3swSY&#39;, &#39;LD3HYOwk1Bc&#39;, &#39;QXXexH-ow_k&#39;, &#39;-UUCSKoHeMA&#39;, &#39;RVC8l5hf2Eg&#39;, &#39;89ZmOPOilu4&#39;, &#39;xobMRm5Vs44&#39;, &#39;xmLJHru6Z1M&#39;, &#39;zfZUOvZZTuk&#39;, &#39;LJGL2sGvSS0&#39;, &#39;Pbu6oAKFCvo&#39;, &#39;nFTo-Lz4Fr8&#39;, &#39;CwF97vXPYX4&#39;, &#39;WJM8u2I2rQ4&#39;, &#39;8XZszYrUSCQ&#39;, &#39;fT6SrlsWV7M&#39;, &#39;fWAKek8jA5M&#39;, &#39;jXQmVFcOiUI&#39;, &#39;KrmVX-HANew&#39;, &#39;kXhJ3hHK9hQ&#39;, &#39;oPlnhc0DkcU&#39;, &#39;OFia3dWgaoI&#39;, &#39;VVtx4IDsHZA&#39;, &#39;VIVkYG31Oas&#39;, &#39;I9iV9q3ePhI&#39;, &#39;ZeH7gZw94k0&#39;, &#39;wznRBN1fWj4&#39;, &#39;226601&#39;, &#39;TcTGCIh6e5s&#39;, &#39;1S6ji_d4OLI&#39;, &#39;mRqqH_gx7Q0&#39;, &#39;Zb7bdrjWyEY&#39;, &#39;VDkBM0ZG4q8&#39;, &#39;BJS5KCSowgU&#39;, &#39;3UOqbf_B_Yc&#39;, &#39;LyOq9mOEPuk&#39;, &#39;-MeTTeMJBNc&#39;, &#39;QLI-OnegxFM&#39;, &#39;3odZe3AGilc&#39;, &#39;VXy21GwGs8o&#39;, &#39;7deoh0NoMs4&#39;, &#39;DzdPl68gV5o&#39;, &#39;VS7xSvno7NA&#39;, &#39;H_x5O9GdknI&#39;, &#39;67uKYi7mslE&#39;, &#39;Kn99u05vlpA&#39;, &#39;gKojBrXHhUc&#39;, &#39;AHiA9hohKr8&#39;, &#39;kmgsC68hIL8&#39;, &#39;XVWiAArXYpE&#39;, &#39;X_TusmBIlC0&#39;, &#39;dlE05KC95uk&#39;, &#39;KXsjl0DKgL0&#39;, &#39;b92iw0OAnI4&#39;, &#39;j1m6ctAgjsM&#39;, &#39;6EDoVEm16fU&#39;, &#39;jE3gYsxr_5s&#39;, &#39;6EJHA6IDLNk&#39;, &#39;xXXcgb9eZ9Y&#39;, &#39;rcfnqiD0y8o&#39;, &#39;224370&#39;, &#39;237363&#39;, &#39;7IxmlIwqigw&#39;, &#39;6gtfeIqGasE&#39;, &#39;9GzFQBljNIY&#39;, &#39;vGkIaClHKDg&#39;, &#39;iXiMifHNKPI&#39;, &#39;BTjV5dU_rfY&#39;, &#39;g6VJg6ycUk0&#39;, &#39;Y8dI1GTWCk4&#39;, &#39;y5Jpf48SUX8&#39;, &#39;iFxFTtCQ6zA&#39;, &#39;ZKKNd0zR8Io&#39;, &#39;U8VYG_g6yVE&#39;, &#39;GNP0PFas12Y&#39;, &#39;ussfmzwftQ8&#39;, &#39;jLN6B0aSrW0&#39;, &#39;OXsTIPdytiw&#39;, &#39;121400&#39;, &#39;CKqDDh50gcU&#39;, &#39;HJTxq72GuMs&#39;, &#39;273207&#39;, &#39;lxBKEPIUSgc&#39;, &#39;234046&#39;, &#39;59673&#39;, &#39;USkMa4Evv7U&#39;, &#39;213327&#39;, &#39;kXiBdruxTvE&#39;, &#39;201005&#39;, &#39;94481&#39;, &#39;ChhZna-aBK4&#39;, &#39;a4PHWxILDp0&#39;, &#39;SqAiJrvHXNA&#39;, &#39;kg-W6-hP2Do&#39;, &#39;9Lr4i7bIB6w&#39;, &#39;fhADeWE5VgU&#39;, &#39;-yRb-Jum7EQ&#39;, &#39;DVAY-u_cJWU&#39;, &#39;wd8VQ5E7o7o&#39;, &#39;N2nCB34Am-E&#39;, &#39;f8Puta8k8fU&#39;, &#39;272838&#39;, &#39;Qfa1fY_07bQ&#39;, &#39;WBA79Q3e_PU&#39;, &#39;ozA7pRW4gFM&#39;, &#39;WuaoxGAKue0&#39;, &#39;-AUZQgSxyPQ&#39;, &#39;l4oMbKDuW3Y&#39;, &#39;198112&#39;, &#39;H9BNzzxlscA&#39;, &#39;224631&#39;, &#39;111881&#39;, &#39;U-KihZeIfKI&#39;, &#39;JXYot3NDQn0&#39;, &#39;7f3ndBCx_JE&#39;, &#39;F8eQI8E-6q4&#39;, &#39;112509&#39;, &#39;46615&#39;, &#39;p1zqEGwRMI8&#39;, &#39;Wu-wQTmxRgo&#39;, &#39;V0SvSPkiJUY&#39;, &#39;28006&#39;, &#39;cia8OM3Oe7Q&#39;, &#39;_aJghSQmxD8&#39;, &#39;97ENTofrmNo&#39;, &#39;252912&#39;, &#39;bkX5FOX22Tw&#39;, &#39;108146&#39;, &#39;Iemw8vqt-54&#39;, &#39;vTAV6FThy30&#39;, &#39;_WA0HtVEe8U&#39;, &#39;132028&#39;, &#39;nXWNHVZtV9A&#39;, &#39;MLegxOZBGUc&#39;, &#39;224263&#39;, &#39;q5M1God4M6Y&#39;, &#39;9cYNT_YSo8o&#39;, &#39;22689&#39;, &#39;SD6wphlw72U&#39;, &#39;SqofxdeEcjg&#39;, &#39;_on_E-ZWy0I&#39;, &#39;222247&#39;, &#39;cX8FScpsfLE&#39;, &#39;k1BrzX5bc7U&#39;, &#39;Rb1uzHNcYcA&#39;, &#39;RChkXDS8ulE&#39;, &#39;7mb8Y2AhXIY&#39;, &#39;226640&#39;, &#39;MM2qPdnRmQ8&#39;, &#39;unOeTDc2rlY&#39;, &#39;ROC2YI3tDsk&#39;, &#39;AlJX3Jw3xKk&#39;, &#39;MHVrwCEWLPI&#39;, &#39;bNQOeiAotbk&#39;, &#39;TLPlduck5II&#39;, &#39;An8x4UfwZ7k&#39;, &#39;JHJOK6cdW-0&#39;, &#39;xU3N7ujUB-g&#39;, &#39;RvmTZVNAAuQ&#39;, &#39;LtlL-03S79Q&#39;, &#39;Xy57UpKRNEo&#39;, &#39;lO6N9dyvPTA&#39;, &#39;cW-aX4dPVfk&#39;, &#39;VwGPIUNayKM&#39;, &#39;p7zuPEZgtY4&#39;, &#39;ZtocGyL3Tfc&#39;, &#39;24504&#39;, &#39;5eY4S1F62Z4&#39;, &#39;ttfaZ8IIWCo&#39;, &#39;z0y1ZxH1f74&#39;, &#39;VLQgw-88v4Q&#39;, &#39;1Gp4l-ZTCVk&#39;, &#39;QJBQIOmG1CA&#39;, &#39;jqutn5ou8_0&#39;, &#39;gcFECfN4BCU&#39;, &#39;Dm8AL84e11w&#39;, &#39;tO68uTk-T_E&#39;, &#39;215318&#39;, &#39;DlX-WyVe-D0&#39;, &#39;gLTxaEcx41E&#39;, &#39;RmX9t_HY_dU&#39;, &#39;HbaycY0VuZk&#39;, &#39;dxsPkcG-Q30&#39;, &#39;ZcFzcd4ZoMg&#39;, &#39;yUqNp-poh9M&#39;, &#39;yoDMh8FlHR8&#39;, &#39;167521&#39;, &#39;kbRtSmJM5aU&#39;, &#39;skRqBxLLJkE&#39;, &#39;100178&#39;, &#39;-ri04Z7vwnc&#39;, &#39;mVnqP-vLpuo&#39;, &#39;B9oeT0K92wU&#39;, &#39;_OmWVs0-6UM&#39;, &#39;DebbnL-_rnI&#39;, &#39;GlShH_ua_GU&#39;, &#39;Jz1oMq6l-ZM&#39;, &#39;L-a4Sh6iAcw&#39;, &#39;LDKWr94J0wM&#39;, &#39;aa0J1AXSseY&#39;, &#39;173CpFb3clw&#39;, &#39;202826&#39;, &#39;Wmhif6hmPTQ&#39;, &#39;283935&#39;, &#39;naZi9AusrW4&#39;, &#39;wO8fUOC4OSE&#39;, &#39;_1nvuNk7EFY&#39;, &#39;PHZIx22aFhU&#39;, &#39;ex-dKshlXoY&#39;, &#39;6uoM8-9d0zA&#39;, &#39;ahG9c_uaf8s&#39;, &#39;vR90Pdx9wxs&#39;]\n"
    }
   ],
   "source": [
    "# obtain the train/dev/test splits - these splits are based on video IDs\n",
    "train_split = DATASET.standard_folds.standard_train_fold\n",
    "dev_split = DATASET.standard_folds.standard_valid_fold\n",
    "test_split = DATASET.standard_folds.standard_test_fold\n",
    "\n",
    "# inspect the splits: they only contain video IDs\n",
    "print(test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "esn&#39;t belong to any splits: -dZ1TCboxcQ\nFound video that doesn&#39;t belong to any splits: -hPfPhUIzfA\nFound video that doesn&#39;t belong to any splits: -hPfPhUIzfA\nFound video that doesn&#39;t belong to any splits: -hPfPhUIzfA\nFound video that doesn&#39;t belong to any splits: -hPfPhUIzfA\nFound video that doesn&#39;t belong to any splits: -l_53IwQoj0\nFound video that doesn&#39;t belong to any splits: -m9KtvCk_L8\nFound video that doesn&#39;t belong to any splits: -m9KtvCk_L8\nFound video that doesn&#39;t belong to any splits: -m9KtvCk_L8\nFound video that doesn&#39;t belong to any splits: -o_667Ny7RM\nFound video that doesn&#39;t belong to any splits: -qMhGeJkp5o\nFound video that doesn&#39;t belong to any splits: -qMhGeJkp5o\nFound video that doesn&#39;t belong to any splits: -qMhGeJkp5o\nFound video that doesn&#39;t belong to any splits: -qMhGeJkp5o\nFound video that doesn&#39;t belong to any splits: -qMhGeJkp5o\nFound video that doesn&#39;t belong to any splits: -qMhGeJkp5o\nFound video that doesn&#39;t belong to any splits: -qMhGeJkp5o\nFound video that doesn&#39;t belong to any splits: -qMhGeJkp5o\nFound video that doesn&#39;t belong to any splits: -qMhGeJkp5o\nFound video that doesn&#39;t belong to any splits: -wdUIwjDMfg\nFound video that doesn&#39;t belong to any splits: 1S4p-Lfhbp4\nFound video that doesn&#39;t belong to any splits: 1S4p-Lfhbp4\nFound video that doesn&#39;t belong to any splits: 1S4p-Lfhbp4\nFound video that doesn&#39;t belong to any splits: 1S4p-Lfhbp4\nFound video that doesn&#39;t belong to any splits: 1S4p-Lfhbp4\nFound video that doesn&#39;t belong to any splits: 1S4p-Lfhbp4\nFound video that doesn&#39;t belong to any splits: 1knHBRVPoYQ\nFound video that doesn&#39;t belong to any splits: 1knHBRVPoYQ\nFound video that doesn&#39;t belong to any splits: 1knHBRVPoYQ\nFound video that doesn&#39;t belong to any splits: 1knHBRVPoYQ\nFound video that doesn&#39;t belong to any splits: 1knHBRVPoYQ\nFound video that doesn&#39;t belong to any splits: 1knHBRVPoYQ\nFound video that doesn&#39;t belong to any splits: 1knHBRVPoYQ\nFound video that doesn&#39;t belong to any splits: 1knHBRVPoYQ\nFound video that doesn&#39;t belong to any splits: 4Y8ROYezksQ\nFound video that doesn&#39;t belong to any splits: 5OUcvUDMMWE\nFound video that doesn&#39;t belong to any splits: 5OUcvUDMMWE\nFound video that doesn&#39;t belong to any splits: 5OUcvUDMMWE\nFound video that doesn&#39;t belong to any splits: 5OUcvUDMMWE\nFound video that doesn&#39;t belong to any splits: 5OUcvUDMMWE\nFound video that doesn&#39;t belong to any splits: 5OUcvUDMMWE\nFound video that doesn&#39;t belong to any splits: 5OUcvUDMMWE\nFound video that doesn&#39;t belong to any splits: 5OUcvUDMMWE\nFound video that doesn&#39;t belong to any splits: 5OUcvUDMMWE\nFound video that doesn&#39;t belong to any splits: 5OUcvUDMMWE\nFound video that doesn&#39;t belong to any splits: 5OUcvUDMMWE\nFound video that doesn&#39;t belong to any splits: 6ywl-j2-e-s\nFound video that doesn&#39;t belong to any splits: 6ywl-j2-e-s\nFound video that doesn&#39;t belong to any splits: 7BFhRPlSOf0\nFound video that doesn&#39;t belong to any splits: 7BFhRPlSOf0\nFound video that doesn&#39;t belong to any splits: 7BFhRPlSOf0\nFound video that doesn&#39;t belong to any splits: 7BFhRPlSOf0\nFound video that doesn&#39;t belong to any splits: 7BFhRPlSOf0\nFound video that doesn&#39;t belong to any splits: 8vpoQ5UVpXE\nFound video that doesn&#39;t belong to any splits: 8vpoQ5UVpXE\nFound video that doesn&#39;t belong to any splits: 8vpoQ5UVpXE\nFound video that doesn&#39;t belong to any splits: 8vpoQ5UVpXE\nFound video that doesn&#39;t belong to any splits: 8vpoQ5UVpXE\nFound video that doesn&#39;t belong to any splits: 9OY3NU2ycVw\nFound video that doesn&#39;t belong to any splits: 9OY3NU2ycVw\nFound video that doesn&#39;t belong to any splits: 9OY3NU2ycVw\nFound video that doesn&#39;t belong to any splits: 9OY3NU2ycVw\nFound video that doesn&#39;t belong to any splits: 9OY3NU2ycVw\nFound video that doesn&#39;t belong to any splits: 9OY3NU2ycVw\nFound video that doesn&#39;t belong to any splits: 9OY3NU2ycVw\nFound video that doesn&#39;t belong to any splits: 9OY3NU2ycVw\nFound video that doesn&#39;t belong to any splits: 9OY3NU2ycVw\nFound video that doesn&#39;t belong to any splits: 9kLNVTm3Z90\nFound video that doesn&#39;t belong to any splits: 9kLNVTm3Z90\nFound video that doesn&#39;t belong to any splits: 9kLNVTm3Z90\nFound video that doesn&#39;t belong to any splits: AHXwnFvqYDk\nFound video that doesn&#39;t belong to any splits: AHXwnFvqYDk\nFound video that doesn&#39;t belong to any splits: AHXwnFvqYDk\nFound video that doesn&#39;t belong to any splits: AHXwnFvqYDk\nFound video that doesn&#39;t belong to any splits: AHXwnFvqYDk\nFound video that doesn&#39;t belong to any splits: AHXwnFvqYDk\nFound video that doesn&#39;t belong to any splits: AHXwnFvqYDk\nFound video that doesn&#39;t belong to any splits: AHXwnFvqYDk\nFound video that doesn&#39;t belong to any splits: AcoRjVPIVmQ\nFound video that doesn&#39;t belong to any splits: AcoRjVPIVmQ\nFound video that doesn&#39;t belong to any splits: BgXqrxum5G4\nFound video that doesn&#39;t belong to any splits: BgXqrxum5G4\nFound video that doesn&#39;t belong to any splits: BgXqrxum5G4\nFound video that doesn&#39;t belong to any splits: BgXqrxum5G4\nFound video that doesn&#39;t belong to any splits: BgXqrxum5G4\nFound video that doesn&#39;t belong to any splits: BgXqrxum5G4\nFound video that doesn&#39;t belong to any splits: BgXqrxum5G4\nFound video that doesn&#39;t belong to any splits: BgXqrxum5G4\nFound video that doesn&#39;t belong to any splits: CXvUdCdKTJY\nFound video that doesn&#39;t belong to any splits: CXvUdCdKTJY\nFound video that doesn&#39;t belong to any splits: CXvUdCdKTJY\nFound video that doesn&#39;t belong to any splits: CXvUdCdKTJY\nFound video that doesn&#39;t belong to any splits: EWYivXUavOY\nFound video that doesn&#39;t belong to any splits: EWYivXUavOY\nFound video that doesn&#39;t belong to any splits: EWYivXUavOY\nFound video that doesn&#39;t belong to any splits: EWYivXUavOY\nFound video that doesn&#39;t belong to any splits: EWYivXUavOY\nFound video that doesn&#39;t belong to any splits: EuSp1pkX9i8\nFound video that doesn&#39;t belong to any splits: EuSp1pkX9i8\nFound video that doesn&#39;t belong to any splits: FzxmeVRTjwU\nFound video that doesn&#39;t belong to any splits: FzxmeVRTjwU\nFound video that doesn&#39;t belong to any splits: FzxmeVRTjwU\nFound video that doesn&#39;t belong to any splits: FzxmeVRTjwU\nFound video that doesn&#39;t belong to any splits: FzxmeVRTjwU\nFound video that doesn&#39;t belong to any splits: FzxmeVRTjwU\nFound video that doesn&#39;t belong to any splits: FzxmeVRTjwU\nFound video that doesn&#39;t belong to any splits: FzxmeVRTjwU\nFound video that doesn&#39;t belong to any splits: FzxmeVRTjwU\nFound video that doesn&#39;t belong to any splits: JNt1k3ohkbs\nFound video that doesn&#39;t belong to any splits: JNt1k3ohkbs\nFound video that doesn&#39;t belong to any splits: JNt1k3ohkbs\nFound video that doesn&#39;t belong to any splits: JNt1k3ohkbs\nFound video that doesn&#39;t belong to any splits: JNt1k3ohkbs\nFound video that doesn&#39;t belong to any splits: JNt1k3ohkbs\nFound video that doesn&#39;t belong to any splits: JNt1k3ohkbs\nFound video that doesn&#39;t belong to any splits: MIqL9OXszRw\nFound video that doesn&#39;t belong to any splits: MIqL9OXszRw\nFound video that doesn&#39;t belong to any splits: MIqL9OXszRw\nFound video that doesn&#39;t belong to any splits: MIqL9OXszRw\nFound video that doesn&#39;t belong to any splits: MIqL9OXszRw\nFound video that doesn&#39;t belong to any splits: MIqL9OXszRw\nFound video that doesn&#39;t belong to any splits: NVlxq3jxlAk\nFound video that doesn&#39;t belong to any splits: NVlxq3jxlAk\nFound video that doesn&#39;t belong to any splits: NVlxq3jxlAk\nFound video that doesn&#39;t belong to any splits: NVlxq3jxlAk\nFound video that doesn&#39;t belong to any splits: NVlxq3jxlAk\nFound video that doesn&#39;t belong to any splits: NVlxq3jxlAk\nFound video that doesn&#39;t belong to any splits: NVlxq3jxlAk\nFound video that doesn&#39;t belong to any splits: NVlxq3jxlAk\nFound video that doesn&#39;t belong to any splits: NVlxq3jxlAk\nFound video that doesn&#39;t belong to any splits: NVlxq3jxlAk\nFound video that doesn&#39;t belong to any splits: TdCfpMRiA1c\nFound video that doesn&#39;t belong to any splits: TdCfpMRiA1c\nFound video that doesn&#39;t belong to any splits: TdCfpMRiA1c\nFound video that doesn&#39;t belong to any splits: TdCfpMRiA1c\nFound video that doesn&#39;t belong to any splits: TdCfpMRiA1c\nFound video that doesn&#39;t belong to any splits: TdCfpMRiA1c\nFound video that doesn&#39;t belong to any splits: TdCfpMRiA1c\nFound video that doesn&#39;t belong to any splits: TdCfpMRiA1c\nFound video that doesn&#39;t belong to any splits: TwFl4epL244\nFound video that doesn&#39;t belong to any splits: TwFl4epL244\nFound video that doesn&#39;t belong to any splits: TwFl4epL244\nFound video that doesn&#39;t belong to any splits: TwFl4epL244\nFound video that doesn&#39;t belong to any splits: TwFl4epL244\nFound video that doesn&#39;t belong to any splits: TwFl4epL244\nFound video that doesn&#39;t belong to any splits: TwFl4epL244\nFound video that doesn&#39;t belong to any splits: TwFl4epL244\nFound video that doesn&#39;t belong to any splits: UcINGGuysBE\nFound video that doesn&#39;t belong to any splits: UcINGGuysBE\nFound video that doesn&#39;t belong to any splits: UcINGGuysBE\nFound video that doesn&#39;t belong to any splits: UcINGGuysBE\nFound video that doesn&#39;t belong to any splits: UcINGGuysBE\nFound video that doesn&#39;t belong to any splits: UcINGGuysBE\nFound video that doesn&#39;t belong to any splits: UcINGGuysBE\nFound video that doesn&#39;t belong to any splits: UcINGGuysBE\nFound video that doesn&#39;t belong to any splits: XnPnlRQXSkM\nFound video that doesn&#39;t belong to any splits: XnPnlRQXSkM\nFound video that doesn&#39;t belong to any splits: XnPnlRQXSkM\nFound video that doesn&#39;t belong to any splits: XnPnlRQXSkM\nFound video that doesn&#39;t belong to any splits: ZXQkyX6prBQ\nFound video that doesn&#39;t belong to any splits: ZXQkyX6prBQ\nFound video that doesn&#39;t belong to any splits: ZXQkyX6prBQ\nFound video that doesn&#39;t belong to any splits: ZXQkyX6prBQ\nFound video that doesn&#39;t belong to any splits: ZXQkyX6prBQ\nFound video that doesn&#39;t belong to any splits: ZXQkyX6prBQ\nFound video that doesn&#39;t belong to any splits: ZXQkyX6prBQ\nFound video that doesn&#39;t belong to any splits: aa2MEQ6FL7k\nFound video that doesn&#39;t belong to any splits: aa2MEQ6FL7k\nFound video that doesn&#39;t belong to any splits: aa2MEQ6FL7k\nFound video that doesn&#39;t belong to any splits: aa2MEQ6FL7k\nFound video that doesn&#39;t belong to any splits: aa2MEQ6FL7k\nFound video that doesn&#39;t belong to any splits: aa2MEQ6FL7k\nFound video that doesn&#39;t belong to any splits: bDwAuRFciJM\nFound video that doesn&#39;t belong to any splits: bDwAuRFciJM\nFound video that doesn&#39;t belong to any splits: bDwAuRFciJM\nFound video that doesn&#39;t belong to any splits: bDwAuRFciJM\nFound video that doesn&#39;t belong to any splits: bDwAuRFciJM\nFound video that doesn&#39;t belong to any splits: bDwAuRFciJM\nFound video that doesn&#39;t belong to any splits: bDwAuRFciJM\nFound video that doesn&#39;t belong to any splits: bDwAuRFciJM\nFound video that doesn&#39;t belong to any splits: cuR-l2qCxBc\nFound video that doesn&#39;t belong to any splits: cuR-l2qCxBc\nFound video that doesn&#39;t belong to any splits: cuR-l2qCxBc\nFound video that doesn&#39;t belong to any splits: cuR-l2qCxBc\nFound video that doesn&#39;t belong to any splits: cuR-l2qCxBc\nFound video that doesn&#39;t belong to any splits: cuR-l2qCxBc\nFound video that doesn&#39;t belong to any splits: cuR-l2qCxBc\nFound video that doesn&#39;t belong to any splits: cuR-l2qCxBc\nFound video that doesn&#39;t belong to any splits: cuR-l2qCxBc\nFound video that doesn&#39;t belong to any splits: dYgmvmFegkQ\nFound video that doesn&#39;t belong to any splits: dYgmvmFegkQ\nFound video that doesn&#39;t belong to any splits: dYgmvmFegkQ\nFound video that doesn&#39;t belong to any splits: dYgmvmFegkQ\nFound video that doesn&#39;t belong to any splits: dYgmvmFegkQ\nFound video that doesn&#39;t belong to any splits: dYgmvmFegkQ\nFound video that doesn&#39;t belong to any splits: dYgmvmFegkQ\nFound video that doesn&#39;t belong to any splits: fMUmdlzuc94\nFound video that doesn&#39;t belong to any splits: fMUmdlzuc94\nFound video that doesn&#39;t belong to any splits: fMUmdlzuc94\nFound video that doesn&#39;t belong to any splits: fMUmdlzuc94\nFound video that doesn&#39;t belong to any splits: fMUmdlzuc94\nFound video that doesn&#39;t belong to any splits: fMUmdlzuc94\nFound video that doesn&#39;t belong to any splits: fiXZqpbRYKk\nFound video that doesn&#39;t belong to any splits: fiXZqpbRYKk\nFound video that doesn&#39;t belong to any splits: fiXZqpbRYKk\nFound video that doesn&#39;t belong to any splits: fiXZqpbRYKk\nFound video that doesn&#39;t belong to any splits: fiXZqpbRYKk\nFound video that doesn&#39;t belong to any splits: fiXZqpbRYKk\nFound video that doesn&#39;t belong to any splits: fiXZqpbRYKk\nFound video that doesn&#39;t belong to any splits: fiXZqpbRYKk\nFound video that doesn&#39;t belong to any splits: fiXZqpbRYKk\nFound video that doesn&#39;t belong to any splits: fiXZqpbRYKk\nFound video that doesn&#39;t belong to any splits: fiXZqpbRYKk\nFound video that doesn&#39;t belong to any splits: fiXZqpbRYKk\nFound video that doesn&#39;t belong to any splits: flv4by3RYVk\nFound video that doesn&#39;t belong to any splits: flv4by3RYVk\nFound video that doesn&#39;t belong to any splits: flv4by3RYVk\nFound video that doesn&#39;t belong to any splits: flv4by3RYVk\nFound video that doesn&#39;t belong to any splits: flv4by3RYVk\nFound video that doesn&#39;t belong to any splits: flv4by3RYVk\nFound video that doesn&#39;t belong to any splits: flv4by3RYVk\nFound video that doesn&#39;t belong to any splits: flv4by3RYVk\nFound video that doesn&#39;t belong to any splits: flv4by3RYVk\nFound video that doesn&#39;t belong to any splits: flv4by3RYVk\nFound video that doesn&#39;t belong to any splits: flv4by3RYVk\nFound video that doesn&#39;t belong to any splits: flv4by3RYVk\nFound video that doesn&#39;t belong to any splits: foXArPhK0xY\nFound video that doesn&#39;t belong to any splits: foXArPhK0xY\nFound video that doesn&#39;t belong to any splits: foXArPhK0xY\nFound video that doesn&#39;t belong to any splits: foXArPhK0xY\nFound video that doesn&#39;t belong to any splits: foXArPhK0xY\nFound video that doesn&#39;t belong to any splits: foXArPhK0xY\nFound video that doesn&#39;t belong to any splits: foXArPhK0xY\nFound video that doesn&#39;t belong to any splits: hytnfm-AVaA\nFound video that doesn&#39;t belong to any splits: hytnfm-AVaA\nFound video that doesn&#39;t belong to any splits: hytnfm-AVaA\nFound video that doesn&#39;t belong to any splits: hytnfm-AVaA\nFound video that doesn&#39;t belong to any splits: hytnfm-AVaA\nFound video that doesn&#39;t belong to any splits: hytnfm-AVaA\nFound video that doesn&#39;t belong to any splits: hytnfm-AVaA\nFound video that doesn&#39;t belong to any splits: hytnfm-AVaA\nFound video that doesn&#39;t belong to any splits: mLZZ174bqws\nFound video that doesn&#39;t belong to any splits: mLZZ174bqws\nFound video that doesn&#39;t belong to any splits: mLZZ174bqws\nFound video that doesn&#39;t belong to any splits: mLZZ174bqws\nFound video that doesn&#39;t belong to any splits: nMCCLB9gOag\nFound video that doesn&#39;t belong to any splits: nMCCLB9gOag\nFound video that doesn&#39;t belong to any splits: nMCCLB9gOag\nFound video that doesn&#39;t belong to any splits: nMCCLB9gOag\nFound video that doesn&#39;t belong to any splits: nMCCLB9gOag\nFound video that doesn&#39;t belong to any splits: nMCCLB9gOag\nFound video that doesn&#39;t belong to any splits: nMCCLB9gOag\nFound video that doesn&#39;t belong to any splits: o5OsO6Aq9pA\nFound video that doesn&#39;t belong to any splits: o5OsO6Aq9pA\nFound video that doesn&#39;t belong to any splits: o5OsO6Aq9pA\nFound video that doesn&#39;t belong to any splits: o5OsO6Aq9pA\nFound video that doesn&#39;t belong to any splits: o5OsO6Aq9pA\nFound video that doesn&#39;t belong to any splits: o5OsO6Aq9pA\nFound video that doesn&#39;t belong to any splits: o5OsO6Aq9pA\nFound video that doesn&#39;t belong to any splits: o5OsO6Aq9pA\nFound video that doesn&#39;t belong to any splits: o5OsO6Aq9pA\nFound video that doesn&#39;t belong to any splits: o6Cd9RmqdBc\nFound video that doesn&#39;t belong to any splits: o6Cd9RmqdBc\nFound video that doesn&#39;t belong to any splits: o6Cd9RmqdBc\nFound video that doesn&#39;t belong to any splits: o6Cd9RmqdBc\nFound video that doesn&#39;t belong to any splits: o6Cd9RmqdBc\nFound video that doesn&#39;t belong to any splits: o6Cd9RmqdBc\nFound video that doesn&#39;t belong to any splits: o6Cd9RmqdBc\nFound video that doesn&#39;t belong to any splits: oNnoN0bxVnA\nFound video that doesn&#39;t belong to any splits: oNnoN0bxVnA\nFound video that doesn&#39;t belong to any splits: oNnoN0bxVnA\nFound video that doesn&#39;t belong to any splits: oNnoN0bxVnA\nFound video that doesn&#39;t belong to any splits: oNnoN0bxVnA\nFound video that doesn&#39;t belong to any splits: oNnoN0bxVnA\nFound video that doesn&#39;t belong to any splits: ptp-kfn4vWY\nFound video that doesn&#39;t belong to any splits: ptp-kfn4vWY\nFound video that doesn&#39;t belong to any splits: qYC3gN1AkOA\nFound video that doesn&#39;t belong to any splits: qYC3gN1AkOA\nFound video that doesn&#39;t belong to any splits: qYC3gN1AkOA\nFound video that doesn&#39;t belong to any splits: qYC3gN1AkOA\nFound video that doesn&#39;t belong to any splits: qYC3gN1AkOA\nFound video that doesn&#39;t belong to any splits: qYC3gN1AkOA\nFound video that doesn&#39;t belong to any splits: qYC3gN1AkOA\nFound video that doesn&#39;t belong to any splits: qYC3gN1AkOA\nFound video that doesn&#39;t belong to any splits: rKyXU0ls5aQ\nFound video that doesn&#39;t belong to any splits: rKyXU0ls5aQ\nFound video that doesn&#39;t belong to any splits: rKyXU0ls5aQ\nFound video that doesn&#39;t belong to any splits: rKyXU0ls5aQ\nFound video that doesn&#39;t belong to any splits: rKyXU0ls5aQ\nFound video that doesn&#39;t belong to any splits: slz8pd0Wzxs\nFound video that doesn&#39;t belong to any splits: tkzdanzsA0A\nFound video that doesn&#39;t belong to any splits: tkzdanzsA0A\nFound video that doesn&#39;t belong to any splits: tkzdanzsA0A\nFound video that doesn&#39;t belong to any splits: tkzdanzsA0A\nFound video that doesn&#39;t belong to any splits: tkzdanzsA0A\nFound video that doesn&#39;t belong to any splits: tkzdanzsA0A\nFound video that doesn&#39;t belong to any splits: tkzdanzsA0A\nFound video that doesn&#39;t belong to any splits: tkzdanzsA0A\nFound video that doesn&#39;t belong to any splits: tkzdanzsA0A\nFound video that doesn&#39;t belong to any splits: vn0CFvUK5D0\nFound video that doesn&#39;t belong to any splits: vn0CFvUK5D0\nFound video that doesn&#39;t belong to any splits: vn0CFvUK5D0\nFound video that doesn&#39;t belong to any splits: vn0CFvUK5D0\nFound video that doesn&#39;t belong to any splits: x-oexIBWwR4\nFound video that doesn&#39;t belong to any splits: x-oexIBWwR4\nFound video that doesn&#39;t belong to any splits: x-oexIBWwR4\nFound video that doesn&#39;t belong to any splits: x-oexIBWwR4\nFound video that doesn&#39;t belong to any splits: x-oexIBWwR4\nFound video that doesn&#39;t belong to any splits: x-oexIBWwR4\nFound video that doesn&#39;t belong to any splits: x-oexIBWwR4\nFound video that doesn&#39;t belong to any splits: x-oexIBWwR4\nFound video that doesn&#39;t belong to any splits: x-oexIBWwR4\nFound video that doesn&#39;t belong to any splits: zShw-E-yBqU\nFound video that doesn&#39;t belong to any splits: zShw-E-yBqU\nFound video that doesn&#39;t belong to any splits: zShw-E-yBqU\nFound video that doesn&#39;t belong to any splits: zShw-E-yBqU\nFound video that doesn&#39;t belong to any splits: zShw-E-yBqU\nTotal number of 0 datapoints have been dropped.\n"
    }
   ],
   "source": [
    "# we can see they are in the format of 'video_id[segment_no]', but the splits was specified with video_id only\n",
    "# we need to use regex or something to match the video IDs...\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict\n",
    "\n",
    "# a sentinel epsilon for safe division, without it we will replace illegal values with a constant\n",
    "EPS = 0\n",
    "\n",
    "# construct a word2id mapping that automatically takes increment when new words are encountered\n",
    "word2id = defaultdict(lambda: len(word2id))\n",
    "UNK = word2id['<unk>']\n",
    "PAD = word2id['<pad>']\n",
    "\n",
    "# place holders for the final train/dev/test dataset\n",
    "train = []\n",
    "dev = []\n",
    "test = []\n",
    "\n",
    "# define a regular expression to extract the video ID out of the keys\n",
    "pattern = re.compile('(.*)\\[.*\\]')\n",
    "num_drop = 0 # a counter to count how many data points went into some processing issues\n",
    "\n",
    "for segment in dataset[label_field].keys():\n",
    "    \n",
    "    # get the video ID and the features out of the aligned dataset\n",
    "    vid = re.search(pattern, segment).group(1)\n",
    "    label = dataset[label_field][segment]['features']\n",
    "    _words = dataset[text_field][segment]['features']\n",
    "    _visual = dataset[visual_field][segment]['features']\n",
    "    _acoustic = dataset[acoustic_field][segment]['features']\n",
    "\n",
    "    # if the sequences are not same length after alignment, there must be some problem with some modalities\n",
    "    # we should drop it or inspect the data again\n",
    "    #if not _words.shape[0] == _visual.shape[0] == _acoustic.shape[0]:\n",
    "    #    print(f\"Encountered datapoint {vid} with text shape {_words.shape}, visual shape {_visual.shape}, acoustic shape {_acoustic.shape}\")\n",
    "    #    num_drop += 1\n",
    "    #    continue\n",
    "\n",
    "    # remove nan values\n",
    "    label = np.nan_to_num(label)\n",
    "    _visual = np.nan_to_num(_visual)\n",
    "    _acoustic = np.nan_to_num(_acoustic)\n",
    "\n",
    "    # remove speech pause tokens - this is in general helpful\n",
    "    # we should remove speech pauses and corresponding visual/acoustic features together\n",
    "    # otherwise modalities would no longer be aligned\n",
    "    words = []\n",
    "    visual = []\n",
    "    acoustic = []\n",
    "    for i, word in enumerate(_words):\n",
    "        #if word[0] != b'sp':\n",
    "        #words.append(word2id[word[0].decode('utf-8')]) # SDK stores strings as bytes, decode into strings here\n",
    "        words.append(word2id[word[0]])\n",
    "        visual.append(_visual[i, :])\n",
    "        acoustic.append(_acoustic[i, :])\n",
    "\n",
    "    words = np.asarray(words)\n",
    "    visual = np.asarray(visual)\n",
    "    acoustic = np.asarray(acoustic)\n",
    "\n",
    "    # z-normalization per instance and remove nan/infs\n",
    "    visual = np.nan_to_num((visual - visual.mean(0, keepdims=True)) / (EPS + np.std(visual, axis=0, keepdims=True)))\n",
    "    acoustic = np.nan_to_num((acoustic - acoustic.mean(0, keepdims=True)) / (EPS + np.std(acoustic, axis=0, keepdims=True)))\n",
    "\n",
    "    if vid in train_split:\n",
    "        train.append(((words, visual, acoustic), label, segment))\n",
    "    elif vid in dev_split:\n",
    "        dev.append(((words, visual, acoustic), label, segment))\n",
    "    elif vid in test_split:\n",
    "        test.append(((words, visual, acoustic), label, segment))\n",
    "    else:\n",
    "        print(f\"Found video that doesn't belong to any splits: {vid}\")\n",
    "\n",
    "print(f\"Total number of {num_drop} datapoints have been dropped.\")\n",
    "\n",
    "# turn off the word2id - define a named function here to allow for pickling\n",
    "def return_unk():\n",
    "    return UNK\n",
    "word2id.default_factory = return_unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the dataset\n",
    "\n",
    "Now that we have loaded the data, we can check the sizes of each split, data point shapes, vocabulary size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "16327\n1871\n4662\n(55, 35)\n(1, 7)\n[[1.        0.6666667 0.6666667 0.        0.        0.        0.6666667]]\nTotal vocab size: 15690\n"
    }
   ],
   "source": [
    "# let's see the size of each set and shape of data\n",
    "print(len(train))\n",
    "print(len(dev))\n",
    "print(len(test))\n",
    "\n",
    "print(train[0][0][1].shape)\n",
    "print(train[0][1].shape)\n",
    "print(train[0][1])\n",
    "\n",
    "print(f\"Total vocab size: {len(word2id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate function in PyTorch\n",
    "\n",
    "Collate functions are functions used by PyTorch dataloader to gather batched data from dataset. It loads multiple data points from an iterable dataset object and put them in a certain format. Here we just use the lists we've constructed as the dataset and assume PyTorch dataloader will operate on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([35, 8])\ntorch.Size([35, 8, 35])\ntorch.Size([35, 8, 74])\ntensor([[-0.6667,  0.3333,  0.0000,  0.0000,  0.0000,  0.6667,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.3333,  1.3333,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-1.3333,  0.3333,  1.0000,  0.0000,  0.0000,  0.3333,  0.0000],\n        [-0.3333,  0.0000,  0.3333,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.6667,  0.6667,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.3333,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\ntensor([35, 29, 28, 17, 16, 15,  8,  7])\n"
    }
   ],
   "source": [
    "def multi_collate(batch):\n",
    "    '''\n",
    "    Collate functions assume batch = [Dataset[i] for i in index_set]\n",
    "    '''\n",
    "    # for later use we sort the batch in descending order of length\n",
    "    batch = sorted(batch, key=lambda x: x[0][0].shape[0], reverse=True)\n",
    "    \n",
    "    # get the data out of the batch - use pad sequence util functions from PyTorch to pad things\n",
    "    labels = torch.cat([torch.from_numpy(sample[1]) for sample in batch], dim=0)\n",
    "    sentences = pad_sequence([torch.LongTensor(sample[0][0]) for sample in batch], padding_value=PAD)\n",
    "    visual = pad_sequence([torch.FloatTensor(sample[0][1]) for sample in batch])\n",
    "    acoustic = pad_sequence([torch.FloatTensor(sample[0][2]) for sample in batch])\n",
    "    \n",
    "    # lengths are useful later in using RNNs\n",
    "    lengths = torch.LongTensor([sample[0][0].shape[0] for sample in batch])\n",
    "    return sentences, visual, acoustic, labels, lengths\n",
    "\n",
    "# construct dataloaders, dev and test could use around ~X3 times batch size since no_grad is used during eval\n",
    "batch_sz = 56\n",
    "train_loader = DataLoader(train, shuffle=True, batch_size=batch_sz, collate_fn=multi_collate)\n",
    "dev_loader = DataLoader(dev, shuffle=False, batch_size=batch_sz*3, collate_fn=multi_collate)\n",
    "test_loader = DataLoader(test, shuffle=False, batch_size=batch_sz*3, collate_fn=multi_collate)\n",
    "\n",
    "# let's create a temporary dataloader just to see how the batch looks like\n",
    "temp_loader = iter(DataLoader(test, shuffle=True, batch_size=8, collate_fn=multi_collate))\n",
    "batch = next(temp_loader)\n",
    "\n",
    "print(batch[0].shape) # word vectors, padded to maxlen\n",
    "print(batch[1].shape) # visual features\n",
    "print(batch[2].shape) # acoustic features\n",
    "print(batch[3]) # labels\n",
    "print(batch[4]) # lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, numpy.float64 found",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m&lt;ipython-input-13-99f54f363cca&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mexamine_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamine_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----&gt; 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m&#39; &#39;\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexamine_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# print(&#39; &#39;.join(examine_target[idx][0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamine_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, numpy.float64 found"
     ]
    }
   ],
   "source": [
    "# Let's actually inspect the transcripts to ensure it's correct\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "examine_target = train\n",
    "idx = np.random.randint(0, len(examine_target))\n",
    "print(' '.join(list(map(lambda x: id2word[x], examine_target[idx][0][0].tolist()))))\n",
    "# print(' '.join(examine_target[idx][0]))\n",
    "print(examine_target[idx][1])\n",
    "print(examine_target[idx][2])"
   ]
  },
  {
   "source": [
    "## Define a function which returns all 3 loaders for communication with models in the other files\n",
    "\n",
    "The function will return train, dev (validation) and test dataloaders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders():\n",
    "    return train_loader, dev_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a multimodal model\n",
    "\n",
    "Here we show a simple example of late-fusion LSTM. Late-fusion refers to combining the features from different modalities at the final prediction stage, without introducing any interactions between them before that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define a simple model that can deal with multimodal variable length sequence\n",
    "class LFLSTM(nn.Module):\n",
    "    def __init__(self, input_sizes, hidden_sizes, fc1_size, output_size, dropout_rate):\n",
    "        super(LFLSTM, self).__init__()\n",
    "        self.input_size = input_sizes\n",
    "        self.hidden_size = hidden_sizes\n",
    "        self.fc1_size = fc1_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # defining modules - two layer bidirectional LSTM with layer norm in between\n",
    "        self.embed = nn.Embedding(len(word2id), input_sizes[0])\n",
    "        self.trnn1 = nn.LSTM(input_sizes[0], hidden_sizes[0], bidirectional=True)\n",
    "        self.trnn2 = nn.LSTM(2*hidden_sizes[0], hidden_sizes[0], bidirectional=True)\n",
    "        \n",
    "        self.vrnn1 = nn.LSTM(input_sizes[1], hidden_sizes[1], bidirectional=True)\n",
    "        self.vrnn2 = nn.LSTM(2*hidden_sizes[1], hidden_sizes[1], bidirectional=True)\n",
    "        \n",
    "        self.arnn1 = nn.LSTM(input_sizes[2], hidden_sizes[2], bidirectional=True)\n",
    "        self.arnn2 = nn.LSTM(2*hidden_sizes[2], hidden_sizes[2], bidirectional=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(sum(hidden_sizes)*4, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.tlayer_norm = nn.LayerNorm((hidden_sizes[0]*2,))\n",
    "        self.vlayer_norm = nn.LayerNorm((hidden_sizes[1]*2,))\n",
    "        self.alayer_norm = nn.LayerNorm((hidden_sizes[2]*2,))\n",
    "        self.bn = nn.BatchNorm1d(sum(hidden_sizes)*4)\n",
    "\n",
    "        \n",
    "    def extract_features(self, sequence, lengths, rnn1, rnn2, layer_norm):\n",
    "        packed_sequence = pack_padded_sequence(sequence, lengths)\n",
    "        packed_h1, (final_h1, _) = rnn1(packed_sequence)\n",
    "        padded_h1, _ = pad_packed_sequence(packed_h1)\n",
    "        normed_h1 = layer_norm(padded_h1)\n",
    "        packed_normed_h1 = pack_padded_sequence(normed_h1, lengths)\n",
    "        _, (final_h2, _) = rnn2(packed_normed_h1)\n",
    "        return final_h1, final_h2\n",
    "\n",
    "        \n",
    "    def fusion(self, sentences, visual, acoustic, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        sentences = self.embed(sentences)\n",
    "        \n",
    "        # extract features from text modality\n",
    "        final_h1t, final_h2t = self.extract_features(sentences, lengths, self.trnn1, self.trnn2, self.tlayer_norm)\n",
    "        \n",
    "        # extract features from visual modality\n",
    "        final_h1v, final_h2v = self.extract_features(visual, lengths, self.vrnn1, self.vrnn2, self.vlayer_norm)\n",
    "        \n",
    "        # extract features from acoustic modality\n",
    "        final_h1a, final_h2a = self.extract_features(acoustic, lengths, self.arnn1, self.arnn2, self.alayer_norm)\n",
    "\n",
    "        \n",
    "        # simple late fusion -- concatenation + normalization\n",
    "        h = torch.cat((final_h1t, final_h2t, final_h1v, final_h2v, final_h1a, final_h2a),\n",
    "                       dim=2).permute(1, 0, 2).contiguous().view(batch_size, -1)\n",
    "        return self.bn(h)\n",
    "\n",
    "    def forward(self, sentences, visual, acoustic, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        h = self.fusion(sentences, visual, acoustic, lengths)\n",
    "        h = self.fc1(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(h)\n",
    "        o = self.fc2(h)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained embeddings\n",
    "\n",
    "We define a function for loading pretrained word embeddings stored in GloVe-style file. Contextualized embeddings obviously cannot be stored and loaded this way, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that loads data from GloVe-like embedding files\n",
    "# we will add tutorials for loading contextualized embeddings later\n",
    "# 2196017 is the vocab size of GloVe here.\n",
    "\n",
    "def load_emb(w2i, path_to_embedding, embedding_size=300, embedding_vocab=2196017, init_emb=None):\n",
    "    if init_emb is None:\n",
    "        emb_mat = np.random.randn(len(w2i), embedding_size)\n",
    "    else:\n",
    "        emb_mat = init_emb\n",
    "    f = open(path_to_embedding, 'r')\n",
    "    found = 0\n",
    "    for line in tqdm_notebook(f, total=embedding_vocab):\n",
    "        content = line.strip().split()\n",
    "        vector = np.asarray(list(map(lambda x: float(x), content[-300:])))\n",
    "        word = ' '.join(content[:-300])\n",
    "        if word in w2i:\n",
    "            idx = w2i[word]\n",
    "            emb_mat[idx, :] = vector\n",
    "            found += 1\n",
    "    print(f\"Found {found} words in the embedding file.\")\n",
    "    return torch.tensor(emb_mat).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "Next we train a model. We use Adam with gradient clipping and weight decay for training, and our loss here is Mean Absolute Error (MOSI is a regression dataset). We exclude the embeddings from trainable computation graph to prevent overfitting. We also apply a early-stopping scheme with learning rate annealing based on validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1776dd6725104aee8d69a3a51bc1f125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.1386\n",
      "Validation loss: 1.371\n",
      "Current patience: 8, current trial: 3.\n",
      "Found new best model on dev set!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006124110b404defa70fba36dc69e107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8447\n",
      "Validation loss: 1.2764\n",
      "Current patience: 8, current trial: 3.\n",
      "Found new best model on dev set!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d83b5d754de485b8600d79e0391bfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7315\n",
      "Validation loss: 1.1768\n",
      "Current patience: 8, current trial: 3.\n",
      "Found new best model on dev set!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c4b111b648449f96fb73e5cdae9117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6246\n",
      "Validation loss: 1.2723\n",
      "Current patience: 8, current trial: 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8e8456352c4da3a665978d3a425709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5839\n",
      "Validation loss: 1.1591\n",
      "Current patience: 7, current trial: 3.\n",
      "Found new best model on dev set!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390ef9e360204aada617d4710b490170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5475\n",
      "Validation loss: 1.0939\n",
      "Current patience: 8, current trial: 3.\n",
      "Found new best model on dev set!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6b61e9a66a488096107d8c9380b2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4957\n",
      "Validation loss: 1.115\n",
      "Current patience: 8, current trial: 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e95171ce2c147d18aaef064f7c498a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4584\n",
      "Validation loss: 1.0815\n",
      "Current patience: 7, current trial: 3.\n",
      "Found new best model on dev set!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86670ce55198413e8d70152dcb67ee53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.427\n",
      "Validation loss: 1.0937\n",
      "Current patience: 8, current trial: 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea97e40e4bf544fbbc2ec69a72b2084e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4032\n",
      "Validation loss: 1.1297\n",
      "Current patience: 7, current trial: 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7375a34b178c44f2a5bdeb95c05e09a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4028\n",
      "Validation loss: 1.0685\n",
      "Current patience: 6, current trial: 3.\n",
      "Found new best model on dev set!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a832076cf24c4cf6a0bb7c7425bae2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4014\n",
      "Validation loss: 1.0524\n",
      "Current patience: 8, current trial: 3.\n",
      "Found new best model on dev set!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e58ace9166489081c560fc20f7813b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4073\n",
      "Validation loss: 1.0986\n",
      "Current patience: 8, current trial: 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fffd1094b534ac487a51baad9181d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3845\n",
      "Validation loss: 1.1033\n",
      "Current patience: 7, current trial: 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d8cd06000a4154b0cbc37419d0634b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3848\n",
      "Validation loss: 1.0728\n",
      "Current patience: 6, current trial: 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc3e1051b6c4afd80b9da493d662ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3926\n",
      "Validation loss: 1.1008\n",
      "Current patience: 5, current trial: 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bad5e20455433baae057f85fcc5297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3858\n",
      "Validation loss: 1.094\n",
      "Current patience: 4, current trial: 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272b85fae6e14cd7bb3f6ec701d9d3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3794\n",
      "Validation loss: 1.1458\n",
      "Current patience: 3, current trial: 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc8fd82384f4a098908715cf3b8f700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3848\n",
      "Validation loss: 1.0925\n",
      "Current patience: 2, current trial: 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecf71919604473eb0979c33492c32ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3872\n",
      "Validation loss: 1.1394\n",
      "Current patience: 1, current trial: 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d2b2a289d54467b2fcc67e0df8d188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3716\n",
      "Validation loss: 1.1504\n",
      "Current patience: 0, current trial: 3.\n",
      "Running out of patience, loading previous best model.\n",
      "Current learning rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1c4114b8a444bbbbec42ba28227455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3474\n",
      "Validation loss: 1.0385\n",
      "Current patience: 8, current trial: 2.\n",
      "Found new best model on dev set!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77618b96991543febe93129e36650e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2987\n",
      "Validation loss: 1.037\n",
      "Current patience: 8, current trial: 2.\n",
      "Found new best model on dev set!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc71bb04b33e40fcaab2b80a2dba13fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2694\n",
      "Validation loss: 1.0471\n",
      "Current patience: 8, current trial: 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b06c93355b4d2b935f6360cbe9a9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2567\n",
      "Validation loss: 1.0351\n",
      "Current patience: 7, current trial: 2.\n",
      "Found new best model on dev set!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e690223607148bdaf3f9a5abb057b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2306\n",
      "Validation loss: 1.0464\n",
      "Current patience: 8, current trial: 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aedf3a77ccc4346bf2d3244b5ef6b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2383\n",
      "Validation loss: 1.0647\n",
      "Current patience: 7, current trial: 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b505d7f8cf84ca88697386769181d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2395\n",
      "Validation loss: 1.0512\n",
      "Current patience: 6, current trial: 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e5b4e8fbe8439ea27aa8e668df08fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.224\n",
      "Validation loss: 1.0518\n",
      "Current patience: 5, current trial: 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0780ec753db4db083a5bceaa91c9f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2209\n",
      "Validation loss: 1.0604\n",
      "Current patience: 4, current trial: 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04deceb4c37246429538b469f4e6d6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2029\n",
      "Validation loss: 1.0552\n",
      "Current patience: 3, current trial: 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e10873f5744dada6e0824b06960466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2093\n",
      "Validation loss: 1.0569\n",
      "Current patience: 2, current trial: 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07aa9d339def4e5388a716a30b0f7050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2015\n",
      "Validation loss: 1.0552\n",
      "Current patience: 1, current trial: 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bf26bd5827411e812d8b0e2cfd3ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2096\n",
      "Validation loss: 1.0622\n",
      "Current patience: 0, current trial: 2.\n",
      "Running out of patience, loading previous best model.\n",
      "Current learning rate: 1.0000000000000003e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2485a53aa74e4e2db0ba56ad95b87fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2436\n",
      "Validation loss: 1.0393\n",
      "Current patience: 8, current trial: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7003eb39dfb24e279945e2626c959a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2258\n",
      "Validation loss: 1.0402\n",
      "Current patience: 7, current trial: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b333b96e403d4e5d91656643cdc8642d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2464\n",
      "Validation loss: 1.0353\n",
      "Current patience: 6, current trial: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c940ab618644e8e843fe613ec66ac99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2323\n",
      "Validation loss: 1.0411\n",
      "Current patience: 5, current trial: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0088b4a26d6c4d15a83daceff5cf7cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2406\n",
      "Validation loss: 1.0407\n",
      "Current patience: 4, current trial: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3529d31911343dd9a3205db81a6949d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2243\n",
      "Validation loss: 1.0435\n",
      "Current patience: 3, current trial: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e6536dfc154d1a85fd62af7d4f569d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2372\n",
      "Validation loss: 1.0399\n",
      "Current patience: 2, current trial: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320c2a946d344e259065b8262637522d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2314\n",
      "Validation loss: 1.0417\n",
      "Current patience: 1, current trial: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbc1a08a63c40b08c71706913ff6a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2231\n",
      "Validation loss: 1.0375\n",
      "Current patience: 0, current trial: 1.\n",
      "Running out of patience, loading previous best model.\n",
      "Current learning rate: 1.0000000000000002e-06\n",
      "Running out of patience, early stopping.\n",
      "Test set performance: 1.0627462150406664\n",
      "Test set accuracy is 0.7167883211678832\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from torch.optim import Adam, SGD\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "MAX_EPOCH = 1000\n",
    "\n",
    "text_size = 300\n",
    "visual_size = 47\n",
    "acoustic_size = 74\n",
    "\n",
    "# define some model settings and hyper-parameters\n",
    "input_sizes = [text_size, visual_size, acoustic_size]\n",
    "hidden_sizes = [int(text_size * 1.5), int(visual_size * 1.5), int(acoustic_size * 1.5)]\n",
    "fc1_size = sum(hidden_sizes) // 2\n",
    "dropout = 0.25\n",
    "output_size = 1\n",
    "curr_patience = patience = 8\n",
    "num_trials = 3\n",
    "grad_clip_value = 1.0\n",
    "weight_decay = 0.1\n",
    "\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    pretrained_emb, word2id = torch.load(CACHE_PATH)\n",
    "elif WORD_EMB_PATH is not None:\n",
    "    pretrained_emb = load_emb(word2id, WORD_EMB_PATH)\n",
    "    torch.save((pretrained_emb, word2id), CACHE_PATH)\n",
    "else:\n",
    "    pretrained_emb = None\n",
    "\n",
    "model = LFLSTM(input_sizes, hidden_sizes, fc1_size, output_size, dropout)\n",
    "if pretrained_emb is not None:\n",
    "    model.embed.weight.data = pretrained_emb\n",
    "model.embed.requires_grad = False\n",
    "optimizer = Adam([param for param in model.parameters() if param.requires_grad], weight_decay=weight_decay)\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "criterion = nn.L1Loss(reduction='sum')\n",
    "criterion_test = nn.L1Loss(reduction='sum')\n",
    "best_valid_loss = float('inf')\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "lr_scheduler.step() # for some reason it seems the StepLR needs to be stepped once first\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "for e in range(MAX_EPOCH):\n",
    "    model.train()\n",
    "    train_iter = tqdm_notebook(train_loader)\n",
    "    train_loss = 0.0\n",
    "    for batch in train_iter:\n",
    "        model.zero_grad()\n",
    "        t, v, a, y, l = batch\n",
    "        batch_size = t.size(0)\n",
    "        if CUDA:\n",
    "            t = t.cuda()\n",
    "            v = v.cuda()\n",
    "            a = a.cuda()\n",
    "            y = y.cuda()\n",
    "            l = l.cuda()\n",
    "        y_tilde = model(t, v, a, l)\n",
    "        loss = criterion(y_tilde, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_([param for param in model.parameters() if param.requires_grad], grad_clip_value)\n",
    "        optimizer.step()\n",
    "        train_iter.set_description(f\"Epoch {e}/{MAX_EPOCH}, current batch loss: {round(loss.item()/batch_size, 4)}\")\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"Training loss: {round(train_loss, 4)}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0.0\n",
    "        for batch in dev_loader:\n",
    "            model.zero_grad()\n",
    "            t, v, a, y, l = batch\n",
    "            if CUDA:\n",
    "                t = t.cuda()\n",
    "                v = v.cuda()\n",
    "                a = a.cuda()\n",
    "                y = y.cuda()\n",
    "                l = l.cuda()\n",
    "            y_tilde = model(t, v, a, l)\n",
    "            loss = criterion(y_tilde, y)\n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    valid_loss = valid_loss/len(dev)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print(f\"Validation loss: {round(valid_loss, 4)}\")\n",
    "    print(f\"Current patience: {curr_patience}, current trial: {num_trials}.\")\n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print(\"Found new best model on dev set!\")\n",
    "        torch.save(model.state_dict(), 'model.std')\n",
    "        torch.save(optimizer.state_dict(), 'optim.std')\n",
    "        curr_patience = patience\n",
    "    else:\n",
    "        curr_patience -= 1\n",
    "        if curr_patience <= -1:\n",
    "            print(\"Running out of patience, loading previous best model.\")\n",
    "            num_trials -= 1\n",
    "            curr_patience = patience\n",
    "            model.load_state_dict(torch.load('model.std'))\n",
    "            optimizer.load_state_dict(torch.load('optim.std'))\n",
    "            lr_scheduler.step()\n",
    "            print(f\"Current learning rate: {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "    \n",
    "    if num_trials <= 0:\n",
    "        print(\"Running out of patience, early stopping.\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(torch.load('model.std'))\n",
    "y_true = []\n",
    "y_pred = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for batch in test_loader:\n",
    "        model.zero_grad()\n",
    "        t, v, a, y, l = batch\n",
    "        if CUDA:\n",
    "            t = t.cuda()\n",
    "            v = v.cuda()\n",
    "            a = a.cuda()\n",
    "            y = y.cuda()\n",
    "            l = l.cuda()\n",
    "        y_tilde = model(t, v, a, l)\n",
    "        loss = criterion_test(y_tilde, y)\n",
    "        y_true.append(y_tilde.detach().cpu().numpy())\n",
    "        y_pred.append(y.detach().cpu().numpy())\n",
    "        test_loss += loss.item()\n",
    "print(f\"Test set performance: {test_loss/len(test)}\")\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "                  \n",
    "y_true_bin = y_true >= 0\n",
    "y_pred_bin = y_pred >= 0\n",
    "bin_acc = accuracy_score(y_true_bin, y_pred_bin)\n",
    "print(f\"Test set accuracy is {bin_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}