# -*- coding: utf-8 -*-
"""TextEmbedder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FuATpaV3EMweaIBrJJUa5dUAoTFfN_Vo

# Install and Import all the dependencies
"""

! pip install validators
! pip install colorama

import sys
sys.path.append('/content/drive/My Drive/HT')
from Data.constants import SDK_PATH, DATA_PATH_HIGH_LEVEL, DATA_PATH_LABELS, ALIGNED_DATA_PATH_HIGH_LEVEL, ALIGNED_DATA_PATH_LABELS, WORD_EMB_PATH, CACHE_PATH, DATA_PATH_RAW
#from constants import SDK_PATH, DATA_PATH_HIGH_LEVEL, DATA_PATH_LABELS, ALIGNED_DATA_PATH_HIGH_LEVEL, ALIGNED_DATA_PATH_LABELS, WORD_EMB_PATH, CACHE_PATH, DATA_PATH_RAW
import os
import gc
import sys

if SDK_PATH is None:
    print("SDK path is not specified! Please specify first in constants/paths.py")
    exit(0)
else:
    sys.path.append(SDK_PATH)

if not os.path.exists(SDK_PATH):
    print("Check the relative address of SDK in the constants/paths.py, current address is: ", SDK_PATH)

if not os.path.exists(DATA_PATH_HIGH_LEVEL):
    print("Check the relative address of high level features of the data in the constants/paths.py, current address is: ", DATA_PATH_HIGH_LEVEL)

if not os.path.exists(DATA_PATH_LABELS):
    print("Check the relative address of labels of the high level features in the constants/paths.py, current address is: ", DATA_PATH_LABELS)

if not os.path.exists(ALIGNED_DATA_PATH_HIGH_LEVEL):
    print("Check the relative address of aligned high level features of the data in the constants/paths.py, current address is: ", ALIGNED_DATA_PATH_HIGH_LEVEL)

if not os.path.exists(ALIGNED_DATA_PATH_LABELS):
    print("Check the relative address of aligned labels of the high level features in the constants/paths.py, current address is: ", ALIGNED_DATA_PATH_LABELS)

if not os.path.exists(DATA_PATH_RAW):
    print("Check the relative address of raw features in the constants/paths.py, current address is: ", DATA_PATH_RAW)

import re
import math
import random
import numpy as np
from mmsdk import mmdatasdk as md

"""# Load the Datasets"""

DATASET = md.cmu_mosei

data_files = os.listdir(ALIGNED_DATA_PATH_HIGH_LEVEL)
print('\n'.join(data_files))
data_files = os.listdir(DATA_PATH_RAW)
print('\n'.join(data_files))
data_files = os.listdir(ALIGNED_DATA_PATH_LABELS)
print('\n'.join(data_files))

"""##Only load visual features for this file"""

visual_field = 'FACET 4.2'
acoustic_field = 'COAVAREP'
text_field = 'glove_vectors'
word_field = 'CMU_MOSEI_TimestampedWords'

features = [
    text_field, 
    visual_field, 
    acoustic_field
]

raw_features = [word_field]

"""## Use the SDK to load the computational sequences"""

recipe = {}
recipe[word_field] = os.path.join(DATA_PATH_RAW, word_field) + '.csd'
print(recipe)

dataset = md.mmdataset(recipe)

"""## Inspect the data"""

print(list(dataset.keys()))
print("=" * 80)

word_id = list(dataset[word_field].keys())[10]
print(list(dataset[word_field][word_id].keys()))
print("=" * 80)
print(dataset[word_field].keys())

print('Intervals')
print(list(dataset[word_field][word_id]['intervals'].shape))
print("=" * 80)

print('Features')
print(list(dataset[word_field][word_id]['features'].shape))

print("Different modalities have different number of time steps!")

"""## Load the labels"""

label_field = 'All Labels'

# we add and align to lables to obtain labeled segments
# this time we don't apply collapse functions so that the temporal sequences are preserved
label_recipe = {label_field: os.path.join(ALIGNED_DATA_PATH_LABELS, label_field + '.csd')}
dataset.add_computational_sequences(label_recipe, destination=None)

"""## Split the data into train-valid-test 8:1:1"""

identifiers = list(dataset[label_field].keys())
#print(identifiers[:10])
random.shuffle(identifiers)
print(len(identifiers))

train_index = math.floor(len(identifiers)*0.8)
dev_index = math.floor(len(identifiers)*0.85)
#print(train_index)
#print(dev_index)

train_split = identifiers[:train_index]
dev_split = identifiers[train_index:dev_index]
test_split = identifiers[dev_index:]

#train_split = identifiers[:100]
#dev_split = identifiers[100:115]
#test_split = identifiers[100:130]

# inspect the splits: they only contain video IDs
print(test_split)

"""#Attention cell for feature extraction

## Install and Import dependecies
"""

import torch
import torch.nn as nn

from tqdm import tqdm 
from collections import defaultdict
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence

import torch.optim as optim

from ray import tune

from sklearn.metrics import classification_report

"""## Setup for the data loader"""

# a sentinel epsilon for safe division, without it we will replace illegal values with a constant
EPS = 0

# construct a word2id mapping that automatically takes increment when new words are encountered
word2id = defaultdict(lambda: len(word2id))
sep = word2id['[SEP]']
cls = word2id['[CLS]']
pad = word2id['[PAD]']
sp = word2id['[SP]']

# place holders for the final train/dev/test dataset
train = []
dev = []
test = []

# define a regular expression to extract the video ID out of the keys
pattern = re.compile('(.*)\[.*\]')
num_drop = 0 # a counter to count how many data points went into some processing issues

for segment in dataset[label_field].keys():
#for segment in identifiers[:130]:
    
    # get the video ID and the features out of the aligned dataset
    vid = re.search(pattern, segment).group(1)
    label = dataset[label_field][segment]['features']
    _words = dataset[word_field][vid]['features']

    # remove nan values
    label = np.nan_to_num(label)

    #print(_words.shape, label.shape, segment)

    # remove speech pause tokens - this is in general helpful

    words = []
    for i, word in enumerate(_words):
        if word[0] != b'sp':
            words.append(word2id[word[0].decode('utf-8')]) # SDK stores strings as bytes, decode into strings here

    words = np.asarray(words)

    if segment in train_split:
        ###train.append(((words, visual, acoustic), label, segment))
        train.append([words, label, segment])
    elif segment in dev_split:
        ###dev.append(((words, visual, acoustic), label, segment))
        dev.append([words, label, segment])
    elif segment in test_split:
        ###test.append(((words, visual, acoustic), label, segment))
        test.append([words, label, segment])
    else:
        print(f"Found video that doesn't belong to any splits: {vid}")

    #input('Enter: ')

print(f"Total number of {num_drop} datapoints have been dropped.")

# turn off the word2id - define a named function here to allow for pickling
def return_unk():
    return UNK
word2id.default_factory = return_unk

"""## Inspect the Data"""

# let's see the size of each set and shape of data
print(len(train))
print(len(dev))
print(len(test))

print(train[0][0].shape)
#print(train[0][0][1].shape)
#print(train[0][0][2].shape)
print(train[0][1].shape)
print(train[0][2])

print(f"Total vocab size: {len(word2id)}")

"""## Custom Collator to pad variable length visual sequences"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#device= 'cpu'

def multi_collate(batch):
    '''
    Collate functions assume batch = [Dataset[i] for i in index_set]
    '''
    # for later use we sort the batch in descending order of length
    batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)
    
    # get the data out of the batch - use pad sequence util functions from PyTorch to pad things
    labels = torch.cat([torch.from_numpy(sample[1]).to(device) for sample in batch], dim=0)
    sentences = pad_sequence([torch.LongTensor(sample[0]).to(device) for sample in batch], padding_value=pad)
    
    # lengths are useful later in using RNNs
    lengths = torch.LongTensor([sample[0].shape[0] for sample in batch])

    return sentences, labels, lengths

"""## Custom Collator to pad variable length visual sequences"""

# construct dataloaders, dev and test could use around ~X3 times batch size since no_grad is used during eval
batch_sz = 32
train_loader = DataLoader(train, shuffle=True, batch_size=batch_sz, collate_fn=multi_collate)
dev_loader = DataLoader(dev, shuffle=False, batch_size=batch_sz*3, collate_fn=multi_collate)
test_loader = DataLoader(test, shuffle=False, batch_size=batch_sz*3, collate_fn=multi_collate)

"""## Checking whether the dataloader works"""

# let's create a temporary dataloader just to see how the batch looks like
temp_loader = iter(DataLoader(train, shuffle=True, batch_size=8, collate_fn=multi_collate))
batch = next(temp_loader)

print(batch[0].shape) # word vectors
print(batch[1]) # labels 
print(batch[2]) # lengths

"""## Examining the encoded data"""

# Let's actually inspect the transcripts to ensure it's correct

id2word = {v:k for k, v in word2id.items()}
examine_target = test
idx = np.random.randint(0, len(examine_target))

print(' '.join(list(map(lambda x: id2word[x], examine_target[idx][0].tolist()))))
print(examine_target[idx][1])
print(examine_target[idx][2])

"""# Model"""

! pip install transformers

"""## Initialize the model and training parameters"""

from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW, Adafactor
from torch.optim import Adam

tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')
model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', return_dict=True, num_labels=6)

model = model.to(device)

optimizer = Adam(model.parameters(), lr=1e-6)

# Function to calculate the accuracy of our predictions vs labels
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

"""## Training settings"""

training_args_finetuning = transformers.TrainingArguments(
    overwrite_output_dir=True,
    num_train_epochs=5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    warmup_steps=150,
    weight_decay=0.01,
    learning_rate=2e-5,
    evaluate_during_training=True,
    logging_steps=100,
)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true=labels, y_pred=preds, labels=[0,1,2,3,4,5], average='macro')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

def model_init():
    return model

trainer_finetuning = transformers.Trainer(
    model=model
    model_init = model_init,
    args=training_args_finetuning,
    compute_metrics=compute_metrics,
    train_dataset=train_loader,
    eval_dataset=valid_loader
)

"""## Hyper-parameter Tuning with Ray-Tune"""

! pip install ray[tune]
! pip install optuna

trainer_finetuning.hyperparameter_search(direction="maximize")

"""## Training loop"""

trainer_finetuning = transformers.Trainer(
    model=model_finetuning,
    compute_metrics=compute_metrics,
    train_dataset=train_loader,
    eval_dataset=test_loader
)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# trainer_finetuning.train()

"""## Save the model to the local directory"""

trainer_finetuning.save_model('./saved_models/text_embedder')

tokenizer_finetuning.save_pretrained('./saved_models/text_embedder')

"""## Evaluation"""

trainer_finetuning.evaluate()

"""# Function to send embeddings to other files"""

def get_embeddings:
    return model.output_hidden_states