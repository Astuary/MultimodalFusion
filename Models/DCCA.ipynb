{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCCA.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Z6veIY7ltRKFMdtXcJ3YY5I7HpRnc2k-","authorship_tag":"ABX9TyNLW04K3uhmjXDie6qo/FKZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YskEOFLJZ7mY"},"source":["# Install and import all the dependencies"]},{"cell_type":"code","metadata":{"id":"bcJa3EWVX0oL"},"source":["# Add path to the data directory, do not include the directory name\n","import sys\n","sys.path.append('/content/drive/MyDrive/HT')\n","\n","# Fetch the relative path variables\n","from Data.constants import SDK_PATH, DATA_PATH_HIGH_LEVEL, DATA_PATH_LABELS, ALIGNED_DATA_PATH_HIGH_LEVEL, ALIGNED_DATA_PATH_LABELS, WORD_EMB_PATH, CACHE_PATH, DATA_PATH_RAW\n","\n","from Models import TextEmbedder, VisualsEmbedder, AcousticsEmbedder"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWMgeCwKbvmC"},"source":["# Get the embeddings of all three modalities"]},{"cell_type":"code","metadata":{"id":"8HGtdrLmbz-L"},"source":["text_emb = TextEmbedder.get_embeddings()\n","visual_emb = VisualsEmbedder.get_embeddings()\n","acoustic_emb = AcousticsEmbedder.get_embeddings()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ay3KNdFwdI7K"},"source":["# Deep Canonical Correlation Analysis Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P93PdRP0hksh","executionInfo":{"status":"ok","timestamp":1606678930367,"user_tz":300,"elapsed":3855,"user":{"displayName":"Kunjal Panchal","photoUrl":"","userId":"13952576303720058694"}},"outputId":"9dfee1c5-3b26-4b64-c035-d3e3c97bc2ff"},"source":["! pip install scikit-learn"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.17.0)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gs6qKIUWdWRE","executionInfo":{"status":"ok","timestamp":1606678949194,"user_tz":300,"elapsed":332,"user":{"displayName":"Kunjal Panchal","photoUrl":"","userId":"13952576303720058694"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.cross_decomposition import CCA"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p5OEBfwvf8d9"},"source":["## Encoder"]},{"cell_type":"code","metadata":{"id":"hTZp7nVxdQqc","executionInfo":{"status":"ok","timestamp":1606678824465,"user_tz":300,"elapsed":344,"user":{"displayName":"Kunjal Panchal","photoUrl":"","userId":"13952576303720058694"}}},"source":["class Encoder(nn.Module):\n","    def __init__(self, in_dim, embed_dim):\n","        super(Encoder, self).__init__()\n","\n","        # Text Modality\n","        self.linear1 = nn.Linear(in_dim, 150)\n","        self.linear2 = nn.Linear(150, 50)\n","\n","        # Visual Modality\n","        self.norm = nn.BatchNorm1d(embed_dim)\n","        self.conv1 = nn.Conv1d(1, 16, 3, 1)\n","        self.conv2 = nn.Conv1d(16, 32, 3, 1)\n","        self.linear3 = nn.Linear(int(embed_dim/4), 50)\n","\n","        # Acoustic Modality\n","        self.conv3 = nn.Conv1d(1, 16, 3, 1)\n","        self.conv4 = nn.Conv1d(16, 32, 3, 1)\n","        self.linear4 = nn.Linear(int(embed_dim/4), 50)\n","\n","    def forward(self, t, v, a):\n","\n","        # Text Modality\n","        t_ = nn.ReLU(self.linear1(t))\n","        t_ = nn.ReLU(self.linear2(t_))\n","\n","        # Visual Modality\n","        v_ = nn.ReLU(self.norm(self.conv1(v)))\n","        v_ = nn.ReLU(self.norm(self.conv2(v_)))\n","        v_ = nn.ReLU(self.linear3(v_))\n","\n","        # Acoustic Modality\n","        a_ = nn.ReLU(self.norm(self.conv3(a)))\n","        a_ = nn.ReLU(self.norm(self.conv4(a_)))\n","        a_ = nn.ReLU(self.linear4(a_))\n","\n","        return torch.cat((t_, v_, a_), dim=1)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FYLpKn8dgqIT"},"source":["## Canonical Correlation Analysis"]},{"cell_type":"code","metadata":{"id":"zL55tnQdgp0S","executionInfo":{"status":"ok","timestamp":1606678826770,"user_tz":300,"elapsed":577,"user":{"displayName":"Kunjal Panchal","photoUrl":"","userId":"13952576303720058694"}}},"source":["model = Encoder(300, 74)\n","inputs = Encoder.forward(text_emb, visual_emb, acoustic_emb)\n","\n","cca_module = CCA(n_components=3)\n","outputs = cca_module.fit_transform(inputs)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qc0n6i5Xjw_C"},"source":["# A Function to send the fused modalities"]},{"cell_type":"code","metadata":{"id":"yx3jL5LNj1QK"},"source":["def get_fusion():\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QlpKed1Nk3UE"},"source":["# Cross Fusion with standard attention module"]},{"cell_type":"markdown","metadata":{"id":"Kz9WrIxclJdm"},"source":["Credit for the base self-attention:\n","https://github.com/pbloem/former/blob/master/former/modules.py by *pbloem*\n","\n","https://discuss.pytorch.org/t/my-implementation-of-self-attention/81130 by *omer_sahban*\n","\n","https://pytorch.org/docs/master/_modules/torch/nn/modules/activation.html#MultiheadAttention\n"]},{"cell_type":"code","metadata":{"id":"2MOlZqCilHYu"},"source":["class CrossAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads = 1, dropout = 0.0):\n","        super(CrossAttention, self).__init__()\n","        self.emb_dim = emb_dim\n","        self.num_heads = num_heads\n","\n","        self.w_k = nn.Linear(emb_dim, emb_dim * num_heads, bias=False)\n","        self.w_q = nn.Linear(emb_dim, emb_dim * num_heads, bias=False)\n","        self.w_v = nn.Linear(emb_dim, emb_dim * num_heads, bias=False)\n","        self.w_out = nn.Linear(emb_dim * num_heads, emb_dim)\n","\n","    def forward(self, x):\n","\n","        b, t, _ = x.shape\n","        e = self.emb_dim\n","        h = self.num_heads\n","        keys = self.w_k(x).view(b, t, h, e)\n","        values = self.w_v(x).view(b, t, h, e)\n","        queries = self.w_q(x).view(b, t, h, e)\n","\n","        keys = keys.transpose(2, 1)\n","        queries = queries.transpose(2, 1)\n","        values = values.transpose(2, 1)\n","\n","        dot = queries @ keys.transpose(3, 2)\n","        dot = dot / np.sqrt(e)\n","        dot = F.softmax(dot, dim=3)\n","\n","        out = dot @ values\n","        out = out.transpose(1,2).contiguous().view(b, t, h * e)\n","        out = self.w_out(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aNZmXSHYmB2V"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"BJcyO17JnCyg"},"source":["from tqdm import tqdm \n","from collections import defaultdict\n","from torch.utils.data import DataLoader, Dataset\n","\n","import torch.optim as optim\n","\n","from ray import tune\n","\n","from sklearn.metrics import classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3VL8eUf-mtkb"},"source":["## Initialize the model and training parameters"]},{"cell_type":"code","metadata":{"id":"0dAXv7QwmEmD"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_crossattn = CrossAttention(50)\n","model_crossattn = model_crossattn.to(device)\n","\n","epochs = 4\n","\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3L2aruXzn7Wp"},"source":["## Hyper-parameter Tuning with Ray-Tune"]},{"cell_type":"code","metadata":{"id":"xZnu3b3Dn8IB"},"source":["for i in range(10):\n","    train(modemodel_crossattnl, optimizer, train_loader)\n","    acc = test(model_crossattn, valid_loader)\n","    tune.report(mean_accuracy=acc)\n","\n","analysis = tune.run(train_loader, config={\"lr\": tune.grid_search([0.001, 0.01, 0.1])})\n","\n","print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))\n","\n","# Get a dataframe for analyzing trial results.\n","df = analysis.dataframe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Jy4v3Map2Em"},"source":["## Train the model"]},{"cell_type":"code","metadata":{"id":"KaeqI00zpz15"},"source":["model_crossattn.train()\n","\n","for epoch in range(epochs):\n","    print(\"Epoch \", epoch)\n","\n","    with tqdm(train_loader, unit=\"batch\") as tepoch:\n","        for inputs, labels, lengths in tepoch:\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","\n","            labels = torch.argmax(labels[:,1:], dim= 1)\n","            inputs = torch.transpose(inputs, 0, 1).unsqueeze(1)\n","            inputs = inputs.reshape(inputs.shape[0], 1, -1)\n","\n","            print(inputs.shape)\n","            print(labels.shape)\n","\n","\n","            outputs = model_crossattn(inputs)\n","\n","            print(outputs[0].shape, outputs[1].shape)\n","            \n","            logits = outputs[0].squeeze(0)\n","            print(logits.shape)\n","\n","            loss = criterion(logits, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            #input()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jju1KQLBp-kK"},"source":["## Evaluate the model"]},{"cell_type":"code","metadata":{"id":"LPX_d1zop_M1"},"source":["model_crossattn.eval()\n","\n","for epoch in range(epochs):\n","    print(\"Epoch \", epoch)\n","\n","    with tqdm(test_loader, unit=\"batch\") as tepoch:\n","        for inputs, labels, lengths in tepoch:\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","\n","            with torch.no_grad():\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                labels = torch.argmax(labels[:,1:], dim= 1)\n","                inputs = torch.transpose(inputs, 0, 1).unsqueeze(1)\n","                inputs = inputs.reshape(inputs.shape[0], 1, -1)\n","\n","                print(inputs.shape)\n","                print(labels.shape)\n","\n","                outputs = model_crossattn(inputs)\n","\n","                print(outputs[0].shape, outputs[1].shape)\n","                \n","                logits = outputs[0].squeeze(0)\n","                print(logits.shape)\n","\n","                loss = criterion(logits, labels)\n","\n","                sm = nn.Softmax(1)\n","                y_pred = torch.argmax(sm(logits), 1)\n","                print(y_pred.shape)\n","\n","                print(classification_report(labels, y_pred))\n","\n","                #input()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-01sln6eqs3p"},"source":["# If we just want to use the fused embeddings to directly classify"]},{"cell_type":"code","metadata":{"id":"ytEiXeGRqyhM"},"source":["logits = model.forward(outputs)\n","\n","sm = nn.Softmax(1)\n","pred = sm(logits)\n","\n","labels = {0: \"Happy\", 1: \"Sad\", 2: \"Anger\", 3: \"Surprise\", 4: \"Disgust\", 5: \"Fear\"}\n","class_label = torch.argmax(pred, 1)\n","\n","print(\"Predicted Label:\", class_label, \"Corresponding Class:\", labels[class_label])"],"execution_count":null,"outputs":[]}]}