{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeltaMultiheadAttention.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyM1ZchRzohg4SZMf60SYwG4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CfXX5-VO-Bfu"},"source":["# Import all the dependencies"]},{"cell_type":"code","metadata":{"id":"UJkWrNk3523u"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TcUTm1jtIzNF"},"source":["# Updated Self-Attention module with relative position embeddings"]},{"cell_type":"markdown","metadata":{"id":"DzJ5sr0lI_iB"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","Credit for the base self-attention:\n","https://github.com/pbloem/former/blob/master/former/modules.py by *pbloem*\n","\n","https://discuss.pytorch.org/t/my-implementation-of-self-attention/81130 by *omer_sahban*\n","\n","https://pytorch.org/docs/master/_modules/torch/nn/modules/activation.html#MultiheadAttention\n"]},{"cell_type":"code","metadata":{"id":"jLW004t7-IW0"},"source":["class DeltaMultiheadAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads = 1, dropout = 0.0):\n","        super(DeltaMultiheadAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.dropout = dropout\n","\n","        self.w_k = nn.Linear(emb_dim, emb_dim * num_heads, bias=False)\n","        self.w_q = nn.Linear(emb_dim, emb_dim * num_heads, bias=False)\n","        self.w_v = nn.Linear(emb_dim, emb_dim * num_heads, bias=False)\n","        self.w_r = nn.Linear(emb_dim, emb_dim * num_heads, bias=False)\n","        self.w_out = nn.Linear(emb_dim * num_heads, emb_dim)\n","\n","    def forward(self, queries, keys, values):\n","\n","        b, t, e = keys.shape\n","        h = self.num_heads\n","\n","        keys = keys.transpose(2, 1).unsqueeze(-1)\n","        keys = keys.repeat(b, t, e, h)\n","        queries = queries.transpose(2, 1).unsqueeze(-1)\n","        queries = queries.repeat(b, t, e, h)\n","        values = values.transpose(2, 1).unsqueeze(-1)\n","        values = values.repeat(b, t, e, h)\n","\n","        relational_pos_embed = self.w_r(queries).view(b, t, h, e)\n","\n","        dot = queries @ keys.transpose(3, 2)\n","        dot = dot / np.sqrt(e) + relational_pos_embed\n","        dot = F.softmax(dot, dim=3)\n","\n","        out = dot @ values\n","        out = out.transpose(1,2).contiguous().view(b, t, h * e)\n","        out = self.w_out(out)\n","        return out"],"execution_count":null,"outputs":[]}]}