{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AcousticsEmbedder.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1TFZeLEP_RM0NGoamZFmKVddJO98QYXqE","authorship_tag":"ABX9TyPRevOyK5pBmRNllhY1mBua"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1Sdwo8yXLxqj"},"source":["# Install and Import all the dependencies"]},{"cell_type":"code","metadata":{"id":"ucm53nVafR2J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605096980096,"user_tz":300,"elapsed":6299,"user":{"displayName":"Kunjal Panchal","photoUrl":"","userId":"13952576303720058694"}},"outputId":"aedaf9cb-cc2d-498f-d45f-9965a7ddcd12"},"source":["! pip install validators\n","! pip install colorama\n","\n","import sys\n","sys.path.append('/content/drive/My Drive/HT')\n","from Data.constants import SDK_PATH, DATA_PATH_HIGH_LEVEL, DATA_PATH_LABELS, ALIGNED_DATA_PATH_HIGH_LEVEL, ALIGNED_DATA_PATH_LABELS, WORD_EMB_PATH, CACHE_PATH, DATA_PATH_RAW\n","#from constants import SDK_PATH, DATA_PATH_HIGH_LEVEL, DATA_PATH_LABELS, ALIGNED_DATA_PATH_HIGH_LEVEL, ALIGNED_DATA_PATH_LABELS, WORD_EMB_PATH, CACHE_PATH, DATA_PATH_RAW\n","import os\n","import gc\n","import sys\n","\n","if SDK_PATH is None:\n","    print(\"SDK path is not specified! Please specify first in constants/paths.py\")\n","    exit(0)\n","else:\n","    sys.path.append(SDK_PATH)\n","\n","if not os.path.exists(SDK_PATH):\n","    print(\"Check the relative address of SDK in the constants/paths.py, current address is: \", SDK_PATH)\n","\n","if not os.path.exists(DATA_PATH_HIGH_LEVEL):\n","    print(\"Check the relative address of high level features of the data in the constants/paths.py, current address is: \", DATA_PATH_HIGH_LEVEL)\n","\n","if not os.path.exists(DATA_PATH_LABELS):\n","    print(\"Check the relative address of labels of the high level features in the constants/paths.py, current address is: \", DATA_PATH_LABELS)\n","\n","if not os.path.exists(ALIGNED_DATA_PATH_HIGH_LEVEL):\n","    print(\"Check the relative address of aligned high level features of the data in the constants/paths.py, current address is: \", ALIGNED_DATA_PATH_HIGH_LEVEL)\n","\n","if not os.path.exists(ALIGNED_DATA_PATH_LABELS):\n","    print(\"Check the relative address of aligned labels of the high level features in the constants/paths.py, current address is: \", ALIGNED_DATA_PATH_LABELS)\n","\n","if not os.path.exists(DATA_PATH_RAW):\n","    print(\"Check the relative address of raw features in the constants/paths.py, current address is: \", DATA_PATH_RAW)\n","\n","import re\n","import math\n","import random\n","import numpy as np\n","from mmsdk import mmdatasdk as md"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: validators in /usr/local/lib/python3.6/dist-packages (0.18.1)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from validators) (1.15.0)\n","Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from validators) (4.4.2)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (0.4.4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ew20Dck4L781"},"source":["# Load the Datasets"]},{"cell_type":"code","metadata":{"id":"tX5SJedXgDtK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605096980097,"user_tz":300,"elapsed":6284,"user":{"displayName":"Kunjal Panchal","photoUrl":"","userId":"13952576303720058694"}},"outputId":"3ee9f8b4-75ae-4579-8134-4acf319fb64f"},"source":["DATASET = md.cmu_mosei\n","\n","data_files = os.listdir(ALIGNED_DATA_PATH_HIGH_LEVEL)\n","print('\\n'.join(data_files))\n","data_files = os.listdir(DATA_PATH_RAW)\n","print('\\n'.join(data_files))\n","data_files = os.listdir(ALIGNED_DATA_PATH_LABELS)\n","print('\\n'.join(data_files))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["FACET 4.2.csd\n","glove_vectors.csd\n","COAVAREP.csd\n","OpenSMILE.csd\n","OpenFace_2.0.csd\n","CMU_MOSEI_TimestampedWords.csd\n","All Labels.csd\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NGNllBJOL-n_"},"source":["##Only load visual features for this file"]},{"cell_type":"code","metadata":{"id":"tI3tLbb8gDll"},"source":["visual_field = 'FACET 4.2'\n","acoustic_field = 'COAVAREP'\n","text_field = 'glove_vectors'\n","word_field = 'CMU_MOSEI_TimestampedWords'\n","\n","features = [\n","    #text_field, \n","    #visual_field, \n","    acoustic_field\n","]\n","\n","#raw_features = [word_field]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eh_Y0i8HMCGZ"},"source":["## Use the SDK to load the computational sequences"]},{"cell_type":"code","metadata":{"id":"6lfByRRfi-pc"},"source":["recipe = {feat: os.path.join(ALIGNED_DATA_PATH_HIGH_LEVEL, feat) + '.csd' for feat in features}\n","print(recipe)\n","\n","dataset = md.mmdataset(recipe)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RIALUt8DMF9g"},"source":["## Inspect the data"]},{"cell_type":"code","metadata":{"id":"BCJTTh2Sj8uD"},"source":["print(list(dataset.keys()))\n","print(\"=\" * 80)\n","\n","print(list(dataset[acoustic_field].keys())[:10])\n","print(\"=\" * 80)\n","\n","some_id = list(dataset[acoustic_field].keys())[15]\n","print(list(dataset[acoustic_field][some_id].keys()))\n","print(\"=\" * 80)\n","\n","word_id = list(dataset[acoustic_field].keys())[15]\n","print(list(dataset[acoustic_field][word_id].keys()))\n","print(\"=\" * 80)\n","print(dataset[acoustic_field].keys())\n","\n","print('Intervals')\n","print(list(dataset[acoustic_field][some_id]['intervals'].shape))\n","print(\"=\" * 80)\n","\n","print('Features')\n","print(list(dataset[acoustic_field][some_id]['features'].shape))\n","\n","print(\"Different modalities have different number of time steps!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1eJB4gJaMKKN"},"source":["## Load the labels"]},{"cell_type":"code","metadata":{"id":"ELOA36RukWwX"},"source":["label_field = 'All Labels'\n","\n","# we add and align to lables to obtain labeled segments\n","# this time we don't apply collapse functions so that the temporal sequences are preserved\n","label_recipe = {label_field: os.path.join(ALIGNED_DATA_PATH_LABELS, label_field + '.csd')}\n","dataset.add_computational_sequences(label_recipe, destination=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZ3ZovB_MN1i"},"source":["## Split the data into train-valid-test 8:1:1"]},{"cell_type":"code","metadata":{"id":"dRKQPBUbktwh"},"source":["identifiers = list(dataset[label_field].keys())\n","#print(identifiers[:10])\n","random.shuffle(identifiers)\n","print(len(identifiers))\n","\n","train_index = math.floor(len(identifiers)*0.8)\n","dev_index = math.floor(len(identifiers)*0.9)\n","test_index = len(identifiers)\n","#print(train_index)\n","#print(dev_index)\n","\n","## Just for testing puposes, we pick out a small cunk of samples\n","train_index = 100\n","dev_index = 120\n","test_index = 140\n","\n","train_split = identifiers[:train_index]\n","dev_split = identifiers[train_index:dev_index]\n","test_split = identifiers[dev_index:test_index]\n","\n","# inspect the splits: they only contain video IDs\n","print(test_split)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tplwd9e6MVfI"},"source":["#Attention cell for feature extraction"]},{"cell_type":"markdown","metadata":{"id":"KMWqIi7hMWTA"},"source":["## Install and Import dependecies "]},{"cell_type":"code","metadata":{"id":"S3WCHl-3MY-6"},"source":["! pip install ray"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IuVroLPYlZx6"},"source":["import torch\n","import torch.nn as nn\n","\n","from tqdm import tqdm \n","from collections import defaultdict\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","\n","import torch.optim as optim\n","\n","from ray import tune\n","\n","from sklearn.metrics import classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tfYTnmunNlz_"},"source":["## Setup for the data loader"]},{"cell_type":"code","metadata":{"id":"LgJArHpYl0AH"},"source":["# a sentinel epsilon for safe division, without it we will replace illegal values with a constant\n","EPS = 0\n","\n","# place holders for the final train/dev/test dataset\n","train = []\n","dev = []\n","test = []\n","\n","# define a regular expression to extract the video ID out of the keys\n","pattern = re.compile('(.*)\\[.*\\]')\n","num_drop = 0 # a counter to count how many data points went into some processing issues"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QWDPvJMl4td"},"source":["#for segment in dataset[label_field].keys():\n","for segment in identifiers[:test_index]:\n","    \n","    # get the video ID and the features out of the aligned dataset\n","    vid = re.search(pattern, segment).group(1)\n","    label = dataset[label_field][segment]['features']\n","    _acoustics = dataset[acoustic_field][segment]['features']\n","\n","    # remove nan values\n","    label = np.nan_to_num(label)\n","    _acoustics = np.nan_to_num(_acoustics)\n","\n","    print(\"Acoustics data shape:\", _acoustics.shape, \"Labels:\", label.shape, \"Segment ID:\", segment)\n","\n","    acoustics = np.asarray(_acoustics)\n","\n","    # z-normalization per instance and remove nan/infs\n","    acoustics = np.nan_to_num((acoustics - acoustics.mean(0, keepdims=True)) / (EPS + np.std(acoustics, axis=0, keepdims=True)))\n","\n","    if segment in train_split:\n","        ###train.append(((words, visual, acoustic), label, segment))\n","        train.append([acoustics, label, segment])\n","    elif segment in dev_split:\n","        ###dev.append(((words, visual, acoustic), label, segment))\n","        dev.append([acoustics, label, segment])\n","    elif segment in test_split:\n","        ###test.append(((words, visual, acoustic), label, segment))\n","        test.append([acoustics, label, segment])\n","    else:\n","        print(f\"Found video that doesn't belong to any splits: {vid}\")\n","\n","    #input('Enter: ')\n","\n","print(f\"Total number of {num_drop} datapoints have been dropped.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J0OQ1ZJhNhz6"},"source":["## Inspect the Data"]},{"cell_type":"code","metadata":{"id":"1Isv0WOln2b4"},"source":["# let's see the size of each set and shape of data\n","print(len(train))\n","print(len(dev))\n","print(len(test))\n","\n","print(train[0][0].shape)\n","print(train[0][1].shape)\n","print(train[0][2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S8XNklv_Nflo"},"source":["## Custom Collator to pad variable length visual sequences"]},{"cell_type":"code","metadata":{"id":"NZxeo0B3obYs"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def multi_collate(batch):\n","    '''\n","    Collate functions assume batch = [Dataset[i] for i in index_set]\n","    '''\n","    # for later use we sort the batch in descending order of length\n","    batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n","    \n","    # get the data out of the batch - use pad sequence util functions from PyTorch to pad things\n","    labels = torch.cat([torch.from_numpy(sample[1]).to(device) for sample in batch], dim=0)\n","    acoustic = pad_sequence([torch.FloatTensor(sample[0]) for sample in batch])\n","    \n","    # lengths are useful later in using RNNs\n","    lengths = torch.LongTensor([sample[0].shape[0] for sample in batch])\n","\n","    return acoustic, labels, lengths"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GA_NegCDNdPo"},"source":["## Create data-loaders"]},{"cell_type":"code","metadata":{"id":"vwhSXG0Rp1Uj"},"source":["# construct dataloaders, dev and test could use around ~X3 times batch size since no_grad is used during eval\n","batch_sz = 32\n","train_loader = DataLoader(train, shuffle=True, batch_size=batch_sz, collate_fn=multi_collate)\n","dev_loader = DataLoader(dev, shuffle=False, batch_size=batch_sz*3, collate_fn=multi_collate)\n","test_loader = DataLoader(test, shuffle=False, batch_size=batch_sz*3, collate_fn=multi_collate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-LQoVd4TNbI5"},"source":["## Checking whether the dataloader works"]},{"cell_type":"code","metadata":{"id":"SyGRmz5Pp4d7"},"source":["# let's create a temporary dataloader just to see how the batch looks like\n","temp_loader = iter(DataLoader(train, shuffle=True, batch_size=8, collate_fn=multi_collate))\n","batch = next(temp_loader)\n","\n","print(batch[0].shape) # acoustics vectors\n","print(batch[1]) # labels \n","print(batch[2]) # lengths"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8RTzRQOkN-S-"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"8nx3FFwS6W7y"},"source":["class DeltaSelfAttention(nn.Module):\n","\n","    def __init__(self, in_dim, embed_dim, num_heads, dropout=0.0):\n","        super(DeltaSelfAttention, self).__init__()\n","        \n","        self.embedding_dimensions = embed_dim\n","        self.number_heads = num_heads\n","        self.in_dim = in_dim\n","        hid_dim = 1000\n","\n","        self.pool = nn.AvgPool1d(2)\n","        self.conv = nn.Conv1d(1, 8, 5, 2)\n","        self.linear1 = nn.Linear(in_dim, hid_dim)\n","        self.linear2 = nn.Linear(hid_dim, embed_dim*2)\n","        \n","        try:\n","            self.mha = DeltaMultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)\n","        except:\n","            self.mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)\n","        \n","        self.key_linear = nn.Linear(embed_dim*2, embed_dim)\n","        self.query_linear = nn.Linear(embed_dim*2, embed_dim)\n","        self.value_linear = nn.Linear(embed_dim*2, embed_dim)\n","    \n","    def forward(self, inputs):\n","        x = self.pool(self.conv(inputs))\n","        x = x.reshape(x.shape[0], -1)[:, :self.in_dim]\n","        print(x.shape)\n","        x = self.linear1(x)\n","        print(x.shape)\n","        x = self.linear2(x)\n","        print(x.shape)\n","        q = self.query_linear(x).unsqueeze(0)\n","        k = self.key_linear(x).unsqueeze(0)\n","        v = self.value_linear(x).unsqueeze(0)\n","        print(q.shape)\n","        print(k.shape)\n","        print(v.shape)\n","        x = self.mha(q, k, v)\n","        print(x[0].shape)\n","\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hPp6WMmXOClR"},"source":["## Initialize the model and training parameters"]},{"cell_type":"code","metadata":{"id":"8rlqSjg4OE3c"},"source":["model = DeltaSelfAttention(20000, 30, 3)\n","model = model.to(device)\n","\n","epochs = 4\n","\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xx73sGfPOIJ0"},"source":["## Hyper-parameter Tuning with Ray-Tune"]},{"cell_type":"code","metadata":{"id":"VkWFf0HaOJt4"},"source":["for i in range(10):\n","    train(model, optimizer, train_loader)\n","    acc = test(model, valid_loader)\n","    tune.report(mean_accuracy=acc)\n","\n","analysis = tune.run(train_loader, config={\"lr\": tune.grid_search([0.001, 0.01, 0.1])})\n","\n","print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))\n","\n","# Get a dataframe for analyzing trial results.\n","df = analysis.dataframe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8qED7Z-ROLn9"},"source":["torch.save(model.state_dict(), LOADER_PATH + 'acoustic_embedder')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RB4GoVwrONxH"},"source":["## Train the model"]},{"cell_type":"code","metadata":{"id":"y_MaiZ3NOQou"},"source":["model = torch.load(LOADER_PATH + 'acoustic_embedder')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_5RIw-N6Yi_"},"source":["model.train()\n","\n","for epoch in range(epochs):\n","    print(\"Epoch \", epoch)\n","\n","    with tqdm(train_loader, unit=\"batch\") as tepoch:\n","        for inputs, labels, lengths in tepoch:\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","\n","            labels = torch.argmax(labels[:,1:], dim= 1)\n","            inputs = torch.transpose(inputs, 0, 1).unsqueeze(1)\n","            inputs = inputs.reshape(inputs.shape[0], 1, -1)\n","\n","            print(inputs.shape)\n","            print(labels.shape)\n","\n","\n","            outputs = model(inputs)\n","\n","            print(outputs[0].shape, outputs[1].shape)\n","            \n","            logits = outputs[0].squeeze(0)\n","            print(logits.shape)\n","\n","            loss = criterion(logits, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            #input()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2onUiLpwOW0o"},"source":["## Evaluate the model"]},{"cell_type":"code","metadata":{"id":"NGCq-YUxOYmg"},"source":["model.eval()\n","\n","for epoch in range(epochs):\n","    print(\"Epoch \", epoch)\n","\n","    with tqdm(test_loader, unit=\"batch\") as tepoch:\n","        for inputs, labels, lengths in tepoch:\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","\n","            with torch.no_grad():\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                labels = torch.argmax(labels[:,1:], dim= 1)\n","                inputs = torch.transpose(inputs, 0, 1).unsqueeze(1)\n","                inputs = inputs.reshape(inputs.shape[0], 1, -1)\n","\n","                print(inputs.shape)\n","                print(labels.shape)\n","\n","                outputs = model(inputs)\n","\n","                print(outputs[0].shape, outputs[1].shape)\n","                \n","                logits = outputs[0].squeeze(0)\n","                print(logits.shape)\n","\n","                loss = criterion(logits, labels)\n","\n","                sm = nn.Softmax(1)\n","                y_pred = torch.argmax(sm(logits), 1)\n","                print(y_pred.shape)\n","\n","                print(classification_report(labels, y_pred))\n","\n","                #input()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zo2d9C6dVOoa"},"source":["# Function to send embeddings to other files"]},{"cell_type":"code","metadata":{"id":"jB4zhjeoVOWq"},"source":["def get_embeddings:\n","    return model.output_hidden_states "],"execution_count":null,"outputs":[]}]}