{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextEmbedderXLNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vISiLIheBVdk",
        "outputId": "7247d17c-664b-4ea0-88bd-3c1bc67db73e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "! pip install validators\n",
        "! pip install colorama\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/HT')\n",
        "from Data.constants import SDK_PATH, DATA_PATH_HIGH_LEVEL, DATA_PATH_LABELS, ALIGNED_DATA_PATH_HIGH_LEVEL, ALIGNED_DATA_PATH_LABELS, WORD_EMB_PATH, CACHE_PATH, DATA_PATH_RAW\n",
        "#from constants import SDK_PATH, DATA_PATH_HIGH_LEVEL, DATA_PATH_LABELS, ALIGNED_DATA_PATH_HIGH_LEVEL, ALIGNED_DATA_PATH_LABELS, WORD_EMB_PATH, CACHE_PATH, DATA_PATH_RAW\n",
        "import os\n",
        "import gc\n",
        "import sys\n",
        "\n",
        "if SDK_PATH is None:\n",
        "    print(\"SDK path is not specified! Please specify first in constants/paths.py\")\n",
        "    exit(0)\n",
        "else:\n",
        "    sys.path.append(SDK_PATH)\n",
        "\n",
        "if not os.path.exists(SDK_PATH):\n",
        "    print(\"Check the relative address of SDK in the constants/paths.py, current address is: \", SDK_PATH)\n",
        "\n",
        "if not os.path.exists(DATA_PATH_HIGH_LEVEL):\n",
        "    print(\"Check the relative address of high level features of the data in the constants/paths.py, current address is: \", DATA_PATH_HIGH_LEVEL)\n",
        "\n",
        "if not os.path.exists(DATA_PATH_LABELS):\n",
        "    print(\"Check the relative address of labels of the high level features in the constants/paths.py, current address is: \", DATA_PATH_LABELS)\n",
        "\n",
        "if not os.path.exists(ALIGNED_DATA_PATH_HIGH_LEVEL):\n",
        "    print(\"Check the relative address of aligned high level features of the data in the constants/paths.py, current address is: \", ALIGNED_DATA_PATH_HIGH_LEVEL)\n",
        "\n",
        "if not os.path.exists(ALIGNED_DATA_PATH_LABELS):\n",
        "    print(\"Check the relative address of aligned labels of the high level features in the constants/paths.py, current address is: \", ALIGNED_DATA_PATH_LABELS)\n",
        "\n",
        "if not os.path.exists(DATA_PATH_RAW):\n",
        "    print(\"Check the relative address of raw features in the constants/paths.py, current address is: \", DATA_PATH_RAW)\n",
        "\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from mmsdk import mmdatasdk as md"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: validators in /usr/local/lib/python3.6/dist-packages (0.18.1)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from validators) (4.4.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from validators) (1.15.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (0.4.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaSuOcytDSM-",
        "outputId": "80a53c09-6500-47a4-dd82-97f38633614f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "DATASET = md.cmu_mosei\n",
        "\n",
        "data_files = os.listdir(ALIGNED_DATA_PATH_HIGH_LEVEL)\n",
        "print('\\n'.join(data_files))\n",
        "data_files = os.listdir(DATA_PATH_RAW)\n",
        "print('\\n'.join(data_files))\n",
        "data_files = os.listdir(ALIGNED_DATA_PATH_LABELS)\n",
        "print('\\n'.join(data_files))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FACET 4.2.csd\n",
            "glove_vectors.csd\n",
            "COAVAREP.csd\n",
            "OpenSMILE.csd\n",
            "OpenFace_2.0.csd\n",
            "CMU_MOSEI_TimestampedWords.csd\n",
            "All Labels.csd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zENMg7wPEhSY"
      },
      "source": [
        "visual_field = 'FACET 4.2'\n",
        "acoustic_field = 'COAVAREP'\n",
        "text_field = 'glove_vectors'\n",
        "word_field = 'CMU_MOSEI_TimestampedWords'\n",
        "\n",
        "features = [\n",
        "    text_field, \n",
        "    visual_field, \n",
        "    acoustic_field\n",
        "]\n",
        "\n",
        "raw_features = [word_field]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SNwa_W5FEYX",
        "outputId": "51487d35-b554-41fd-a1db-adeebf7248ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "recipe = {}\n",
        "recipe[word_field] = os.path.join(DATA_PATH_RAW, word_field) + '.csd'\n",
        "print(recipe)\n",
        "\n",
        "dataset = md.mmdataset(recipe)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'CMU_MOSEI_TimestampedWords': '/content/drive/My Drive/HT/Dataset/RawData/CMU_MOSEI_TimestampedWords.csd'}\n",
            "\u001b[92m\u001b[1m[2020-10-25 12:44:59.077] | Success | \u001b[0mComputational sequence read from file /content/drive/My Drive/HT/Dataset/RawData/CMU_MOSEI_TimestampedWords.csd ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|‚ñè         | 65/3837 [00:00<00:06, 603.07 Computational Sequence Entries/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[94m\u001b[1m[2020-10-25 12:45:01.516] | Status  | \u001b[0mChecking the integrity of the <words> computational sequence ...\n",
            "\u001b[94m\u001b[1m[2020-10-25 12:45:01.516] | Status  | \u001b[0mChecking the format of the data in <words> computational sequence ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                                                  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m\u001b[1m[2020-10-25 12:45:03.888] | Success | \u001b[0m<words> computational sequence data in correct format.\n",
            "\u001b[94m\u001b[1m[2020-10-25 12:45:03.888] | Status  | \u001b[0mChecking the format of the metadata in <words> computational sequence ...\n",
            "\u001b[92m\u001b[1m[2020-10-25 12:45:03.888] | Success | \u001b[0m<words> computational sequence metadata in correct format.\n",
            "\u001b[92m\u001b[1m[2020-10-25 12:45:03.888] | Success | \u001b[0m<words> computational sequence is valid!\n",
            "\u001b[92m\u001b[1m[2020-10-25 12:45:03.888] | Success | \u001b[0mDataset initialized successfully ... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYTiVKTuFUYI",
        "outputId": "5b359673-e42d-47bc-d2e9-e5d6a61e6b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "print(list(dataset.keys()))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "word_id = list(dataset[word_field].keys())[10]\n",
        "print(list(dataset[word_field][word_id].keys()))\n",
        "print(\"=\" * 80)\n",
        "print(dataset[word_field].keys())\n",
        "\n",
        "print('Intervals')\n",
        "print(list(dataset[word_field][word_id]['intervals'].shape))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print('Features')\n",
        "print(list(dataset[word_field][word_id]['features'].shape))\n",
        "\n",
        "print(\"Different modalities have different number of time steps!\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CMU_MOSEI_TimestampedWords']\n",
            "================================================================================\n",
            "['features', 'intervals']\n",
            "================================================================================\n",
            "dict_keys(['--qXJuDtHPw', '-3g5yACwYnA', '-3nNcZdcdvU', '-571d8cVauQ', '-6rXp3zJ3kc', '-9YyBTjo1zo', '-9y-fZ3swSY', '-AUZQgSxyPQ', '-Alixo7euuU', '-Eqdz5y4pEY', '-HeZS2-Prhc', '-HvKLjmsO5U', '-HwX2H8Z4hY', '-IUUR2yyNbw', '-I_e4mIh0yE', '-IqSFQePnpU', '-KCahx2qBOI', '-LnuDPiuuZw', '-MeTTeMJBNc', '-NFrJFQijFE', '-RfYyzHpjk4', '-RpZEe4w4fY', '-SYSVSQnTnA', '-THoVjtIkeU', '-UUCSKoHeMA', '-UacrmKiTn4', '-UuX1xuaiiE', '-VmheDA92mM', '-WXXTNIJcVM', '-ZgjBOA1Yhw', '-a55Q6RWvTA', '-aNfi7CP8vM', '-aqamKhZ1Ec', '-bl5PfNIYrk', '-cEhr0cQcDM', '-cmk6cfUeMs', '-dZ1TCboxcQ', '-dxfTGcXJoc', '-egA8-b7-3M', '-hPfPhUIzfA', '-hnBHBN8p5A', '-iRBcNs9oI8', '-l_53IwQoj0', '-lqc32Zpr7M', '-lzEya4AM_4', '-m9KtvCk_L8', '-mJ2ud6oKI8', '-mqbVkbCndg', '-o_667Ny7RM', '-qDkUB0GgYY', '-qMhGeJkp5o', '-ri04Z7vwnc', '-rxZxtG0xmY', '-s9qJ7ATP7w', '-t217m2on-s', '-tANM6ETl_M', '-tPCytz4rww', '-uywlfIYOS8', '-vxjVxOeScU', '-wMB_hJL-3o', '-wdUIwjDMfg', '-wny0OAz3g8', '-yRb-Jum7EQ', '03X1FwF6udc', '06st0Ez_E_I', '07Z16yRBFUQ', '08d4NTXkSxw', '0AA-wmk8WdA', '0AkGtPzl7D8', '0BVed2nBq1g', '0DBfvvIVmqY', '0EAAfLCQabY', '0EphpADwdPg', '0FA32bEZ6xI', '0Fqav67TDEw', '0JaYazphxfM', '0JuRmhhgMw8', '0K7dCp80n9c', '0OC3wf9x3zQ', '0OXeN3MhFeA', '0PlQc98SccA', '0SfejBuLgFo', '0Svu5BldR5A', '0W4WsjhDgUI', '0WIwQgH4lKg', '0Xs8wJrAmxs', '0YdYFtVdlWE', '0YiAKqU36KE', '0bsTzavb-AE', '0bxhZ-LIfZY', '0cBmQ_ItVUc', '0eTibWQdO5M', '0hCOCt6fWas', '0hCihQ5Kep0', '0jtdrsmVzYQ', '0mXv97u2OCM', '0nJrksJ7azM', '0p9XKC_J3hU', '0r8KVXUCV6c', '0rE8jEvQW_I', '0tkWa2skSmA', '0uftSGdwo0Q', '0utROiKxejc', '0vXaXWx7Rvo', '0y022OlZ3W0', '0zl_GDVzykw', '100178', '100232', '100367', '100446', '100499', '100961', '10124', '101281', '101513', '101635', '101708', '101733', '101747', '101787', '101851', '101880', '102168', '102184', '10219', '102213', '10223', '102389', '102408', '102424', '102534', '102858', '103114', '103311', '10397', '104739', '104741', '105507', '105537', '105553', '105906', '105963', '106037', '106077', '106514', '106941', '106973', '107182', '107456', '107551', '107585', '108146', '108573', '108793', '109524', '109909', '110003', '110203', '110209', '110543', '110565', '110690', '110754', '110766', '110788', '110794', '110824', '110983', '111104', '111363', '111734', '111881', '112029', '112148', '112169', '112172', '112223', '112425', '112433', '112509', '112604', '112631', '112674', '112903', '113162', '113265', '113369', '113491', '114006', '114016', '114419', '114624', '114845', '114945', '115134', '116202', '116213', '116219', '116221', '116461', '116481', '11650', '117tQqLjza4', '118354', '118573', '118583', '118639', '119348', '119397', '11UtTaDYgII', '11j7EfglCAs', '120342', '120363', '120vnZnmhq4', '121117', '121128', '121358', '121400', '121427', '121584', '121759', '122439', '122581', '122602', '122842', '123986', '124190', '125344', '125676', '125698', '125708', '125726', '125730', '125845', '125868', '125895', '126505', '126542', '126831', '126872', '127470', '127490', '127539', '127622', '127908', '128059', '12812', '128258', '128600', '128752', '128763', '128949', '129728', '129733', '130149', '130366', '130426', '130448', '130456', '130633', '131650', '131871', '131936', '132028', '132476', '132570', '133201', '133888', '134252', '134298', '135623', '135658', '136196', '136205', '136211', '136215', '136234', '136416', '136647', '137455', '137827', '137920', '139006', '139032', '13JpsLvddoY', '140293', '140315', '140317', '154449', '15837', '158832', '158855', '159031', '15Wu4U2VPLw', '15ktF-6wutk', '160225', '16145', '16530', '167521', '172044', '172048', '172050', '172060', '172075', '173CpFb3clw', '17622', '176266', '176541', '17769', '17811', '178163', '17874', '178941', '179797', '179875', '180923', '180971', '181978', '183364', '184784', '186631', '186643', '187033', '187566', '187775', '188004', '188062', '188122', '188343', '188815', '188825', '18938', '189856', '189966', '18JTkk8JbeU', '18QjfdhJEM4', '190599', '190726', '190740', '190743', '191616', '191941', '192799', '192827', '192978', '193093', '193291', '193322', '193514', '193894', '193921', '193924', '194299', '194625', '195575', '19573', '196364', '19664', '196665', '197232', '197778', '198112', '19850', '19915', '199215', '199227', '19k6sEC7814', '1A-aRhYx-GQ', '1A7dqFxx8wU', '1CjUHNYzW1E', '1ESU5ONMMxs', '1F0qH0EEBfo', '1Gp4l-ZTCVk', '1HK7Nea0PFo', '1HS2HcN2LDo', '1J9Fa995Ojs', '1JQ9UCBEmCU', '1LkYxsqRPZM', '1RBT_QoSkIU', '1RxliidxXyQ', '1S4p-Lfhbp4', '1S6ji_d4OLI', '1W38PnuWIkM', '1YEgMQ1xxzQ', '1ZjIX9AK860', '1bzS0YF5hfs', '1cyUKHOvdYY', '1dWsw38VDjY', '1fqNb8cfLFE', '1jogeKX0wGw', '1k7br8xXMSU', '1knHBRVPoYQ', '1mHjMNZZvFo', '1nXhAjuICTA', '1o8Gy4J6WSs', '1olbQVO_C-8', '1pl2FVdQWj0', '1pvK0_9SQns', '1rVly2oeBJg', '1sJjdaXXuJg', '1tQOvm5eQOA', '1tuMFJnlXJc', '1x7kf9vArjU', '1zXAYdPdzy8', '2-4autDbHVQ', '2-5hH_k2VFk', '2-EJz4RG4g4', '200941', '201005', '201497', '201582', '201980', '202431', '202810', '202826', '202990', '203466', '203806', '204085', '204378', '204519', '204792', '205268', '206049', '206051', '206148', '206179', '206376', '206507', '206585', '206606', '206621', '207118', '207812', '207867', '207958', '207964', '207977', '208148', '208299', '208322', '208416', '208465', '208592', '209354', '209758', '209775', '209869', '20LfN8ENbhM', '210098', '210238', '210240', '210259', '210433', '210555', '210618', '21137', '211611', '211875', '212532', '21285', '213207', '213327', '213375', '213619', '214095', '215259', '215318', '215343', '216007', '216097', '21638', '216831', '216857', '21696', '21727', '21735', '217395', '21844', '218708', '218757', '218912', '219310', '219350', '219460', '219600', '219605', '219614', '219620', '219627', '219775', '21Kxt5L9OBE', '220134', '220200', '220548', '220550', '220809', '220832', '221104', '221137', '221153', '221274', '221740', '222116', '222247', '222510', '222605', '22277', '22305', '223333', '223338', '22335', '223377', '223431', '22344', '22360', '22373', '223883', '223885', '223926', '224238', '224263', '224271', '224285', '224292', '224325', '224370', '224472', '224498', '224599', '224622', '224631', '224648', '224649', '224772', '224817', '224869', '225343', '225416', '225768', '225770', '22649', '226601', '226602', '226640', '22689', '227173', '22719', '227416', '227426', '227556', '22785', '22798', '22821', '228561', '22880', '22884', '228925', '22901', '229090', '229296', '229637', '229808', '229903', '229967', '230252', '230422', '230692', '231025', '231412', '231453', '232464', '23289', '233171', '233356', '233366', '233389', '233405', '23343', '233431', '233880', '233939', '233940', '234046', '234053', '234406', '234587', '234641', '236021', '236108', '236306', '236399', '236442', '23656', '236696', '237009', '237363', '237385', '238023', '238039', '238042', '238060', '238063', '238100', '238545', '238567', '238597', '238624', '238645', '238683', '238803', '238858', '238889', '239180', '239235', '239242', '239572', '23YlZoucbyM', '240915', '241124', '241164', '241172', '241178', '24157', '241629', '241638', '24196', '24202', '242166', '243056', '243338', '243341', '24351', '243646', '243797', '243981', '244180', '244261', '244623', '244817', '244829', '244836', '24504', '24508', '245207', '245243', '245276', '245322', '245497', '245582', '245926', '24602', '246117', '246216', '246789', '246798', '247108', '247318', '247382', '247538', '247764', '248024', '24814', '248400', '248837', '24xayGF5yYA', '250343', '250430', '251251', '251417', '251646', '251826', '251839', '252097', '252177', '25271', '252912', '252919', '252998', '253709', '254216', '254298', '254427', '254488', '255205', '25522', '255224', '255226', '255338', '255343', '255408', '255852', '256174', '25640', '256935', '256976', '257045', '257247', '257277', '257531', '257534', '257771', '258654', '258672', '258802', '259260', '259470', '259857', '25t8nrkUfRY', '260011', '260199', '261052', '26110', '26113', '26115', '261267', '261326', '261900', '261902', '262165', '262226', '262341', '263444', '263889', '264418', '264446', '265302', '265811', '265959', '266366', '266396', '266791', '266852', '266861', '26690', '266938', '267092', '267252', '267255', '267278', '267354', '267466', '267694', '267799', '26808', '268127', '268258', '268536', '268836', '26WrTHLJJIU', '270254', '270416', '270439', '270444', '270449', '270628', '270665', '270771', '270956', '270993', '271366', '271594', '271598', '272375', '272624', '272817', '272838', '273032', '273171', '273207', '273237', '273250', '273314', '273510', '273531', '273539', '274073', '274185', '274219', '274917', '275248', '275267', '275603', '275620', '276217', '276771', '27798', '277991', '278474', '27857', '27863', '27880', '27881', '27886', '27888', '279373', '27ubp7CLLKQ', '27v7Blr0vjw', '28006', '280584', '280794', '280951', '28142', '28182', '28191', '282560', '282586', '282985', '283495', '283935', '28412', '284673', '286943', '288714', '288766', '290062', '290088', '29044', '290546', '291121', '292277', '294178', '294226', '295793', '29751', '29758', '29771', '298459', '298736', '298774', '29920', '299754', '2BuFtglEcaY', '2Crezdbre6I', '2HP4KedfmSc', '2HmBM3GGTlg', '2ItiGjefTRA', '2J4zrDn7K4s', '2JY5HXBWO84', '2Ky9DBSl49w', '2LpVvLemgvw', '2OLVF-KEaZU', '2PgJok07td4', '2QOf2wSu0j0', '2QXHdu2zlQY', '2RC_DXV_dPQ', '2RDubrUjI-k', '2S00zYaVrtc', '2Vtv2gPzM7w', '2W-U94hXuK0', '2Wo-0trLBjU', '2Xt31_d0XEE', '2YwgSmmLibs', '2Zx4huY-KN8', '2ahFSMFvs_c', '2c2fhh_Dsoo', '2c6Gb73nx4E', '2cwNG0YuwtQ', '2f6ZHqJ76EU', '2fbBrB1nJEQ', '2h9VVQUZjK0', '2iukwQJjGHE', '2lj0BaPsJpM', '2m58ShI1QSI', '2mWPHvbKzL8', '2o2ljA0QD7g', '2qKajMu7Vhc', '2vX-ZRQQDwo', '2vsgDSlJ9pU', '2w7rpDe-HoA', '2ze94yo2aPo', '30036', '301320', '301321', '30162', '30171', '302220', '30646', '306700', '30762', '30763', '30858', '31197', '31392', '31474', '31544', '31tcuDMQ_H8', '323217', '32459', '32681', '327282', '327283', '32j1yMF37hA', '33089', '33170', '33272', '33312', '33436', '33439', '33ueXDUn3mA', '34-ogj6rh6w', '341382', '341763', '341983', '342197', '342407', '34346', '34640', '34684', '34984', '34989', '34cU3HO_hEA', '35684', '35692', '35694', '35934', '36060', '36098', '36116', '36164', '367506', '367576', '368460', '36o2cux9kfQ', '370050', '370404', '37117', '37459', '37462', '37NMbnd7r20', '38019', '38154', '38374', '38387', '39lcRCFvV-s', '3At-BKm9eYk', '3B1zIUj-k3o', '3DOrdP4H8SA', '3DgOMTs3A1E', '3FpmKxZgihw', '3GyEnzE1NTQ', '3HRnhUin-5g', '3HyAaqre_Fk', '3IUVpwx23cY', '3J3Thmd_vks', '3MFNIag0wNE', '3OMgvtWNHp4', '3OYY5Tsz_2k', '3OuNMjZo5wg', '3QcqpwQiiLs', '3REPHw7oLWo', '3UOqbf_B_Yc', '3V2qm0rw920', '3VsrcbjPwZY', '3WZ6R9B0PcU', '3Wi0if_L6Ug', '3XShFTBsp_Q', '3ZmLhK_YHl8', '3aD0xAgxxyw', '3aIQUQgawaI', '3f6xCzKx9CA', '3fhCxePBuZY', '3gA34VxBijI', '3hOlJf_JQDs', '3j94B0pzSVs', '3kNPvhdF1NA', '3lkn8MS3n8Q', '3n44IP3tCVE', '3oHPsYc8bmw', '3ocpWAmSNUE', '3odZe3AGilc', '3paDwuRQP1w', '3sDL7mwosP0', '3sUS8vXbRwg', '3u_Joh0WJdw', '3w9I5SBhHNc', '3wHE78v9zr4', '3y2YInNUmHY', '40129', '40181', '40247', '40260', '40266', '40367', '40970', '41026', '41032', '41381', '41692', '42426', '424SXFTCFsA', '424vOT3Nnyk', '42946', '43342', '43371', '43428', '43444', '43456', '43469', '44457', '44780', '44MVdpDEQJs', '45175', '45184', '45186', '45676', '45860', '45c3QQzGakM', '46495', '46497', '46503', '46592', '46604', '46615', '46618', '46663', '46808', '46826', '46860', '47056', '47472', '47797', '47939', '48019', '48219', '48300', '48724', '48BD8qjClfE', '48nifdN2vSI', '48u68AzcWvE', '49029', '49073', '49264', '49358', '49417', '49903', '49962', '4AQC7_uCuEE', '4BoE-woHtwA', '4CKhrjuuofk', '4EDblUpJieU', '4EEru3cZwXo', '4Fw9TbWIVJI', '4GYfmpEEtl4', '4H9RfQuqsvg', '4HAC00EAdno', '4HazSkYihtw', '4Kh5N9W-Jr4', '4Q7OkrJLzAc', '4Q95Lg3Icho', '4Ry6M_ZfJQc', '4VA4kqMnEqA', '4Vl6AeEkAg4', '4WTVL5bNybU', '4Y8ROYezksQ', '4YfyP0uIqw0', '4Yxe1_ohz7Y', '4Z47qIrVil4', '4ZhIx2ozUIE', '4cFeG2ydm9A', '4dAYMzRyndc', '4dV3slGSc60', '4hEWns7JBg0', '4iG0ffmnCOw', '4iw1jTY-X3A', '4jTUcNlxlYA', '4kF0U8kIMp4', '4lKz4xyCmkk', '4lro65JwQdc', '4o4ilPK9rl8', '4oeKDFIaL7o', '4poXF3xo6Z8', '4qVvLLFEYnk', '4qjz4BQCZyM', '4qoB5a6dR3w', '4t5k_yILGJM', '4wLP4elp1uM', '4wdeBJ39Cuw', '4wuug3-5cWo', '50103', '50302', '50306', '50307', '50444', '50453', '50478', '50479', '51224', '52067', '52068', '52160', '52839', '53233', '535523', '53609', '53742', '53766', '54CM1_GA-uw', '55156', '55T4QNcdpOk', '56006', '56276', '56853', '56989', '57231', '57294', '57295', '57598', '57618', '58096', '58097', '58151', '58554', '58795', '59302', '59333', '59673', '59712', '5B0PQx3Ep3k', '5F-KEGK9eXo', '5FbzKjx3Lho', '5Fs_7A_V2kA', '5HjbZAbB-Lg', '5IhAgV0Bf_4', '5JMpMKAO-Mk', '5O1W39o56gg', '5OLO6zEbrVw', '5OUcvUDMMWE', '5OXuBO5buF8', '5PqwxUYIe3c', '5QXHyL3BMto', '5R5fDxZUL8M', '5RVF_3YBUVI', '5Tqu1IXJjGY', '5VKMctXBN9M', '5XX0csik6VQ', '5YnYGjrhzF4', '5eA5HpyFsIk', '5eY4S1F62Z4', '5fKPJPFqPho', '5hXC8JbiLd8', '5i6AgQSjXO4', '5jGUBcStsZs', '5kaCGzjYukM', '5lrDS7LluCA', '5oEuiBPUTYc', '5pxFqJ5hKMU', '5qQs9Cfydo4', '5qxZFmIaynE', '5sDBK-lP8Pg', '5tV5rZZJDCE', '5vwXp27bCLw', '5wpCKY2U1NE', '5xa0Ac2krGs', '6-0bcijTR8k', '60037', '60308', '60328', '60351', '60402', '60405', '60428', '60983', '60Qw70Rd9Yg', '61035', '61247', '61277', '61531', '61557', '62438', '63841', '63951', '63956', '65068', '65939', '65kkuNV921k', '663uRzFOfrg', '66505', '66623', '67uKYi7mslE', '68721', '68828', '69234', '69268', '69307', '69707', '69824', '69870', '69NtV79qgOg', '6E-yr6nPxGA', '6EDoVEm16fU', '6EJHA6IDLNk', '6G8JJ69aN6o', '6IUj6jyoTl0', '6J_yzh1LNAE', '6KvG5VbLalY', '6MYUY5tC2Us', '6MloL1gIccs', '6PLlauWxF5E', '6RFVsOWK1m4', '6T1GHj7qwYs', '6TDXQtGUNEs', '6TKaGMkO69E', '6UV6ktwbLoo', '6Vg10LxmWUY', '6WDchK7TuL8', '6YNAmRjpxlA', '6ZA8I83x-oY', '6_ZT8beQXq8', '6_k3cw9_k-Y', '6aoTjlvo0js', '6brtMLkjjYk', '6c7vAwDSVGc', '6gtfeIqGasE', '6hFiTU77Sm0', '6hp1SGhVYfY', '6ng8CZ0ULK4', '6q5uKCgairk', '6qPjYvdZiLA', '6qmReTSoz0E', '6rkV4QRcVnk', '6slq62RwH3Q', '6uPGcKTpC4I', '6uoM8-9d0zA', '6vbd4o5rIjM', '6xvVFYR5di0', '6ywl-j2-e-s', '6zjv1TLqfF0', '70280', '70299', '70420', '70710', '708sewcSXVY', '70HptJgAhZM', '71459', '7155', '7156', '71736', '71987', '72017', '72385', '72tXTrSXoMk', '73360', '73447', '73449', '74101', '74115', '74184', '74197', '74204', '74268', '74447', '74532', '74870', '75393', '75441', '75892', '75938', '75RGHaxmUK8', '76104', '76124', '76819', '76PsKs4kPXE', '76sN4tvbPGk', '78398', '78577', '78752', '78Ncka6CY70', '79019', '79203', '79356', '79644', '79858', '79925', '79934', '79935', '7BFhRPlSOf0', '7CLe7PZtna4', '7EWOMjaKlus', '7IxmlIwqigw', '7R70kKqtGh0', '7SsmaZZ7mz0', '7U_z0JfleAw', '7UlSX-syPeo', '7Vp6Lvin1rA', '7ZzbemE4QEE', '7_K3F-lAv1k', '7deoh0NoMs4', '7eJqqvvziQo', '7eWclVCXOtk', '7f3ndBCx_JE', '7fPYpqzw1iY', '7hSWYI99-24', '7idU7rR77Ss', '7ifnoHhrMtA', '7l3BNtSE0xc', '7mb8Y2AhXIY', '7npCA0zoQ8Q', '7oFimEZJQ_A', '7pZPQbMKGkY', '7r4vRxKLFCA', '7rMLN0KKE5k', '7sgAWvbLtiM', '7vJy1zbopJ8', '7wiaDbpiAQ8', '7xiu1tAJ2d8', '8-Hi9NmF4rM', '80566', '80620', '80627', '80640', '80855', '80866', '80914', '81371', '81406', '81538', '81563', '81615', '81668', '81707', '82666', '830KzUc6CbU', '83119', '83310', '83400', '83859', '83_N43pvgHI', '8404', '84133', '84140', '84176', '84670', '84772', '84924', '856LGms_0s0', '86494', '86c2OkQ3_U8', '87161', '87163', '87400', '87434', '87MsiC3E2-w', '88077', '88119', '88245', '88791', '88792', '88797', '88881', '88888', '88S1BjwMfdw', '88oSn21TFcA', '89184', '89266', '89747', '89787', '89835', '89951', '89ZmOPOilu4', '8Cp8c-eQELQ', '8EHEEXex5D8', '8F1mj-XT6KA', '8Lmqsakmw3M', '8M0AlvtMrv8', '8NPaDkOiXw4', '8P0zm2oYDyY', '8TDAP0KNIIw', '8UH0aPUf7sQ', '8VB6pdJTpX4', '8VP1qxX9mh4', '8VhVf0TbjDA', '8X5tGQLBqPo', '8XODJwsvBa0', '8XZszYrUSCQ', '8YsIvz4qnk4', '8aZbX-xMdgI', '8eaYvALnJ0o', '8f_lBxZgDb0', '8i7u3fl-hP8', '8jY_RhOS89o', '8lfS97s2AKc', '8ovb-GaZ3QE', '8qCNtqjygW8', '8u5aet7s9_w', '8vpoQ5UVpXE', '8wNr-NQImFg', '8wQhzezNcUY', '8wdxczYf1jI', '8x81fzwcJko', '8x_nCOh6Wmc', '8xfW_azeNPk', '9-EO2oCAGA4', '9-K1CXCXui4', '90008', '90172', '90667', '90wZz0thye4', '91002', '91166', '91252', '91276', '91284', '91292', '91574', '91844', '91996', '91y7nPEWdmQ', '92221', '92291', '92331', '92496', '92521', '92533', '92tWNUhG9DY', '93116', '93119', '93807', '93821', '93828', '93839', '93843', '93845', '93iGT5oueTA', '94215', '94439', '94481', '94525', '94532', '94983', '94ULum9MYX0', '95147', '95205', '95388', '95887', '95mDHiLeDCM', '96099', '96106', '96179', '96194', '96337', '96350', '96361', '96377', '96595', '96642', '96694', '96700', '96o9hxy64wI', '97076', '97095', '97289', '97463', '97908', '97992', '97ENTofrmNo', '98155', '98187', '98442', '984VkHzXl8w', '98505', '98513', '98562', '98636', '98649', '99331', '99501', '9BceQC_SdTM', '9GzFQBljNIY', '9JqwFRCTKOM', '9Jyr5laC8mg', '9K4aWWoXuyU', '9K5mYSaoBL4', '9Lr4i7bIB6w', '9Nhoi1jfDbY', '9OY3NU2ycVw', '9PzZSheh10U', '9QI-9w_VdB4', '9TAGpMywQyE', '9Un1O2ypJmI', '9VZ4cpoTFbo', '9XCmrHbU-rU', '9bAgEmihzLs', '9cSJglGi9Pg', '9cYNT_YSo8o', '9ceYD_OzPz8', '9cxlcpbmrH0', '9dFEGb_RfwE', '9f6A3ucYArQ', '9iRncKVM2PA', '9kLNVTm3Z90', '9orb0lQnVW4', '9pdyTplXwM4', '9pzn93kdP_g', '9sAoeFTKILY', '9tX18DNC74w', '9zBj8VkRBpE', '9zWeMrfr-l0', 'A0aE2Gzym-M', 'A1lFJXUpxZo', 'A2E78CGi5TU', 'A4dzxgHDhF0', 'A6R6J8SJtBc', 'A8c7q_P4jQg', 'A8plGi4rbxM', 'AB1PbMaW03s', 'ABSC1JNdpTw', 'ACOlIiRdDyE', 'ADYia28RrFU', 'AG5EY8ZaFj4', 'AGFgaKPzjMQ', 'AGM-hNupJik', 'AHU3PUztC_M', 'AHXwnFvqYDk', 'AHiA9hohKr8', 'AIPqRuTjI4E', 'AKofHsmxa1I', 'ALlKbGE6BH0', 'APYv5s4ucd0', 'AQ4Pktv4-Gc', 'ARNWtLCbfW4', 'ATfnMuJJDkk', 'AVScVfLgNmw', 'AbNmF4RTzOI', 'AcoRjVPIVmQ', 'AepwIhSbAws', 'AgH84SNRx5s', 'AggyS1coOb8', 'AjgDuPEp9sI', 'AlJX3Jw3xKk', 'AlmUwZgY6bs', 'AmGocfFQfVE', 'An8x4UfwZ7k', 'Arh2TKRi_mg', 'AsrhHuCfvF4', 'AuPCZlKBzq8', 'AxNy9TeTLq8', 'Az3e1Opu0LE', 'B1B71X_SmwY', 'B1PzCwfgXyU', 'B9oeT0K92wU', 'BAi3ZJo5Vwg', 'BBSVLGf7zPI', 'BB_VwJSg3AI', 'BBoe8-HWPSw', 'BDCFV4EqHjY', 'BJS5KCSowgU', 'BLFUj7_uy_0', 'BMhG11CXgww', 'BNaRO9g8ugQ', 'BR2pkk3TK-0', 'BRSyH6yfDLk', 'BRdw-_dbJXc', 'BRzjQHYWbSs', 'BSKAHXFYF_I', 'BTjV5dU_rfY', 'BXqnWuDA3H4', 'BZOB3r5AoKE', 'BaX10vbFhuU', 'Baz0VVQ-E9E', 'BdqoZ5qpz0E', 'Bf5owLv3X98', 'Bg-PA5KTjNs', 'BgXqrxum5G4', 'BmDybDBTe7o', 'BnWvk0YRgE4', 'Bo5IDtP8_Fk', 'Bpy61RdLAvo', 'BseI8-TYWm8', 'BuPhjBayua8', 'BvvttuYysvg', 'C-UUQju3bXo', 'C03473pYcRM', 'C2xNDBU2Z0s', 'C3lDRFu1ZcI', 'C5-cY1nPQ20', 'C74-of1rjg4', 'C864zaWmEd0', 'C8Fhlk-eczU', 'CBYQDIqucxE', 'CE9yV4SearE', 'CHh6hWo1AFE', 'CIqRibqChDQ', 'CJGuudyvA0Y', 'CK31s0kfp-k', 'CKqDDh50gcU', 'CL4hDu95Eqw', 'CLkTjujFVKU', 'CO2YoTZbUr0', 'CO6n-skQJss', 'CPjbogfLHR4', 'CQnwOZRgQXs', 'CT7QOWbhfv4', 'CU6U-gS76K4', 'CVx494kExvc', 'CXojk-oSaEw', 'CXvUdCdKTJY', 'CZVmxwrWv34', 'CZigX1ntOsI', 'Cb8ay6WtJuM', 'CbQxC1iPyS8', 'CbRexsp1HKw', 'Cd0JH1AreDw', 'CdB8QBhFccs', 'Ceu9AkIjuLY', 'CfWxp5bRt2A', 'ChhZna-aBK4', 'Cj7R36s4dbM', 'CqFt3_uka14', 'Ctc23Icfzvg', 'CuF-UOcAFG8', 'CuFLRnu4FQ8', 'CvW0Mh1EWFA', 'CwF97vXPYX4', 'CzstrpOTtgo', 'D-DqVICJXBU', 'D02PT0bl88A', 'D1I-Z8jaby8', 'D5cQWqYcBAE', 'DEFDuYyc-Xw', 'DGeBPk1op3E', 'DMtFQjrY7Hg', 'DNzA2UIkRZk', 'DQ7QKE3hzVg', 'DR65no1jCbg', 'DR9Hhpy8aA8', 'DVAY-u_cJWU', 'DVvJoMt9o80', 'DXQGngge7vU', 'DZBpdzrMelA', 'DZIFCYOhosQ', 'DaXkixKFEvE', 'DatH-ra0VKY', 'DbAppk7xT0Y', 'DdUJojoFU_c', 'DebbnL-_rnI', 'DfTMoqwMHVE', 'DgO5O-oDeX8', 'DhXBaBOB4E8', 'DiOw-w5LDJ0', 'DjcZrtcBZi4', 'DlX-WyVe-D0', 'DlbYiwOnTyk', 'Dm8AL84e11w', 'DnBHq5I52LM', 'Dnhxm6sHQpc', 'DpZGnuj-BA8', 'Dq7V30WhOJQ', 'Drhrzj1yhVs', 'DtbT85s3i94', 'DtwbwyfajnI', 'Dws8ZrzF7xQ', 'DzdPl68gV5o', 'E05bA058vts', 'E0g6nae4Ae4', 'E1r0FrFyNTw', 'E7ZLz3tSwNM', 'E8L1Z71vKG8', 'E8MuzHUhUHk', 'EEUGfVTyTQM', 'EEjCkc82oyk', 'EGA6iulTr00', 'EHbZ2EeVags', 'EJT25p7JahU', 'EK2bnbzmJs4', 'ELtbrO8FbuI', 'EMS14J0odIE', 'EO_5o9Gup6g', 'EO_SN04Xef0', 'EOkdFMw0pmk', 'EQxkXiWAm08', 'ERnYvxojeu0', 'ET1cU5_A2YM', 'EULEL3QhKD0', 'EWYivXUavOY', 'EZkOWskJl20', 'E_ocLuUw-1I', 'E_oqbJatW3E', 'Ea_Huc2-lwo', 'EfQoSpEZa0o', 'EiG3r8hAVeY', 'Emi9cw9xa00', 'Emjv2cDzW1E', 'EmmuWoCUgXs', 'EoC83JhkCAw', 'EpadyoYlAjQ', 'Es9MkKsMsjU', 'EuSp1pkX9i8', 'EujJ0SwiCRE', 'ExDLNkDVsxY', 'EyoMU2yoJPY', 'Ezn3p5FWF_U', 'F1U26PLiXjM', 'F2M7WbPXRps', 'F2hc2FLOdhI', 'F4OhXQTMOEc', 'F4yFRROWtdE', 'F59hwsm4Ld0', 'F5SGdvSRJmo', 'F6Ci45lP5aA', 'F6JV-K840EY', 'F7zQPzwFToE', 'F8KE2ZhoUe4', 'F8eQI8E-6q4', 'F9NruwaEmA8', 'FB_b1dm0SRY', 'FCMbWx2JpZ0', 'FGnHDWGYsA8', 'FHDVQkU-nGI', 'FHlzZ2VxuGQ', 'FJgO-FICHlA', 'FLyWn2DCci4', 'FMW6diQ9rMo', 'FMenDv3y8jc', 'FMif4HMBZjo', 'FNbULlE9RKg', 'FOMXt3lStco', 'FP3o75bpi8k', 'FR0B64ID-hA', 'FRKwvmrwrZA', 'FSGL5K2EEdo', 'FWBCTZiijEM', 'FX-YgVKC1AI', 'FX87sfrE47w', 'F_YaG_pvZrA', 'FaHosBIr0xY', 'FcAgbLF6__s', 'FdTczBnXtYU', 'FfavRdRKsAQ', 'FmE9BtTmxnE', 'FmIS69vB12I', 'Fmwc7wkIc4Q', 'FphtHpVV-7w', 'FqEekswKPWE', 'FrS-6P6FdCM', 'FsmNLTdE2p0', 'FvoZgWvc3gc', 'FxxJB8hBpyQ', 'Fy0Uid9afM8', 'FylhSeozvG8', 'FzxmeVRTjwU', 'G1Z_JUTgZqo', 'G38DwNDQ8Cc', 'G3RPgs_7l3U', 'G4HMKRdIva0', 'G8aL3bM39co', 'G8p4QMjLUXI', 'GAVpYuhMZAw', 'GAY5ICoVnA8', 'GCOFujDHDx0', 'GICVzJMDfv0', 'GJyzeK-Cn-0', 'GK-Pprzh0t0', 'GKsjv42t284', 'GMHW6XEO1ms', 'GMa0cIAltnw', 'GMjQUrlMa50', 'GMnoxaqqReQ', 'GNP0PFas12Y', 'GO0V4ZGSF28', 'GOj7TBcEA8E', 'GRFXnnrQaB4', 'GSnt_fW8qjI', 'GTGtVdAuyRc', 'GUffC7vu9zA', 'GXIfrEUJ5d4', 'G_M3lYn6hAA', 'Gbg8qiKQkhM', 'Gc_zIjqqUys', 'GcfETVXgtg0', 'GdFP_p4eQX0', 'GeSTiDe1hG0', 'Gg9ZcRlwnbc', 'GjCUDqWSI6c', 'GkoBk5K-FH0', 'GlShH_ua_GU', 'Gljee9uq_Rc', 'Gmhi58erY6k', 'GmpDbIstUdc', 'GpRDC-S88dM', 'GtRxTvuF6J0', 'Gv0kKbfwPpI', 'Gvg_80PlAAs', 'GxWLhv-Vt78', 'GxcOSmJAyYQ', 'Gz2qsCqWNUY', 'H-74k5vclCU', 'H-AJ-grXvgQ', 'H1DpVwktfDQ', 'H26MMmrHTlQ', 'H4RysMUSQD8', 'H4w9mU8B3Mc', 'H6ci6Myq8uI', 'H9BNzzxlscA', 'H9mSft9YGuk', 'H9zVQlzit8Y', 'HA2AiTz-qxc', 'HAid0IEUoFQ', 'HAnQVHOd3hg', 'HByf7qiO-Kg', 'HD17J-FvROQ', 'HFPGeaEPy9o', 'HGicRLwgtkM', 'HIsJqphLCyw', 'HJTxq72GuMs', 'HK0U0bIN-cw', 'HLFaW4oVP9E', 'HMRqR-P68Ws', 'HOCsiS0pmf0', 'HObh42PhOfw', 'HQ2LwfRcrVc', 'HQ9NnDQcxD4', 'HR18U0yAlTc', 'HS-2vVP95u8', 'HUH9BXVmAig', 'HUOMbK1x7MI', 'HVksxaUzv9Y', 'HWLpH3NMtFs', 'HXuRR2eBHHQ', 'H_x5O9GdknI', 'Ha7DMd_iKyM', 'HbaycY0VuZk', 'HetCe0lLacc', 'HfLVQZLvcH8', 'HgDdU_RB9UA', 'HhpziYYP2Vg', 'HjLpXXFoAQw', 'Hkt_kncJGVM', 'HmXXW45eZL4', 'Hq3cHc6X8BM', 'HqtvFTa29L8', 'HuIKyKkEL0Q', 'HxyfY7hsjOg', 'I--8nRKCkJI', 'I3SPlbTGuNc', 'I5cS_TlV5B8', 'I8MM8y1XVjs', 'I9iV9q3ePhI', 'IAmZ5iWHxw4', 'IAubgwEiCcY', 'IB0lGIxP8YY', 'ICeSewdyf34', 'IDVbPEket6U', 'IHp8hd1jm6k', 'IHv_yBn83P8', 'IIPYcCii7Sg', 'IKXWSNf4m2k', 'IM6HW1HwHhM', 'IOpWjKAHG8Q', 'IRSxo_XXArg', 'IRY4D_-mx3Q', 'ITQTVfk9Iqo', 'IUmxJNs2zaY', 'IW7IssCKgmI', 'I_r2CP56toU', 'IawsOlEKRf0', 'Ie6sDptjAsU', 'Iemw8vqt-54', 'IgenXuE4qIM', 'IiGzuoblKSM', 'IiX1k0oYW_w', 'IkvL4IAiZog', 'IlgnbEdOhmI', 'IsAtAzltJNM', 'It6xg0WcqJ0', 'ItnyexqXR_U', 'Itqbc2TBXWY', 'IwQWibE8EOM', 'Iz_PDdyXr-8', 'J3-NEG8uoiE', 'J3JU1zBQ4RM', 'J6zbBfzW7a0', 'J7UduuBfnrE', 'JATMzuV6sUE', 'JAbFsRxxnFo', 'JAnRCQ9A0vI', 'JCg26b8EO48', 'JDgqyOkzXHw', 'JF-5nlUNx_g', 'JGEEA_JVriE', 'JGZb_ygtB0g', 'JGzFUzPeAA8', 'JHJOK6cdW-0', 'JHW3nD6flgU', 'JKueLneBoik', 'JLqBVj6f5gI', 'JMb18N7lQ0A', 'JMzilFvwNXE', 'JNhqI4JtPXA', 'JNt1k3ohkbs', 'JOK1SSmPtFw', 'JP95Fby6qLE', 'JPT38CA3sGI', 'JSlTJsUulH0', 'JVpF99zuQhA', 'JW2HHfQiGVs', 'JW3OfSCZlhc', 'JWG_MUy7EWQ', 'JX-mwjSw0dk', 'JXYot3NDQn0', 'JY-krIuEUm4', 'JYdfUNjyYxo', 'JYsAacFmXEo', 'J_DtcW86D9c', 'Ja7phs8EbWk', 'JcmqmLaL_MQ', 'JdFaSggG58c', 'Jg2okZn6Ut8', 'Jh1uDYOJMwk', 'JkHxzOWOLfs', 'JlmpDdm1A1Y', 'Jv4CxSgWP2k', 'JvLMNXW7B10', 'JvgofTcCSEc', 'JwzxqrD8tIo', 'Jz1oMq6l-ZM', 'Jz9nlw15QZo', 'JzydLJw6y6o', 'K0m1tO3Ybyc', 'K1VeXZxR24U', 'K5cGfs3i-tk', 'K62NK2KYhws', 'K7FiMf-Dmwo', 'K7KEtrgBkz4', 'K8gp7ncdUl8', 'K9-KyChw8RM', 'K9U2TAF9DDY', 'K9faaGel838', 'KB5hSnV1emg', 'KB73TD4xQ8Y', 'KD0w4BrNXQg', 'KDjl2exStlI', 'KEUVzxC7T3E', 'KF5wgQPp2yc', 'KI2sU-mhM44', 'KJyz9fCt7Is', 'KLy4aBLqwCk', 'KPeos6f4HuQ', 'KQ86HmgwUns', 'KR4MpF0YtgE', 'KRje7su4I5U', 'KSyX-lejDcc', 'KUVGWCTT5DU', 'KXsjl0DKgL0', 'KYQTwFVBzME', 'KZzFCrEyKF0', 'K_5u2Wh_wGk', 'K_K3wpxkRc8', 'KanWhGY33Hk', 'Kep_mLLXsqk', 'KgXuILFtgJQ', 'KkKWFZZmmCM', 'KmYG8zaRmOA', 'Kmtib-wZ0jM', 'Kn5eKHlPD0k', 'Kn99u05vlpA', 'KpMCvUyQCXg', 'Kpe5fISxRTg', 'KqFarHUGjSc', 'KrmVX-HANew', 'Kyz32PTyP4I', 'L-7oRnbi9-w', 'L-a4Sh6iAcw', 'L0WUuWYNeCo', 'L1eFHRQGFeg', 'L1gXZVU6AE4', 'L2uC4_Co9uw', 'L382XZ6iZmM', 'L3b9wwZ8vRQ', 'L5a2ijeWpUI', 'L6pg3DQKoH4', 'L8VjQM9HQfs', 'LAj3rqJvS3s', 'LD3HYOwk1Bc', 'LDKWr94J0wM', 'LFOwCSiGOvw', 'LHC7ZKJy4Zw', 'LJGL2sGvSS0', 'LKAEv8x9jKI', 'LL02UFLfQKo', 'LQnXVd-PR_Y', 'LUKIYWa0UGA', 'LUf9vbe0TcQ', 'LX2rT5C3sJs', 'LZl9Sj1safM', 'L_XttWZBKss', 'LcfubBagG6Q', 'Lh8r-5eHX6Q', 'LityRNuQihc', 'LpTbjgELALo', 'LpbSYaPRTqI', 'LppEvM60-xo', 'LtlL-03S79Q', 'LueQT0vY1zI', 'Lxs_6Tvq12E', 'LyOq9mOEPuk', 'M2Pb2kx77ww', 'M6HKUzJNlsE', 'MBDZbACupsc', 'MCRCXXPqFOI', 'MCsvw5-7lVE', 'MD6Tq_4VlGc', 'MFXQH7ZlBJI', 'MFrwi-RibUk', 'MHVrwCEWLPI', 'MHyW857u_X8', 'MIqL9OXszRw', 'MIuJDn_0yVU', 'MJnc4GUhUuY', 'MKShLlJ7X7g', 'MLegxOZBGUc', 'MM2qPdnRmQ8', 'MMaEsDdBsvM', 'MMpo2bDk-DU', 'MNLG4-diuYM', 'MNXBOeiPhec', 'MOfEl_1dh-g', 'MPRqaQqrd9Y', 'MRU65o1odI4', 'MSHBEntSDjU', 'MSmTFZ_-THM', 'MTU_thei3-c', 'MWCNzYUCys0', 'MYEyQUpMe3k', 'MZUr1DfYNNw', 'MZoLQD83R-4', 'MbpPBLUjpV4', 'McnbZjgCPLw', 'MhojeOmmjC8', 'MjaaudUc5-4', 'MmJD4KBT0nk', 'MnFRVJtHAeo', 'MoQmPA7Q07Q', 'MpuWcGn2Lcg', 'MroQfGehC84', 'MsjnLoTKAXo', 'MtIklGnIMGo', 'MvEw24PU2Ac', 'MvFFYObY9A8', 'Mw4EZYZAd9c', 'MyDLelMxt-A', 'MyfRrTjglm8', 'N-NnCI6U52c', 'N0GhVkCwrqw', 'N0Y7x_fJ0Nw', 'N0d2JL7JC1s', 'N0pKIn6VhOw', 'N188QSyfmeQ', 'N1X63d_jgmw', 'N2nCB34Am-E', 'N3QuD26DuCI', 'N3vSaV8Ztrg', 'N4WOBzpZtWM', 'N5xfBtD6rLY', 'N7IerkWUClI', 'N7tS7A0WWpM', 'NCJTV5KaJJc', 'NDMAVMZyISM', 'NG7QLq4XlWo', 'NIpuKcoJhGM', 'NKindd5Xn3s', 'NOGhjdK-rDI', 'NOl0v54DaXo', 'NSihgq4R8Z0', 'NVLPURuAVLU', 'NVlxq3jxlAk', 'NYuY6HVjMZQ', 'NZtIGzAzJZM', 'N_dNuV-4iAo', 'Na6170hd8po', 'NaWmaHwjElo', 'Nd55vByy2F8', 'NgENhhXf0LE', 'NiAjz4ciA60', 'NkbRuTJaj_M', 'NlrCjfHELLE', 'Nmd50Uzyybs', 'NoOt0oU843M', 'NoZsdSGnNpI', 'NocexkPXja8', 'Nrurc5sMUOE', 'Ns_h5IVcZ9k', 'NuRvTWhELqs', 'Nzh2KkaGwrE', 'NzpymNJaFjo', 'NzvNW7vP-Co', 'O-b3DQg0QmA', 'O0Nn-5UkfoA', 'O2CEvvYWWbU', 'O2ShYliS3CU', 'O3whFU2lAAk', 'O4UkHVJlIs8', 'O8IJh_L0EfM', 'OAV6KKf60bQ', 'OBaMn1-x7jQ', 'OEMSGahoCmM', 'OEOkmrKvTUo', 'OEesmAnTpeA', 'OFBc3z99bIc', 'OFia3dWgaoI', 'OGXUtVG0Y9w', 'OHWyYVux-yw', 'OIJr0voCvVQ', 'OJwO6g01qO8', 'OKJPFisBoPY', 'OKSx1zxHGpQ', 'OLP0nCKwU6k', 'OORklkFql3k', 'OOh1_ffKOKk', 'OPXmk49KyiU', 'OPuu8EknNqk', 'OSyKmOye5Uo', 'OUjzDchhkIA', 'OWWHjP3pX9o', 'OXsTIPdytiw', 'O_fTDhCtAhw', 'Oa2xVjzAMFc', 'OaWYjsS02fk', 'Ob8vPQCPDX0', 'OcqJx_cRc3Y', 'OctOcfI4KSs', 'OdKp0hYomgs', 'OiMkqEH5WeE', 'Oj3XwBBrzXc', 'Olx-sTx7HTk', 'OoOD_VBL0i0', 'Op3CnqRLD9w', 'Or-9Nc_GAq8', 'OrYBtyO0aPI', 'OwE-7MwzfYI', 'OwYfPi9St0w', 'OxyQSJMvWBo', 'OyK86reBnJE', 'P0UHzR4CmYg', 'P0WaXnH37uI', 'P17tYiqMGRU', 'P35dyIit1yM', 'P3QEOJWxXI8', 'P4qxfjZlIrY', 'PANWQeBKStk', 'PEBwwe0PLZ8', 'PGBrZLC15yA', 'PHJ8eybXJdw', 'PHZIx22aFhU', 'PJlDO21zl88', 'PMH4mdJeojE', 'PN3ApHIGopo', 'PORCNDRKtko', 'PQYmOknHA4c', 'PRZ4VhESz1A', 'PUIsiLk8etk', 'PYylJk8R9Ak', 'PboaYD5hlG8', 'Pbu6oAKFCvo', 'PbyI-sUzLZY', 'PcqKFt1p3UQ', 'PexNiFbPTYM', 'PmemTgQHagk', 'PoKZEKOf1dA', 'PohW-isYMK0', 'Pt-D0LUHDSc', 'PtH_8O__EAY', 'PtdK-TLOuD0', 'PwapK9d8IGk', 'PyQrAYl1bFs', 'PybuzkS31J4', 'PzI6yY9Y2xQ', 'Q2uHXKA8cq8', 'Q4WzApjaCNI', 'Q5YU2sLKVL4', 'Q5fZk_jPjBI', 'Q5uYByRyswI', 'Q7OvlYjNt9w', 'Q7S0tX4FUNA', 'Q7nhtfkIPN4', 'Q8naC1qriIU', 'QBc7X5jj8oA', 'QCR7uyowjhM', 'QDRtKv8PvFs', 'QE0pAVSPsKQ', 'QEG_hkJsaYc', 'QI-TTqj0KnA', 'QIonRUsCqBs', 'QJBQIOmG1CA', 'QJCDZCllSYQ', 'QLEOYF7Mju0', 'QLI-OnegxFM', 'QMAztkqnvws', 'QMEgjkpyVzQ', 'QNxiqu1y1N0', 'QOxIdbNwpx0', 'QP2CjKap818', 'QQVRIGEUa_Q', 'QUpI5kMw_nA', 'QVyxySAaehE', 'QWaUvAL-yiA', 'QWll4lS1qqI', 'QWzIJuhLLrc', 'QXPOQW2tRBs', 'QXXexH-ow_k', 'QYp0lGhyL6A', 'Q_py5n2fQXw', 'Qa6Yz62V-Yc', 'QbJiphIpzYc', 'Qe4AT670hq4', 'Qfa1fY_07bQ', 'QjcQx8_PQfw', 'QkMTYY69JYQ', 'QnYlpSeVOYo', 'QoIsjc4-GIg', 'QpchvHpGFc8', 'QxU625Hn370', 'QzdEjKQFisQ', 'R-17BoOk4kc', 'R-HKj8Yn06k', 'R0o37yJ1PP0', 'R0uRDh-engU', 'R5a3TP-l_n4', 'R7IkQVUuLew', 'R8Kw_4Y9E0s', 'R8leu0tEJ4o', 'R9xTBw3MCWI', 'R9ypacCuEZo', 'RAiO_rChmdU', 'RArhIHk4Qs4', 'RB3HA-ZMtFw', 'RChkXDS8ulE', 'RE-LIPOFnrE', 'REkPZ77s3Vc', 'RIAEgUKtzxw', 'RImmdklTOW0', 'RKP6j28FCWE', 'RLHZ6xtIWSM', 'ROC2YI3tDsk', 'ROcTx5VEn7o', 'RRKez9BR-94', 'RS5WM3q2ZZQ', 'RST6PgpsLws', 'RTzDzj4NxIM', 'RVC8l5hf2Eg', 'RXcI6PK37lM', 'RaTQZrdICKo', 'RadU51t1kL0', 'Rb1uzHNcYcA', 'RdExUaEIWIk', 'RdOc70IrjJo', 'Rdf0Psjyrwc', 'RfKxYiFd-fs', 'RgubFcmvgDE', 'RmX9t_HY_dU', 'RqOgkJbqFXg', 'RrGpzInAwCg', 'RsE2gYghZ2s', 'Rse6laKf1i8', 'Rt9rN1ntS3E', 'Rugd2NMu4bA', 'RvIohowRPAk', 'RvmTZVNAAuQ', 'Rw2pzFGosmw', 'RwcTz-tOIeo', 'Ry2NJPd4N3s', 'RzOFxDk26p0', 'S0ecaAi_CJo', 'S21ApRNQ9Ko', 'S2znq3iRPwE', 'S6S2XL0-ZpM', 'S8yP6lBOdKo', 'S9PDgK7fpJI', 'SBFnCNzlynQ', 'SCXp4l0CO4s', 'SD6wphlw72U', 'SGsiGz2fdpo', 'SH0OYx3fR7s', 'SH0wXyhsx9s', 'SHa756SwGJQ', 'SI39azgVUR8', 'SKTyBOhDX6U', 'SLi2gT5H6m8', 'SMr2DprlLu4', 'SMzV26l6iA0', 'SNBqe_JpZC4', 'SO53x_pbou8', 'SRP5FYP71Ng', 'SSNrLu-eZiM', 'STeqtJLakQQ', 'SWHmteF5Rlk', 'SWhfaL7AdEc', 'SYQ_zv8dWng', 'SYnygYs-fAg', 'SZ7HK5ns6mE', 'SZRxuS6fn9s', 'S_ISgjJR1bc', 'S_mxvk4LEh0', 'S_tGoxSEh5Y', 'SaNXCez-9iQ', 'Sb2xMPwNKPo', 'Sb6ftNgzz9M', 'Sglx7douG_4', 'ShgrdU9WAJE', 'Sir2QeCh4B0', 'SmftC-VAfYQ', 'So3bPzg2bq0', 'Sq6DIhFxPqQ', 'SqAiJrvHXNA', 'SqofxdeEcjg', 'SqtZGF_f63s', 'SrMN3rwvCsE', 'StgqrTr0JqU', 'SuIcJvERiFk', 'SuKnUzid-1g', 'SwT0gh0V8fI', 'SxbTZjZD0vU', 'SzJf6viBWBg', 'T-8TvJcS78w', 'T-vqHR989bM', 'T1icdxz0TNs', 'T4e4Ba1sRg0', 'TBCjKGYBNIo', 'TBwetweXRb4', 'TFKgo5JlPro', 'TH32jop4FiI', 'TJelZLbfT2A', 'TLPlduck5II', 'TM7cHOHfF70', 'TNwwVWA-oAE', 'TO5HjoK8Iu8', 'TSDC7CCyeGY', 'TUY83eDK5jc', 'TUzD45ghy4k', 'TVhXqAI9gyA', 'TWMFQED1Myk', 'TWf0XFpXwfs', 'TWszOxkOV0w', 'TXCXFYIc3Yo', 'TXiOSZdaLJ8', 'TYJzg5IJN8o', 'TZXcQemh8n8', 'Ta_IgA3XJ3s', 'TahiuVpcROM', 'TbZPa_keawc', 'TcTGCIh6e5s', 'TdCfpMRiA1c', 'Tdw0fJt0lco', 'Te-vgmYgGM4', 'TfBVq9oWBmM', 'Tiq67-bAV3M', 'TksWzxo81Mk', 'Tpdqfpuumg4', 'TqMh7U3J4D4', 'TqQgIxR7thU', 'TsfyeZ8hgwE', 'TtAyUQtmTLk', 'TwFl4epL244', 'TwTLsgfpKvA', 'TxJjAK0WrOY', 'TxRS6vJ9ak0', 'TxRtSItpGMo', 'U-KihZeIfKI', 'U2Ydtq6qvL0', 'U4-u5NZLA_w', 'U6IqFbpM-VM', 'U8Ewftg_8CI', 'U8VYG_g6yVE', 'UF9OlX-QNHU', 'UKVwc6I06x8', 'UK_IXtJ2BqI', 'ULkFbie8g-I', 'ULtBuI6nMaY', 'UNLD7eYPzfQ', 'UPghCarcnMc', 'URP2YlSNuZ4', 'USkMa4Evv7U', 'UTErYLpdAY0', 'UTNP1roJV3I', 'UUukBV82P9A', 'UXvMaHD9kmA', 'Ua4g9q0r-dI', 'Uc-0G7RRxoY', 'UcINGGuysBE', 'UcV-bpksJi0', 'Uc_6d-fkZEc', 'UeOIb6s0pVM', 'UfLMdSGGQVw', 'UgTM9y2rD38', 'UiPmjKggv1s', 'Uiq0lf9ibF8', 'UiurP5k-f1A', 'UjT8pHMN8Q8', 'UjqA6KVW2m8', 'UkqTwAuA8jI', 'UlTJmndbGHM', 'UmB5c_QqUPs', 'UtjenieM1Ak', 'UtuTyW9pUN8', 'UuR4Rw38v_U', 'Uu_XyXyKHAk', 'UvniNGPyeBc', 'Uw1nWfSuyo8', 'UweZVaFqruU', 'V0SvSPkiJUY', 'V27mmEkN80g', 'V2X1NU5RkwY', 'V5lfNohNlEg', 'V7OFSHYcQD0', 'VA6lVHlj5d0', 'VAFdgtqZz4g', 'VAXhC2U9-2A', 'VAr7gJydnys', 'VCIe-4p0xoM', 'VDjnKdFxuBE', 'VDkBM0ZG4q8', 'VIVkYG31Oas', 'VLQgw-88v4Q', 'VN1IPN1TL3Q', 'VS7xSvno7NA', 'VUrTT7xsrec', 'VVZLA2neYyw', 'VVtx4IDsHZA', 'VWik5oKP6IU', 'VXy21GwGs8o', 'VZqkWIPmH6w', 'Va54WZgPTdY', 'VbSsGRXIwjc', 'VbV9S4svrTg', 'VbmSzYg_lmg', 'VcZffwwL0bw', 'Vdf1McvE9ao', 'Ve-mmCM8TI0', 'VgqI1KRnDSw', 'VhKJm_jiciU', 'VinxKpWioTo', 'ViyQVpGW05w', 'Viz5vDE39y8', 'VjAvqFXEmmk', 'Vlas-mPydcg', 'VoETCD8fBLI', 'VsXGwSZazwA', 'Vuay0mVKkik', 'VugBIVwSmaI', 'Vv2DTPyyw90', 'Vw4-ae-ucHE', 'VwGPIUNayKM', 'VwvptdTkwEE', 'Vx8npmcFxak', 'VybXjsmsjAY', 'VzDXgBOe5iI', 'W-pA0lOGLR0', 'W-ptEZFARVo', 'W-q9bPlMIVI', 'W1CWpktWtTs', 'W40BaML7mzc', 'W53uRqITk2I', 'WANirpnKa4w', 'WAXJRnJXmOA', 'WBA79Q3e_PU', 'WBCRTcMJZZc', 'WGfGxoI05ew', 'WHUHTmtBJWU', 'WJ2QF4Kw9Y8', 'WJM8u2I2rQ4', 'WMzvDL7b4sM', 'WQFRctNL8AA', 'WSjlhBDxNW8', 'WTF9xgqLIvI', 'WUeSV0Z23Kg', 'WXKgBIiP8IA', 'WYSomBHdl4c', 'WZVfvgTeFPo', 'WaaSYRPwQBw', 'WbtsuXkaGeg', 'We5nTulz1Vs', 'WfNiQBXmPw8', 'Wfn7XvSEwUA', 'WgI8IbJtXHw', 'Wh6sht7xwqQ', 'WmQF5RmbsVQ', 'Wmhif6hmPTQ', 'Wn8nz8GPKkI', 'Wnw1IAEDwDM', 'Wo1RxRjXyYw', 'WoL4fCxGd8Q', 'WovurMxG2Aw', 'Wrcf2xtqbkQ', 'WtiBpLt2wuA', 'Wu-wQTmxRgo', 'WuaoxGAKue0', 'Wx9nThz5z_Y', 'WxYb2x_irTM', 'WxajvjGKz7Y', 'WygyVNL_qdE', 'X0jMZoxUL2A', 'X2Hs89fZ2-c', 'X2leyjuolDM', 'X3p50qqFW_w', 'X6-jJO00DMg', 'X6N7UEFJLNY', 'X7-Gbk8gAD0', 'XAT2IfFFAU0', 'XAazxiP6tP0', 'XAgD-xoil40', 'XCyngF93SmU', 'XDVit9ASVUg', 'XDVkhOziCkg', 'XEZ7e4pf1xE', 'XFGHTk_3hGg', 'XFJQJb0yaKY', 'XIGpuL-Kkyk', 'XKyumlBmix8', 'XLjpZUsFEXo', 'XPoY4LD-A1I', 'XQOUZhWI1B0', 'XQxnAPleuCE', 'XVSDfgstFUQ', 'XVWiAArXYpE', 'XWIp0zH3qDM', 'XXvSLz8QmGk', 'X_TusmBIlC0', 'Xa086gxLJ3Y', 'XaVYxIW0FDg', 'XadAy93f1P8', 'XbkYA-iXUwA', 'XblL7mcdDdg', 'XbolrpSc0KU', 'XcHLB-qfv5M', 'Xe-keAcLge0', 'XgT2WWcXip4', 'XlTYSOaZ_vM', 'XnPnlRQXSkM', 'Xo6-V2RlWss', 'Xq7zLxYHxd8', 'XrNaL-MTjXg', 'XsEPZydHWdo', 'XwOdPYlyqXA', 'Xx7Y2YIbleA', 'Xy57UpKRNEo', 'XyBU_gZtU0M', 'XyP8bVmGX6M', 'XytBlo2R5Yg', 'XzVapdEr_GY', 'XzhRYIeczg4', 'Y2F51I-dzAg', 'Y4izAQYtGEw', 'Y6s_vuUUsJM', 'Y6zyM1nDhN0', 'Y7h-S3cJ52o', 'Y7qkyMyanjU', 'Y8dI1GTWCk4', 'Y9fvHRNz2Bs', 'YAYfMkxhRiQ', 'YB9EUgrzCCo', 'YCEllKyaCrc', 'YCI-ZzclIPQ', 'YD5vqR15nco', 'YEOU4KFDjZo', 'YF3mDOuF46M', 'YGeVGXXK3ao', 'YITXOmhQhYE', 'YJJoYkFPmds', 'YLA7h1RTa9w', 'YLK58srjhNI', 'YQUutkMdLj0', 'YQZPnHZRp1w', 'YQanVofsHoE', 'YRbtXb9fWmI', 'YRmjALBdTdE', 'YUNxD04EvfE', 'YVBsDJtAbk4', 'YVHJpAROBvQ', 'YWjTWl0oNLk', 'YWxvIFmYnNU', 'YY2yjEEoB3U', 'YbuG8xyUO-o', 'YcJ5RJYUr7Y', 'YcyHQQXGXWA', 'YfxtwXXkCmc', 'YgyeyooSz0g', 'YgyqX4aZ4tE', 'Yi6Qyn4X8is', 'YjNYBBs1AZw', 'YmGfjU5PGyE', 'Yn8vNC41UMg', 'YodtMIsjM0c', 'YqHEmUKUy-E', 'YrPXbGBvqGo', 'YsMM_1R1vtw', 'Z-26eyME6Co', 'Z3fcd1wdzr0', 'Z4iYSMMMycQ', 'Z7L3V5bwzI0', 'ZDz8Qr-sJ3E', 'ZHUFlEgKk-w', 'ZJGzEeDXbyM', 'ZKErPftd--w', 'ZKKNd0zR8Io', 'ZKZ8UjaQQT4', 'ZLCK8WX6qFQ', 'ZMAoUt0SD7E', 'ZQaGGpcIPHM', 'ZQuGizjoCTY', 'ZRic9ygn9f8', 'ZS1Nb0OWYNE', 'ZSBoSJaD2tQ', 'ZUqTWOx9jYU', 'ZW4Oe_fKJM0', 'ZXHX_e6HCI0', 'ZXQkyX6prBQ', 'ZZ1IyZaMTWQ', 'ZZzdvUdOTww', 'Zae-zQ3VBpY', 'ZaoFHcbRM9g', 'Zb7bdrjWyEY', 'ZbVa-a2tNFM', 'ZcFzcd4ZoMg', 'ZdX9sUd3bAI', 'ZeH7gZw94k0', 'ZfQYEa2fHGo', 'Zg9DKOHJdw8', 'ZiM_gkdFGJk', 'Zj_4YmbMWtg', 'ZjsQqd6KZIg', 'ZltpfWh2aIw', 'ZmAIDJ_gcXM', 'Zo8jCDGMWjw', 'Zr3psuFQoDY', 'Zs8x712Y-CM', 'ZsLrKF7_Oos', 'ZtocGyL3Tfc', 'ZtuTCuh9C1M', 'Zvisaa77Cng', 'ZzL2sHtTWRc', 'ZznoGQVwTtw', '_0efYOjQYRc', '_1nvuNk7EFY', '_26JmJnPKfM', '_2u0MkRqpjA', '_4K620KW_Is', '_4PNh8dIILI', '_7HVhnSYX1Y', '_8pvMpMdGM4', '_BMdEKNF5Js', '_BXmhCPTQmA', '_BtdwH6mfWg', '_BvNNdEkhZA', '_HabvL0VnrQ', '_HanACduNJk', '_JcML8u5Wes', '_K0okiLaF9I', '_KGd7IpX3B4', '_MGd6t9eZP8', '_OmWVs0-6UM', '_PJ26YwqRJE', '_PKP676ez2c', '_UNQDdiAbWI', '_Vp4SDp_knI', '_WA0HtVEe8U', '_WkXQNVLQXs', '_XWUKMkxa-Q', '_YzIvwvyIq4', '_ZlF5Q9W1DM', '_aJghSQmxD8', '_aZDaIfGfPo', '_bIJOxiIJFk', '_gr7o5ynhnw', '_gzYkdjNvPc', '_iRAn6feOmw', '_iy7ftq27xE', '_on_E-ZWy0I', '_q7DM8WkzAQ', '_qVgNLJCYdA', '_t8ETpf2LnM', '_tBaGXNwlp0', '_v8c7iWkYz0', '_xjBqNfiEY4', 'a-WihYDk6vc', 'a4PHWxILDp0', 'a4vRa-_nuJw', 'a5CSKvCAhbs', 'a5FKWiI90hk', 'a7HHg_JygJo', 'a8UMRrUjavI', 'a8WV5KEMKSw', 'aCwLQrJz4Bo', 'aDvgbBqTWWE', 'aE-X_QdDaqQ', 'aE7sckIAWuw', 'aEaM_LOI-BM', 'aFZIvp3SgHY', 'aFhT0px8AMw', 'aFhc2ddipoM', 'aFjMWlR1QAY', 'aI1tdJPHz2I', 'aIUF3QJJOF4', 'aK5tRCXjUvU', 'aKPfbyW-Qxw', 'aMtFBGh2wKI', 'aNOuoSVlunM', 'aWwjhcYLbRw', 'aXla94xd_AQ', 'aYdMtEO0cbI', 'aZjCq_hv2Pk', 'aa0J1AXSseY', 'aa2MEQ6FL7k', 'absh1hsZeF0', 'afCsKTHteg4', 'afDJvn5nmtY', 'ahG9c_uaf8s', 'ahcAFnY6iAY', 'ai7G98thPpk', 'aiJg0jtIHOM', 'aiUq5xksJy0', 'ak7N0BqMluI', 'amem5EmRG3s', 'an_GzG40hcE', 'ap465Tlzf88', 'aqmRPNfeOy8', 'asWraTfIqq0', 'assx_NKo-L8', 'atxoi7pB5TM', 'auTpzciNgls', 'avW2m6VHTyg', 'axQ5CDRawO0', 'b-FX9NOVQOM', 'b1Il3Nf6BrU', 'b1LEcg6z66g', 'b7DK08bWU6c', 'b86B3hP8ARM', 'b8xmgi4VV0M', 'b92iw0OAnI4', 'bAkUEnPoucg', 'bBJ0BxmIocQ', 'bCBMKwafZKY', 'bCDfIJ12oqM', 'bD0FqMn1KXw', 'bDwAuRFciJM', 'bEXCrGIOoWU', 'bI3DTH2yTbM', 'bJI7-LnKPH4', 'bMuoPr5-Yt4', 'bNQOeiAotbk', 'bNQkb4K5DX0', 'bQ8QteHvrEk', 'bQcJVJcpgAs', 'bU6CgE_kMRY', 'bUFAN2TgPaU', 'bVhWpCvpSs4', 'bWmrrWQOVCM', 'bWunvAd96zI', 'bYt4wybVLRQ', 'bZ2ytqPtyK4', 'b_SffSbPVJM', 'babuyKl7EAw', 'badtXoOJaf8', 'bcns0Gu66D8', 'bdFCD-3BCRg', 'bdtwQxwySN0', 'bfy28AlY-TQ', 'bge5PUG3G4A', 'bh4_Kd4hhd4', 'bhFK-xy76U4', 'bhPB19tr-JY', 'bj9e8w4vNxo', 'bkP-BNW4oMg', 'bkX5FOX22Tw', 'bnzVR2ETQQ8', 'bpvYdXVlgvc', 'bvycs3BXtx0', 'bzPFUVGPS3c', 'c1a1y9ytHH0', 'c5-L8vC6_sk', 'c5AJbOd794U', 'c5VEvmutmVg', 'c5cB7AfVgcE', 'c5zxqITn3ZM', 'c6wuh0NRG1s', 'c7xUcM68IFE', 'c805Td8A_Ek', 'c9hE1ghElrM', 'c9jD33baJ60', 'cAA2mONG98w', 'cEXJ2zFbC9I', 'cHBRDdSXLtU', 'cHrOisasWLU', 'cKUPgzZu-Fk', 'cKldvZeyRDA', 'cKxNJY7sA8s', 'cM1Zuji24dI', 'cMUS4nhcKCQ', 'cMsTvqU_-rc', 'cOcNe3L4VAg', 'cRFiXLzcCOA', 'cR_KQlNhoNE', 'cS3BDGNDu_8', 'cSrM5mHACmA', 'cSraseCDilY', 'cUm0TN3vXRc', 'cW-aX4dPVfk', 'cWN1bibicHc', 'cX8FScpsfLE', 'cXu56oK462o', 'cY8DcaHXNNs', 'cZDAM6NjBzE', 'cZVM4svwE90', 'c_FJuhSte8Q', 'caLWajc18Y4', 'ce1ObTPlI38', 'cfX_iuhCbRY', 'cia8OM3Oe7Q', 'cjgq9d8Zn9g', 'cl1DNRGX578', 'cmJW6mkspkI', 'cml9rShionM', 'cn0WZ8-0Z1Y', 'cnllFPRyBFs', 'ctAZf4sMBUQ', 'ctMCSUT2LlQ', 'cuR-l2qCxBc', 'cyOSDKOxb1o', 'cyu1HhakSaQ', 'd-Uw_uZyUys', 'd0QNH2vcDgU', 'd1CDP6sMuLA', 'd38u9n5GOLM', 'd3EeIRaMbbk', 'd3I7Ug-_3L0', 'd7Dz34-cZ74', 'd89Z_olwc2Y', 'd8BR6hsvzoY', 'd99mfGIenCo', 'dBWvtPeHpWM', 'dCDAcBR505Q', 'dDFyJp_OkeA', 'dEBug0QYayY', 'dEDbm2ubVRQ', 'dEXSb0CfodU', 'dH0FmwvOzs0', 'dHk--ExZbHs', 'dJ5MXl66-UQ', 'dLmtsRyRH4s', 'dLzsNc_hzkQ', 'dOSyvhYsfFo', 'dQ56b0bqmc8', 'dR68gbeOWOc', 'dSax_7klbfE', 'dTcz1am1eUw', 'dWfgSGnd9zM', 'dXMAS2tIknw', 'dYgmvmFegkQ', 'dYyg91d7zjY', 'dZFV0lyedX4', 'dZK01jhM27k', 'dZolFNqR-hA', 'damyXH7mBic', 'dbH51F5Zqi4', 'dbpGH5iP0GE', 'dcUlt6zk_S0', 'ddWHTdJz2O8', 'ddix4XuaUlY', 'dlE05KC95uk', 'dqragS38hCk', 'dsob2MgUPpA', 'dt641WxonBI', 'dxsPkcG-Q30', 'dyZdiWNflko', 'dzSv6GwfF-0', 'e-2QlzFeyNU', 'e0hYEr848TE', 'e0ltF8zpEcw', 'e1E5W_ZefmY', 'e7_2U4lm6TE', 'eD5cScqaf6c', 'eE8Qr9fOvVA', 'eFV7iFPYZB4', 'eJfT7-dDqzA', 'eJwZfdtlks8', 'eKQKEi2-0Ws', 'eKRWfaizBO0', 'eLjNLHkvxH4', 'eMa6gbD7id0', 'eOU_40p9RpA', 'eOiC1kb17P4', 'ePiz4zU6crI', 'ePtlUYCdrNM', 'eQc5uI7FKCU', 'eREud0qYR3s', 'eSHOcLD5T58', 'eTCnM472-cQ', 'eTn2P4wmFr4', 'eUwbRLhV1vs', 'eW1vgHE6FRM', 'eY64P27khzk', 'eYCiRWf2NHE', 'eZVcshSUD3Q', 'eaXDCknxPrU', 'eeN7l4R3nPc', 'efLrpnuLwyU', 'ehZrOdw6PhA', 'ejDe6hQWZOk', 'emtvrtk-jsU', 'eo8L7lCTjDQ', 'ep3IasggD3k', 'erBIIKA6r5Y', 'erNDEjQeox8', 'esR-tOjT6KM', 'esnOE1LefKA', 'etPDgRjrnJ8', 'evAswZ6m3hg', 'ewBrNsyY82Q', 'ex-dKshlXoY', 'ey1lr8wFFDc', 'ezuWKsxPRSM', 'f-VdKweez2U', 'f-ZNjqLlrm4', 'f47bnEUrOvA', 'f49boM5fb8U', 'f5k5cF80aUE', 'f6S2XNPIiFk', 'f8Puta8k8fU', 'fB816aeHRFk', 'fDYZINwIs_A', 'fEm9AdLNrjk', 'fGYbbP8RDLQ', 'fI9KptYFZeg', 'fMUmdlzuc94', 'fOxg1NstCHA', 'fPsy1co_nuw', 'fSOVk2cS-sY', 'fT6SrlsWV7M', 'fTKe7E_4OCQ', 'fTNOAssiTYk', 'fU5AYkq0m9k', 'fVCDn6SdtVM', 'fW1pK2L6RgM', 'fWAKek8jA5M', 'fWOIAxzBQFY', 'f_ZJ7L14oYQ', 'faMq0C_wTpU', 'faUvT7zfsyk', 'faXGVcVcVsE', 'fbHlBmq7Ipo', 'fbSxct2JcKU', 'fcxbB7ybUfs', 'fdK_tsvBXJ8', 'fdc7iyzKvFQ', 'fe2CWCpdTF8', 'ffrJ91swyq0', 'fhADeWE5VgU', 'fhY2vbnjuWY', 'fi7g4GoK-z4', 'fiXZqpbRYKk', 'fjE2EZKgQm8', 'flBtk736di4', 'flGZteJUNYU', 'flv4by3RYVk', 'fnFoRPqkQeo', 'foXArPhK0xY', 'frCWtiam4tE', 'frNJdG0GkYw', 'fsBzpr4k3rY', 'fsd1qPLA3kY', 'fz-MzQcOBwQ', 'fzuTEKwNS94', 'g6VJg6ycUk0', 'g8Cl74tS3oY', 'g8NBNtY3cpU', 'g99kISQbyuI', 'g9rNhtFG9aU', 'gAMbkJk6gnE', 'gC_wF3uKFW0', 'gE7kUqMqQ9g', 'gEgcpVY8WK4', 'gFAOKuqpUq4', 'gJdSxPhf85M', 'gJjkCPO7iXg', 'gKojBrXHhUc', 'gL8h7lOPv1Q', 'gLTxaEcx41E', 'gLdRUsLw4CA', 'gPWyANEm0eE', 'gR3igiwaeyc', 'gR4gM_WXesQ', 'gRAmIaSlm80', 'gU7McujYYPY', 'gVqs4TzqySw', 'gXDRWhJbfFU', 'gXuiIWRxrw4', 'gZDyk95Xob4', 'gZF-YNQHqwI', 'gcFECfN4BCU', 'gcGZp9JX_qE', 'gcpsSao7kHM', 'ge071m9bGeY', 'gfHrSg3BwiQ', 'ggbV7YeSl7k', 'gjEYmdWrBLM', 'gjGL6YY6oMs', 'gjSyl6evrWk', 'gm4l099casQ', 'goMtnNHQ_z4', 'gpn71-aKWwQ', 'gr8mzv2xZ2s', 'gr9YU-UTJzQ', 'grePRQ8zonA', 'grsV1YN1z5s', 'guRD0zQC6Q0', 'gv43BPYYxKQ', 'gvjQ1XfMA_g', 'gvrfI9e5xHQ', 'gzZ8NTeTzRs', 'h1ZZHUU4j0k', 'h4LrwMn94ss', 'h7p5URoookk', 'hASTLbFHrEs', 'hBzw4r0kfjA', 'hCFV5VLgS0A', 'hE-sA5umuCk', 'hF6kFQPdBjY', 'hGRRlXfH0BY', 'hI7ObFqn9Bg', 'hIQhkTjNaQc', 'hOp6I4jGvu4', 'hSEfYcIjIr4', 'hSgKOKK3L8M', 'hThsntvCk2s', 'hUBQrLjm4M4', 'hawztAtYHCE', 'hbJfSyJKBEA', 'hdLC4Ro977Q', 'he475nnmMi0', 'hfK8QtTd7fo', 'hfVXx8hfKak', 'hfvmQuhqMmY', 'hh04W3xXa5s', 'hhK7q30p0Jk', 'hj1ph17-0jE', 'hjBQmIWiWgw', 'hkGN5q3OOCE', 'hlBOP5NskhM', 'hlYDicOj2m0', 'hmjfBZGGZR8', 'hnSdAbnHDy4', 'hntnelpiyl8', 'hp3nsb_eIc0', 'hqzF4IDaIYE', 'hrLoOAh_8LY', 'huzEsVEJPaY', 'hvfaYsv7p8E', 'hyazktfsZew', 'hytnfm-AVaA', 'i5bmKlHv-SA', 'i5of7ZTkyfw', 'i6UngvEEuNs', 'i714Xt6eb1Y', 'iEOBNT0HFtU', 'iFGoQUh81as', 'iFj9wJaJSPE', 'iFxFTtCQ6zA', 'iH04VOkbof0', 'iM3wSwoAtwk', 'iMG962Rj3Vw', 'iMss8xBhFiI', 'iO34hER47xA', 'iPPp6MCythU', 'iQDB_OkAQWs', 'iR1Os7FVGHs', 'iREkcXde5ds', 'iS91daVpH2E', 'iUXqlrui25c', 'iWBx0CP9JqA', 'iXiMifHNKPI', 'iYtD_ArCdKw', 'icIS9VfbzMw', 'icbUzboLcDQ', 'iceSGs0MXaA', 'icehEYAiimM', 'ieWQdiQAh4M', 'iex7I7iVnLk', 'ihPjsliqz5o', 'ihuGFUrzD30', 'io85pRFj_dw', 'ioupaiAuRr0', 'ip8xwAr4MRE', 'ipLoS44xfO4', 'ixQbCXLUUj8', 'ixZAEncExWI', 'izFBeXMTgds', 'j0br-fFy160', 'j1m6ctAgjsM', 'j4MpQ6CKoDE', 'j6R1NTrosuM', 'j7fRIGphgtk', 'j9mRv472dq0', 'jD3iIEsc_K4', 'jE3gYsxr_5s', 'jEQoBvc7V1o', 'jFNgOznarMo', 'jFpRdhl3fgw', 'jHB9kiH6Vas', 'jICmfLeFymA', 'jLN6B0aSrW0', 'jNdvceZ4etc', 'jPGDFrQLD4I', 'jPf0LjZAS14', 'jPrEKz1rAno', 'jPtaz1rN6lc', 'jQ3EmbRIe58', 'jT3FSTBA8Us', 'jUKH63mltLE', 'jV6qzF2YROc', 'jVayR0VClxQ', 'jXQmVFcOiUI', 'jXp595jj734', 'jYT3-RQFy1U', 'jZTe5cbwwEE', 'jZe-2w7pkd8', 'jZnFr-Mzj9M', 'jakiZQd4boM', 'jbJF3aphcP0', 'jbKDW9URnvM', 'jcQi90n008o', 'jfSGWBfVNBI', 'jgNu58Da9cs', 'jhb6SV2dQzg', 'jj8aSNPHMw8', 'jjb1cWmkhI8', 'jjbOD6u7V34', 'jlCFLG6rKKY', 'jlW4Iep4mmE', 'jnG9zog4NCs', 'jqKzaEniiKo', 'jqutn5ou8_0', 'jrDqduyQrfo', 'jscKL5jS-SQ', 'juI9KoLA87A', 'jzNIU-w0MVo', 'k0cGUNUHuyY', 'k1BrzX5bc7U', 'k1ca_xbhohk', 'k4T6GaDrcjs', 'k5-3KuvCFms', 'k5ZOshC2LP4', 'k8NgVOCDYKM', 'k8yDywC4gt8', 'kCQqLNiO0mk', 'kDyK9VGfLW8', 'kI6jzM_aLGs', 'kKMfsWrigTE', 'kKkZihSbxF4', 'kLAXmTx2xOA', 'kM6wo2bb3nw', 'kPch8YhNfPE', 'kPn-t6NSxfQ', 'kRs39SO-neY', 'kRsA87VU6EQ', 'kSooCdVTI6E', 'kU6YY2z7z0I', 'kXhJ3hHK9hQ', 'kXiBdruxTvE', 'kY64gXOapYk', 'kZfcQ4a0kx4', 'kaudsLIvYC8', 'kbGhEJ4W44o', 'kbRtSmJM5aU', 'kcIGTPHEsNo', 'kddCewAsxfA', 'kf3fZcx8nIo', 'kfsO4c0Ekes', 'kg-W6-hP2Do', 'kh6eynD9OIk', 'khw3YCTFLfs', 'kiR5zVo2zvU', 'kj63uF0RhW8', 'kjQAkrdbREk', 'kk4gr5A6_Uw', 'kkaX3okuvjI', 'kld9r0iFkWM', 'kmAisuAFckw', 'kmgsC68hIL8', 'kpS4BXif_Sw', 'kqXpf26EL-s', 'krHZuqfXrPU', 'ktblaVOnFVE', 'ku-dtUqCm2o', 'kuQWPuhnlHs', 'kvUb78Rdzdc', 'kwkhYpCHWPw', 'kx544gnOZB0', 'kxcCxbsKx3w', 'ky7Q5onqxGw', 'kyPepNmxaj8', 'kz7ahNfXFsk', 'kzmaKnujN64', 'l-nM_U1qpko', 'l0vCKpk6Aes', 'l1jW3OMXUzs', 'l4oMbKDuW3Y', 'l5_DDqck4Rs', 'l8LTcZJ-_8k', 'lD4xtQ6NpDY', 'lEsFI5YJGDo', 'lKTeLK8nq8w', 'lO6N9dyvPTA', 'lO87-4Kf0cQ', 'lTfsqoo-jKg', 'lUNNDDsm7R8', 'lVj6ZdyW9pI', 'lWXjdWvB0VI', 'lWf5hHRHIDs', 'lWvaIexY2WU', 'lYwgLa4R5XQ', 'lawHYX5eB20', 'laxadgca3Fo', 'lbx3eu8LUl8', 'lc5bSoGlQwY', 'liJO1yIFsJs', 'lilHOUfWrIA', 'lkIe41StoGI', 'lkeVfgI0eEk', 'lksy-At0vxc', 'll10nEjTvGE', 'lo0R1mvjDT8', 'lp9vkkimXqg', 'lpF-yBYou0s', 'lrjm6F3JJgg', 'ltA1VF_DUSc', 'luQfWSfb1s0', 'lwL4hjhkid4', 'lxBKEPIUSgc', 'ly3QDQDJzzQ', 'lzVA--tIse0', 'lzyVWNrkgbQ', 'm-7yRWZLwLY', 'm0-DH0k115I', 'm7SJs73SF8w', 'm8tzdtrgFUA', 'mHEtr7PHxoA', 'mHGaQ_nv9UY', 'mHIvH6Nnrls', 'mI50zj8cTNw', 'mKP6TVuixWg', 'mL2v-sODUZ8', 'mLW-jn67gRE', 'mLZZ174bqws', 'mMeGgB9-hSE', 'mN1Z5xEOy10', 'mNurUl_Q2UY', 'mO4H5S_Q7aU', 'mOnyMWhb5Ac', 'mQTaEIrNxnQ', 'mRPIse3OO74', 'mRnEJOLkhp8', 'mRqqH_gx7Q0', 'mSWwz7dXUP8', 'mVnqP-vLpuo', 'mW8eL4e7Wrg', 'mXdvCxLbpuY', 'mZ_8em_-CGc', 'maO6KmmgcB8', 'mfpR4CN9LZo', 'mgovo8VYtvk', 'mgpoY1-110U', 'mgsvwAVQAQo', 'mjQfQx2K__8', 'mjYeEikLR8I', 'mjpCK2Edsv8', 'mkkoJ2iVbGs', 'mmg_eTDHjkk', 'mn8_HvzbN5c', 'mpfSRGHFY0g', 'mrQIrm6hYMQ', 'muOYAxkG-Zo', 'mud6tEqFiEM', 'mw_ZwML0SbY', 'mxFU6TrHChY', 'mxa4KXSz9rw', 'mzAu5gxjE-w', 'mzTHW2dtbs4', 'mziQcFSsdRI', 'n-sgVVTE9Io', 'n2BuwHbdilY', 'n4xRx0J3rYQ', 'n5J1ZkGUGnU', 'n5MU2-GFjo8', 'n7K5SpMrQt0', 'nAhn8-kZpM8', 'nD1R4UedEDo', 'nDXKvarLEOM', 'nErOMpqvcRg', 'nF0AMbTH4VM', 'nFTo-Lz4Fr8', 'nGah7qST1dI', 'nHMHePX9WoU', 'nHP7l5zAwGA', 'nISNBD3I95A', 'nKiC9sEP2vM', 'nM5S3xxNOlQ', 'nMCCLB9gOag', 'nNFDj9fRAqE', 'nQ7txb1MtG0', 'nTBKtwqPIYw', 'nTZSH0EwpnY', 'nUZYqTRSGDw', 'nXWNHVZtV9A', 'nZEPf7a8GbI', 'nZFPKP9kBkw', 'naZi9AusrW4', 'namehdJxRIM', 'nbru7qLot04', 'ngp7s5Wvde0', 'nhD9WSEIspQ', 'niX_m3aMxhQ', 'nmWplIQhvoA', 'npIVLL_fTf0', 'nsLACXjD4KU', 'nw6Kf3AtCz4', 'nxksB2kDJ6o', 'nyzIL6YAonY', 'nz2vkMyqfeE', 'o1cRmT9qnOc', 'o2XbNJDpOlc', 'o2bNnLOEEC0', 'o2miCpS7Hwo', 'o4b7hjQwpv0', 'o5OsO6Aq9pA', 'o63doZKEIds', 'o6Cd9RmqdBc', 'o7vcTMAX9fQ', 'o8OZGBDPD8M', 'oANUXY3xXKM', 'oBS-IW-BO00', 'oCsrnIomHos', 'oDznO5w3v0U', 'oGFDE-6nd7Q', 'oGnKsroR0R0', 'oH9fMma8jiQ', 'oHV99Y_EWfI', 'oHff2W51wZ8', 'oK951Y2N_k0', 'oNCvb0F-c88', 'oNnoN0bxVnA', 'oO61Ma17tRA', 'oOhxCwkZsUI', 'oPlnhc0DkcU', 'oPvUDLhTrjI', 'oQizLbmte0c', 'oVqBUyKvgdU', 'oW1OEsP7Dds', 'oWAyEzyp2xQ', 'oZkWWCQw2Js', 'oZxMx8e0x2U', 'oaC1UATdtAg', 'obGF3RfWQKE', 'obnOnuzb-Xw', 'oecQ9yDtfx0', 'ogGweZUAVtU', 'oj7A8mpuBeQ', 'olVQZV_1rdA', 'olcPEN9x5VY', 'omBcvw7izuM', 'omHQ68k50XY', 'ooAF6I-fmTg', 'ooGB4BLK9PM', 'opHUZsc2WO8', 'ossKC1VrusE', 'otna1VHHCow', 'ou6S6fbfBWU', 'ouhCx4TgHlE', 'owT0_Xfu8a0', 'oyHKiX5YGDE', 'ozA7pRW4gFM', 'p18PVgfxook', 'p19Hsjx2xgI', 'p1zqEGwRMI8', 'p4IeTRNcxc4', 'p4WmcxrXkc4', 'p5LEk2u8tXk', 'p7zuPEZgtY4', 'pAclBdj20ZU', 'pC0zFF8CXXk', 'pDRdCSIyjkA', 'pFh-SLxltms', 'pI5Kng_GuLE', 'pIMByitigYI', 'pIaEcqnzI-s', 'pKXaKinzTmY', 'pOrk7fl1AYo', 'pQpy7RSfWzM', 'pRdxEaLB8r4', 'pSxte-ms0t8', 'pU-MjERGTUk', 'pVs1daijYTw', 'pVzHaakhKAw', 'pY4ovojS4rk', 'pYUTSG9frAk', 'pZye4zFzk3o', 'pbFwuNCQlH8', 'pf-78LF4ORE', 'pf6HYOOwfGE', 'pfCPogxnUfw', 'pjWLxORDDP4', 'pkH2TfFN-6s', 'plsot5mFAtQ', 'pnSRLB6anIM', 'pnpFPX34Agk', 'po5jCjnxW1w', 'pqAGD3ijMPw', 'ptgBkE_e5js', 'ptp-kfn4vWY', 'pttwIaJmfcU', 'pu1C7tmYHxI', 'puum3vabpb4', 'pvIQWWiT4-0', 'pvlmYjQdRmA', 'pwj9YeMJC08', 'pzUplbeH_fg', 'q-0gu48ClF4', 'q17gSr9kNww', 'q1_knonj0Lw', 'q3QUybiOYeU', 'q4hP2j7oewo', 'q5M1God4M6Y', 'q7BxnIM2_z0', 'q9yDL81QppI', 'qAip3lZRj-g', 'qBanrqkzobg', 'qCUg4ek1PPE', 'qDIh6YNbIMU', 'qDfSYz0PX9g', 'qEkFReBuwxc', 'qEuJj4uW93E', 'qFppeQvdfxI', 'qFsGMA75-oQ', 'qJBQH14qtHc', 'qJZlp9uxoTU', 'qJh6x4wmmh8', 'qK77ZmbqmOM', 'qP4uuMI-JSk', 'qTkazqluJ_I', 'qXisb7w9LjM', 'qY1sXq-YCHs', 'qYC3gN1AkOA', 'qYJtbgLqizk', 'qaoSn1eEyq0', 'qbIajpM7U3Y', 'qd6YuFGcBGk', 'qdeQg5dZb2E', 'qed3medhsj4', 'qgC8_emxSIU', 'qjMyjLSORY4', 'qkWdc-E6jGI', 'qlDT-bmIzQI', 'qlDiE1d45Bc', 'qmMSumMloAs', 'qmx0nQZYHIo', 'qo2reupzUE0', 'qpht9L3s9G4', 'qrI7XCKRFNk', 'qrSW93EtuMc', 'qszy_WSRFUM', 'qtI3CB1jslQ', 'qvIhzmujTi8', 'qyJiDgtj6YE', 'qyqVc352g3Q', 'r13W4H8ZhqI', 'r1erb5PSm1A', 'r46amqjpWgg', 'r4ul1wl_1T8', 'r5ijC2W9ddk', 'r8Jyl2Bfl14', 'r8OPmbZZlpE', 'r917wP1qsL4', 'r91csZc6AXE', 'r9tgDqXEcuc', 'rApOTQgFW6Y', 'rB6WmWvvyxg', 'rBWeo_rEdcU', 'rC29Qub0U7A', 'rC7qKZMKa0U', 'rCQYq4T8SEI', 'rEWQW_ibDwg', 'rFHIApgUtoI', 'rKyXU0ls5aQ', 'rLHJ_ebXCHQ', 'rLNY4-FWt5c', 'rNXM3avNBKk', 'rVG8gOAY0JE', 'rWjPSlzUc-8', 'rZqWAnrJ1Nc', 'r_vjlTcrI-U', 'raQ9Pcgo-II', 'rcfnqiD0y8o', 'rePYTlT5_AY', 'rexJoy4RMdY', 'rf0yDSeVEUA', 'rhQB8e999-Q', 'rojyUp4FPSg', 'rsQRiALH5FU', 'rv1gp0wEhI0', 'rvQ2cRnroOU', 'rwNlqh0k1j4', 'rwe-hBNpzH8', 'ryE9VBiR3p8', 'rz4Ax6xfZHQ', 'rzsk_IvI7bQ', 's0PUurm1ONo', 's1Yo_MCiMPc', 's1tVE8g6JBc', 's2oX3zxWuEI', 's53UAEi7JEY', 'sC16n1k66io', 'sC1NvtirijU', 'sFLTjqVS7AE', 'sFxUNO2HOKY', 'sIGAq2J4KKI', 'sIusv36VoBY', 'sK9_de5Jx4U', 'sKwPCpFtUUI', 'sLaTZtL0ZIk', 'sMnbpyFqrlE', 'sPremsknoLM', 'sPxoGNvnVzg', 'sRMdbZVjhis', 'sRex0-9yFFc', 'sU40irMa-so', 'sWsDAG6x9n0', 'sfaWfZ2-4c0', 'sjoArVtU8-o', 'skRqBxLLJkE', 'skp_SpnIOCM', 'slLRsFFiiRc', 'slz8pd0Wzxs', 'sn1fEo72gJM', 'snVkfJKvjwk', 'soqnBdPOwoo', 'sqADHmnM164', 'srbrWyZRMCI', 'ssH8WQF4eN0', 'ssqiwP19JhM', 'svsbGQn389o', 'sw-smuVRByI', 'szic2IB9HB4', 't-wXEPHHtME', 't0Wi5Nb6kbI', 't2U7w3K2qac', 't4-ZulW5np4', 't7a3zECpT4k', 't80DGfWJ3fI', 'tC2KicUHB9Q', 'tL8RzwWSmAE', 'tNd3--lvSXE', 'tO68uTk-T_E', 'tQ-CIfgj-Js', 'tQk-eoA7JyE', 'tRSPfPlNbMU', 'tUTf9R5u6r0', 'tW5xAWDnbGU', 'tXQQ0joMq1Q', 'tXQpm-nKxNA', 'tXxXhGD1aMo', 'tXxf6-CilNI', 'tZDNinnrGf8', 'ta26Z2YEVMQ', 'taCchyi5h68', 'taGs3-SMoYM', 'tdIZZ9v0IGA', 'teQqaAmqqx0', 'tgrxA3CiBv0', 'tkzdanzsA0A', 'tnWmVXZ87h0', 'tnhRxo-g3Xw', 'trQLgl6ncmk', 'ttfaZ8IIWCo', 'tv8IhQZQuJI', 'tvE_DucE0_k', 'tw2xFbfgV4U', 'txjqbr6FoZs', 'tymso_pAxhk', 'u6H0sVuY1ac', 'u6PL_7Z9QlU', 'u8ViR4L2cVk', 'u94VPxEIJPg', 'u9I5WD3Nglk', 'u9ZV8jb_-U0', 'uAExKaLeOl8', 'uBgRo9tnv-I', 'uITHY4LORDs', 'uN0d3ZOkDp8', 'uQds2dN1AiM', 'uTWeng4h5eM', 'uU_3tbZ1zG8', 'uVM4JWjfGgQ', 'uWW1ejwFURI', 'uXYAfpQUYVE', 'uYzZB4ccG1Q', 'uacdDoBtT1k', 'ueCWy7aTmyo', 'uea5xJZygsc', 'um8WVjZMLUc', 'unNY4zIk8MM', 'unOeTDc2rlY', 'uogwnZGb-iE', 'uooIiW6dEyg', 'ussfmzwftQ8', 'uu1FqsR7JV0', 'uxHBEfIhY-E', 'uy9MGhVqCp8', 'uyOrVTq8Tjk', 'uyeQsv2tq44', 'v0a8N7Y-Vfc', 'v1GKBjohgHc', 'v2DFe9X-jCY', 'v2Rr-EsJda8', 'v3l56fl6aJM', 'v45zoIrjoTo', 'v5AAvrZuR2U', 'v73dbqOUoXQ', 'v9GshyXtiWg', 'v9gjg7TDlTA', 'vA68fTqDars', 'vB_kZocHtYo', 'vC9oWzTU71s', 'vCWpKubBbcw', 'vGkIaClHKDg', 'vGxqVh_kJdo', 'vHzbn_YW76s', 'vI5JH0daNsM', 'vJDDEuE-FlY', 'vM3YB7LmMq4', 'vM7m581tHn0', 'vM9AH4EqCaA', 'vNID-RVhWcQ', 'vR90Pdx9wxs', 'vRhj2bLo1ho', 'vRydHjRzDX0', 'vTAV6FThy30', 'vU0a4F36Ksw', 'vXABB4tBBUU', 'vYOr36tdcpc', 'v_8QeoNc4QY', 'vbVLgaP5IYU', 'vbr3EA8cJTI', 'veA6ECsGFxI', 'veHYwR7ge6Y', 'vi0FsBCqfRE', 'vlnKSMw5v1o', 'vn0CFvUK5D0', 'vo_JbAFAD68', 'vop7tN_K64I', 'vqBRkjHWjXM', 'vrgwwihTxVQ', 'vttEPA6Xffk', 'vwUBtNvjrU4', 'vx8j_4DMqh4', 'vySKXq0ZOb0', 'w-PFjo89Dpw', 'w1MxXu5D7ho', 'w44JDB1NmqM', 'w4xIyGs-3wc', 'w8TXP0iz29A', 'wC_1M7KIv9s', 'wCbQxI_-GKU', 'wCtZdAdoIm8', 'wEs7pJH2mqQ', 'wF8980oGklk', 'wHQYmMZzyJY', 'wHeZHLv9wGI', 'wI7DDCRh4Nw', 'wLJohZMqR-g', 'wLj_gnNON7A', 'wLrgvSYLPUI', 'wMggOhhVouA', 'wNx1ZnQ0MDQ', 'wO8fUOC4OSE', 'wOKWTFYYMuk', 'wOjnqud5GBA', 'wQkNqbrAlB4', 'wR3zalLV_6c', 'wRKVNiABc0w', 'wUf6AvCMHbQ', 'wVdoIjg3ZFQ', 'wXuBJvzeC1Q', 'wY8JbFOsp5E', 'w_qYZSVx2dM', 'wb7CfuiIZFQ', 'wd-LTpCtAzw', 'wd8VQ5E7o7o', 'wdU-0bttAkc', 'werKCgZ5dFE', 'wfLrS6CdFs4', 'wgCSuviySwg', 'wgca4dZnnVQ', 'wj3ur4fsiN4', 'wj4snKbUWZA', 'wk5lFk5kFjY', 'wlrb0HyIs-Q', 'wnL3ld9bM2o', 'wosolsrH3sM', 'wrQVnClcNPM', 'wtCKzu8eVJ0', 'wvLEIxt-_V0', 'wvMA0r3esw8', 'wxxqXK9x-64', 'wz3nVBPVgIA', 'wznRBN1fWj4', 'x-oexIBWwR4', 'x-zGBp7axao', 'x0rLwBIocuI', 'x266rUJQC_8', 'x2lOwQaAn4g', 'x2n19Cn96aw', 'x5xF4yu9Bos', 'x7ZR70cTa84', 'x8UZQkN52o4', 'x92920ole0w', 'x9E8yaFCX0Y', 'xAApNMTj3as', 'xBE9YWYGjtk', 'xBsrNFxnnuY', 'xNYsfeSgDCQ', 'xP81rX2ATLI', 'xPN38lYBQv4', 'xPhlDO-2Foc', 'xRJXYUZVdas', 'xSCvspXYU9k', 'xTBBDx3DkXQ', 'xU3N7ujUB-g', 'xUgAP84vsg4', 'xUz5cCA-oTo', 'xXGEkDhu94E', 'xXIq7YPkdUQ', 'xXXcgb9eZ9Y', 'xYtNgq9eTlg', 'xZ1FrxU6hcg', 'xaSf1d13WoY', 'xbl7bU5PGBs', 'xfP0bm6Z-7E', 'xfeASyC6PPg', 'xgAwddSkrOo', 'xge9aYTxxOA', 'xiRUWSUrGSg', 'xivYoEUQ5sk', 'xjsaS-LeZ14', 'xk4C4p5vHDk', 'xkEK17UUyi4', 'xlNDpmKgSIc', 'xmLJHru6Z1M', 'xn9_4n4NpEw', 'xnNZyfhusAk', 'xobMRm5Vs44', 'xomMHflvVDw', 'xqcqieV0Rn8', 'xsiHAO0gq74', 'xu53lk4dhSc', 'xvnFF0FiyS4', 'xw-Blnej8EA', 'xwNQ7QuXaps', 'xwvSYhQrHoA', 'xyOa9ivNrg4', 'y-nEcwAyQos', 'y1gL_MuokI8', 'y3r2kk8zvl0', 'y5JdqGL5DRc', 'y5Jpf48SUX8', 'y8HXGm1-Ecw', 'y9FyTEyGy5Y', 'yALhl-lMBxk', 'yBGuxyO-ElM', 'yBtMwyQFXwA', 'yCpHmPSshKY', 'yDIFWzOBjXc', 'yEtfjJOMMCs', 'yG3wpvLUQcg', 'yHtR71rx16I', 'yKdIZR5xfcc', 'yLo-Jl8nBXU', 'yNiJGh7JpLM', 'yOQHOiswVjc', 'ySblgk7T7eQ', 'ySfHPyaDbqY', 'yUqNp-poh9M', 'yXE_XZUb8qE', 'yZWH2P-h0jo', 'ybK5wRaaUyE', 'ydzNAuqUAnc', 'ygb8Cfxghbs', 'ymPQOY2O_nw', 'yoDMh8FlHR8', 'yp6Gd2NPwOU', 'ypZhKsR-9iI', 'yrIXBklJ5YQ', 'ysGFcG_PI5I', 'yt9GpittX4U', 'ytXVSpPfKwA', 'yuwvVTnsDjc', 'yxZydS2qHzs', 'yyToAEoUuek', 'yzCHa2qchpg', 'z-bupzylxEc', 'z0y1ZxH1f74', 'z441aDJvAcU', 'z6E-ocntPo4', 'z7FicxE_pMU', 'z8bWG2z-bMM', 'zFTuwjr3xq0', 'zFuRNvfC4So', 'zJWmmDk9fCE', 'zKNAu2CCeRs', 'zOHZDq2BvGo', 'zPskvGHHQtE', 'zR-kKDaSDmE', 'zR62DCEMWgs', 'zShw-E-yBqU', 'zU2_X9lIcUM', 'zURYO3k8lW0', 'zWfDRPy4lXQ', 'zaTyjiyrHnk', 'zcG0ms7CdBE', 'zeQKU2L173M', 'zfZUOvZZTuk', 'zg-ZIxVsBbQ', 'zhE_68GMJIk', 'zhNksSReaQk', 'zi0a6r52ZlY', 'zj4PzGtFwJM', 'zjYEBwXGD8I', 'zk2jTlAtvSU', 'zknhrUs7-lI', 'znNt--6itO4', 'zoPOeViAdOo', 'zqkawTdHN5s', 'zrFZAofNGi4', 'zsRTbbKlsEg', 'zuc7vUhZNfY', 'zv0Jl4TIQDc', 'zvZd3V5D5Ik', 'zwRBZisdpQ0', 'zwTrXwi54us', 'zx4W0Vuus-I'])\n",
            "Intervals\n",
            "[261, 2]\n",
            "================================================================================\n",
            "Features\n",
            "[261, 1]\n",
            "Different modalities have different number of time steps!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnxr_S-FGb5I",
        "outputId": "79100cf2-d03b-408d-c51e-95a3743233a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "label_field = 'All Labels'\n",
        "\n",
        "# we add and align to lables to obtain labeled segments\n",
        "# this time we don't apply collapse functions so that the temporal sequences are preserved\n",
        "label_recipe = {label_field: os.path.join(ALIGNED_DATA_PATH_LABELS, label_field + '.csd')}\n",
        "dataset.add_computational_sequences(label_recipe, destination=None)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m\u001b[1m[2020-10-25 12:45:03.926] | Success | \u001b[0mComputational sequence read from file /content/drive/My Drive/HT/Dataset/AlignedData/labels/All Labels.csd ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|‚ñè         | 451/23248 [00:00<00:05, 4506.52 Computational Sequence Entries/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[94m\u001b[1m[2020-10-25 12:45:05.458] | Status  | \u001b[0mChecking the integrity of the <All Labels> computational sequence ...\n",
            "\u001b[94m\u001b[1m[2020-10-25 12:45:05.458] | Status  | \u001b[0mChecking the format of the data in <All Labels> computational sequence ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                                                    "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m\u001b[1m[2020-10-25 12:45:10.890] | Success | \u001b[0m<All Labels> computational sequence data in correct format.\n",
            "\u001b[94m\u001b[1m[2020-10-25 12:45:10.890] | Status  | \u001b[0mChecking the format of the metadata in <All Labels> computational sequence ...\n",
            "\u001b[92m\u001b[1m[2020-10-25 12:45:10.890] | Success | \u001b[0m<All Labels> computational sequence metadata in correct format.\n",
            "\u001b[92m\u001b[1m[2020-10-25 12:45:10.890] | Success | \u001b[0m<All Labels> computational sequence is valid!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-OEmepcGsNK",
        "outputId": "442602b1-f2c0-4f54-b009-b53843ad6f6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "identifiers = list(dataset[label_field].keys())\n",
        "#print(identifiers[:10])\n",
        "random.shuffle(identifiers)\n",
        "print(len(identifiers))\n",
        "\n",
        "train_index = math.floor(len(identifiers)*0.8)\n",
        "dev_index = math.floor(len(identifiers)*0.85)\n",
        "#print(train_index)\n",
        "#print(dev_index)\n",
        "\n",
        "train_split = identifiers[:train_index]\n",
        "dev_split = identifiers[train_index:dev_index]\n",
        "test_split = identifiers[dev_index:]\n",
        "\n",
        "train_split = identifiers[:100]\n",
        "dev_split = identifiers[100:115]\n",
        "test_split = identifiers[100:130]\n",
        "\n",
        "# inspect the splits: they only contain video IDs\n",
        "print(test_split)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23248\n",
            "['Baz0VVQ-E9E[8]', 'Y9fvHRNz2Bs[3]', 'EO_5o9Gup6g[6]', 'hytnfm-AVaA[7]', 'SmftC-VAfYQ[2]', '6EDoVEm16fU[6]', '238023[4]', '244261[20]', '50103[1]', '238063[0]', 'pwj9YeMJC08[1]', 'f49boM5fb8U[5]', 'GUffC7vu9zA[6]', 'JvgofTcCSEc[1]', '271598[22]', 'icIS9VfbzMw[1]', '266938[24]', '271366[25]', 'x-oexIBWwR4[3]', '-m9KtvCk_L8[1]', '298774[7]', 'aCwLQrJz4Bo[3]', 'PcqKFt1p3UQ[2]', '107585[1]', 'PyQrAYl1bFs[6]', '0EphpADwdPg[5]', '272838[10]', 'h1ZZHUU4j0k[4]', 'CHh6hWo1AFE[0]', '46618[5]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI-4bXrfHZVw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm_notebook\n",
        "from collections import defaultdict"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0YsD_5FHa8x"
      },
      "source": [
        "# a sentinel epsilon for safe division, without it we will replace illegal values with a constant\n",
        "EPS = 0\n",
        "\n",
        "# construct a word2id mapping that automatically takes increment when new words are encountered\n",
        "word2id = defaultdict(lambda: len(word2id))\n",
        "sep = word2id['[SEP]']\n",
        "cls = word2id['[CLS]']\n",
        "pad = word2id['[PAD]']\n",
        "sp = word2id['[SP]']\n",
        "\n",
        "# place holders for the final train/dev/test dataset\n",
        "train = []\n",
        "dev = []\n",
        "test = []\n",
        "\n",
        "# define a regular expression to extract the video ID out of the keys\n",
        "pattern = re.compile('(.*)\\[.*\\]')\n",
        "num_drop = 0 # a counter to count how many data points went into some processing issues"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB390uKcIi_u",
        "outputId": "b6815d08-f473-4d6d-847e-458b839ff59b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#for segment in dataset[label_field].keys():\n",
        "for segment in identifiers[:130]:\n",
        "    \n",
        "    # get the video ID and the features out of the aligned dataset\n",
        "    vid = re.search(pattern, segment).group(1)\n",
        "    label = dataset[label_field][segment]['features']\n",
        "    _words = dataset[word_field][vid]['features']\n",
        "\n",
        "    # remove nan values\n",
        "    label = np.nan_to_num(label)\n",
        "\n",
        "    #print(_words.shape, label.shape, segment)\n",
        "\n",
        "    # remove speech pause tokens - this is in general helpful\n",
        "\n",
        "    words = []\n",
        "    for i, word in enumerate(_words):\n",
        "        if word[0] != b'sp':\n",
        "            words.append(word2id[word[0].decode('utf-8')]) # SDK stores strings as bytes, decode into strings here\n",
        "\n",
        "    words = np.asarray(words)\n",
        "\n",
        "    if segment in train_split:\n",
        "        ###train.append(((words, visual, acoustic), label, segment))\n",
        "        train.append([words, label, segment])\n",
        "    elif segment in dev_split:\n",
        "        ###dev.append(((words, visual, acoustic), label, segment))\n",
        "        dev.append([words, label, segment])\n",
        "    elif segment in test_split:\n",
        "        ###test.append(((words, visual, acoustic), label, segment))\n",
        "        test.append([words, label, segment])\n",
        "    else:\n",
        "        print(f\"Found video that doesn't belong to any splits: {vid}\")\n",
        "\n",
        "    #input('Enter: ')\n",
        "\n",
        "print(f\"Total number of {num_drop} datapoints have been dropped.\")\n",
        "\n",
        "# turn off the word2id - define a named function here to allow for pickling\n",
        "def return_unk():\n",
        "    return UNK\n",
        "word2id.default_factory = return_unk"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of 0 datapoints have been dropped.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr1LrHvWKytB",
        "outputId": "ce990754-725b-4edf-b6ad-67caa6cf9eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# let's see the size of each set and shape of data\n",
        "print(len(train))\n",
        "print(len(dev))\n",
        "print(len(test))\n",
        "\n",
        "print(train[0][0].shape)\n",
        "#print(train[0][0][1].shape)\n",
        "#print(train[0][0][2].shape)\n",
        "print(train[0][1].shape)\n",
        "print(train[0][2])\n",
        "\n",
        "print(f\"Total vocab size: {len(word2id)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "15\n",
            "15\n",
            "(181,)\n",
            "(1, 7)\n",
            "5O1W39o56gg[4]\n",
            "Total vocab size: 4467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syj7HxxxL-0Y"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device= 'cpu'\n",
        "\n",
        "def multi_collate(batch):\n",
        "    '''\n",
        "    Collate functions assume batch = [Dataset[i] for i in index_set]\n",
        "    '''\n",
        "    # for later use we sort the batch in descending order of length\n",
        "    batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
        "    \n",
        "    # get the data out of the batch - use pad sequence util functions from PyTorch to pad things\n",
        "    labels = torch.cat([torch.from_numpy(sample[1]).to(device) for sample in batch], dim=0)\n",
        "    sentences = pad_sequence([torch.LongTensor(sample[0]).to(device) for sample in batch])\n",
        "    \n",
        "    # lengths are useful later in using RNNs\n",
        "    lengths = torch.LongTensor([sample[0].shape[0] for sample in batch])\n",
        "\n",
        "    return sentences, labels, lengths"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFavx3DDMKGZ"
      },
      "source": [
        "# construct dataloaders, dev and test could use around ~X3 times batch size since no_grad is used during eval\n",
        "batch_sz = 1\n",
        "train_loader = DataLoader(train, shuffle=True, batch_size=batch_sz, collate_fn=multi_collate)\n",
        "dev_loader = DataLoader(dev, shuffle=False, batch_size=batch_sz*3, collate_fn=multi_collate)\n",
        "test_loader = DataLoader(test, shuffle=False, batch_size=batch_sz*3, collate_fn=multi_collate)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_slj8IGZMfml",
        "outputId": "581bea8e-6975-42f5-d86e-6171071ef2ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# let's create a temporary dataloader just to see how the batch looks like\n",
        "temp_loader = iter(DataLoader(train, shuffle=True, batch_size=8, collate_fn=multi_collate))\n",
        "batch = next(temp_loader)\n",
        "\n",
        "print(batch[0].shape) # word vectors\n",
        "print(batch[1]) # labels \n",
        "print(batch[2]) # lengths"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([710, 8])\n",
            "tensor([[ 1.0000,  2.3333,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 1.0000,  1.6667,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-1.3333,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3333],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.3333,  0.0000,  0.0000,  0.3333],\n",
            "        [ 0.0000,  0.6667,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 1.6667,  1.3333,  0.0000,  0.6667,  0.0000,  0.0000,  0.3333],\n",
            "        [ 0.0000,  0.0000,  0.3333,  0.0000,  0.3333,  0.0000,  0.0000],\n",
            "        [ 2.0000,  0.6667,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
            "tensor([710, 238, 190, 171, 148, 145, 135, 117])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Elxl7DMyVS",
        "outputId": "a7f50008-4ecf-4b94-8355-85213f33e25e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# Let's actually inspect the transcripts to ensure it's correct\n",
        "\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "examine_target = train\n",
        "idx = np.random.randint(0, len(examine_target))\n",
        "\n",
        "print(' '.join(list(map(lambda x: id2word[x], examine_target[idx][0].tolist()))))\n",
        "print(examine_target[idx][1])\n",
        "print(examine_target[idx][2])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "secretary kerry i am blessed to have a niece named iris who is one of the most extraordinary young women i've met from the day she came into our family she's filled our lives with love and joy and every time i'm with her i am grateful that my sister peggy was able to adopt her from china years ago every child needs and deserves to grow up safe and sound in a loving home but sometimes that's not the kind of environment a child's biological parents are able to provide when parents or relatives aren't able to care for children adoption can help give kids the permanent families that they deserve and when adoptive families are not available in the places where these children live inter country adoptions can help find them a loving home abroad i firmly believe that ethical and transparent inter country adoption is a critical part of the international children's welfare system it helps ensure that kids receive the love and support they need to grow into healthy and productive adults i have seen it firsthand and that's why i worked hard in the united states senate to help families navigate past roadblocks in the international adoption process it's also why i was proud to be a member of senator caucus on adoption today the united states is one of countries that are party to the hague adoption convention a set of internationally supported principles aimed at protecting both birth and adoptive parents and most importantly adopted children and thanks to a law president signed this past january one that i co sponsored when i was a us senator today these adoptions are safer than ever every us accredited inter country adoption provider in every country around the world must adhere to a set of strong universal standards that make the well being of kids the top priority the state department's adoption website is a great resource for anyone who is interested in learning more our bureau of consular affairs keeps this site updated with the latest country information sheets adoption processes and developments that may affect inter country adoption over the past decade more than children from more than countries were adopted by american families and as we mark national adoption month this november the department of state commits to doing our part to find loving homes for thousands and thousands of more children thank you\n",
            "[[1.         0.33333334 0.         0.         0.         0.\n",
            "  0.        ]]\n",
            "f6S2XNPIiFk[6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xixu0tYXRPF5",
        "outputId": "89220264-0f79-4748-acba-cdda8cef3326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJG1t86OPRa5",
        "outputId": "861e11bd-ff53-4bc5-d0e4-fbb3cade717e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
        "\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', return_dict=True, num_labels=6)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4, amsgrad=False, )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `mem_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
            "  FutureWarning,\n",
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-sSST4vROEE",
        "outputId": "a19ae4bf-0d1d-4965-cb91-bde883195d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 4\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch \", epoch)\n",
        "\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #y = torch.argmax(batch[1][:,1:], dim=1).to(device)\n",
        "        #print(y)\n",
        "\n",
        "        #print(batch[0].shape)\n",
        "        #print(batch[1].shape)\n",
        "\n",
        "        for idx, sentence in enumerate(batch[0].T):\n",
        "            x = ' '.join(list(map(lambda x: id2word[x], sentence.tolist())))\n",
        "            inputs = tokenizer.encode(x, return_tensors=\"pt\", add_special_tokens= True).to(device)\n",
        "            labels = torch.argmax(batch[1][idx,1:])\n",
        "\n",
        "            outputs = model(inputs, labels=labels)\n",
        "\n",
        "            print(outputs[0], outputs[1])\n",
        "            \n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #input()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0\n",
            "tensor(0.8336, grad_fn=<NllLossBackward>) tensor([[ 0.8329, -0.2924, -0.1056, -1.9859, -1.1489, -0.1130]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7538, grad_fn=<NllLossBackward>) tensor([[ 0.8343, -0.2734, -0.0851, -1.9924, -1.1767, -0.1304]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8207, grad_fn=<NllLossBackward>) tensor([[ 0.8285, -0.2603, -0.0460, -2.0003, -1.2044, -0.1510]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8544, grad_fn=<NllLossBackward>) tensor([[ 0.8152, -0.2522, -0.0163, -2.0086, -1.2329, -0.1462]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.6656, grad_fn=<NllLossBackward>) tensor([[ 0.8138, -0.2481,  0.0066, -2.0177, -1.2606, -0.1458]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8687, grad_fn=<NllLossBackward>) tensor([[ 0.8052, -0.2486,  0.0471, -2.0277, -1.2876, -0.1506]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8701, grad_fn=<NllLossBackward>) tensor([[ 0.8079, -0.2523,  0.0793, -2.0384, -1.3136, -0.1589]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8632, grad_fn=<NllLossBackward>) tensor([[ 0.8215, -0.2587,  0.1045, -2.0496, -1.3389, -0.1700]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8491, grad_fn=<NllLossBackward>) tensor([[ 0.8440, -0.2675,  0.1226, -2.0612, -1.3631, -0.1839]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8288, grad_fn=<NllLossBackward>) tensor([[ 0.8748, -0.2785,  0.1345, -2.0732, -1.3864, -0.2000]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.5752, grad_fn=<NllLossBackward>) tensor([[ 0.9129, -0.2913,  0.1410, -2.0855, -1.4088, -0.2179]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7878, grad_fn=<NllLossBackward>) tensor([[ 0.9391, -0.3069,  0.1661, -2.0982, -1.4305, -0.2390]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7670, grad_fn=<NllLossBackward>) tensor([[ 0.9730, -0.3237,  0.1845, -2.1110, -1.4514, -0.2612]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.2266, grad_fn=<NllLossBackward>) tensor([[ 1.0134, -0.3414,  0.1967, -2.1240, -1.4713, -0.2842]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7237, grad_fn=<NllLossBackward>) tensor([[ 1.0404, -0.3600,  0.2017, -2.1375, -1.4521, -0.3111]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1135, grad_fn=<NllLossBackward>) tensor([[ 1.0739, -0.3792,  0.2016, -2.1509, -1.4357, -0.3382]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6874, grad_fn=<NllLossBackward>) tensor([[ 1.0952, -0.3994,  0.1954, -2.1642, -1.4243, -0.3376]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.6072, grad_fn=<NllLossBackward>) tensor([[ 1.1234, -0.4199,  0.1853, -2.1774, -1.4151, -0.3398]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6600, grad_fn=<NllLossBackward>) tensor([[ 1.1402, -0.4420,  0.1963, -2.1906, -1.4088, -0.3457]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6464, grad_fn=<NllLossBackward>) tensor([[ 1.1638, -0.4641,  0.2014, -2.2037, -1.4039, -0.3540]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1867, grad_fn=<NllLossBackward>) tensor([[ 1.1941, -0.4862,  0.2022, -2.2166, -1.4007, -0.3638]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6190, grad_fn=<NllLossBackward>) tensor([[ 1.2118, -0.5085,  0.1969, -2.2293, -1.4013, -0.3475]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3722, grad_fn=<NllLossBackward>) tensor([[ 1.2365, -0.5307,  0.1883, -2.2418, -1.4030, -0.3353]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3738, grad_fn=<NllLossBackward>) tensor([[ 1.2481, -0.5277,  0.1743, -2.2544, -1.4048, -0.3278]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.6892, grad_fn=<NllLossBackward>) tensor([[ 1.2485, -0.5026,  0.1563, -2.2672, -1.4069, -0.3242]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(4.1225, grad_fn=<NllLossBackward>) tensor([[ 1.2392, -0.4832,  0.1599, -2.2800, -1.4105, -0.3250]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6134, grad_fn=<NllLossBackward>) tensor([[ 1.2156, -0.4728,  0.1535, -2.2276, -1.4193, -0.3334]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2861, grad_fn=<NllLossBackward>) tensor([[ 1.2030, -0.4656,  0.1441, -2.1814, -1.4283, -0.3435]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6266, grad_fn=<NllLossBackward>) tensor([[ 1.1820, -0.4369,  0.1302, -2.1411, -1.4369, -0.3558]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2145, grad_fn=<NllLossBackward>) tensor([[ 1.1714, -0.4134,  0.1137, -2.1061, -1.4456, -0.3697]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6383, grad_fn=<NllLossBackward>) tensor([[ 1.1525, -0.3705,  0.0937, -2.0761, -1.4538, -0.3853]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6409, grad_fn=<NllLossBackward>) tensor([[ 1.1438, -0.3344,  0.0716, -2.0504, -1.4622, -0.4021]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6381, grad_fn=<NllLossBackward>) tensor([[ 1.1449, -0.3044,  0.0482, -2.0286, -1.4709, -0.4197]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6303, grad_fn=<NllLossBackward>) tensor([[ 1.1548, -0.2799,  0.0237, -2.0103, -1.4801, -0.4380]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2474, grad_fn=<NllLossBackward>) tensor([[ 1.1719, -0.2606, -0.0024, -1.9953, -1.4892, -0.4572]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0380, grad_fn=<NllLossBackward>) tensor([[ 1.1782, -0.2463, -0.0303, -1.9830, -1.5010, -0.4484]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6154, grad_fn=<NllLossBackward>) tensor([[ 1.1741, -0.2124, -0.0602, -1.9738, -1.5117, -0.4438]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.3133, grad_fn=<NllLossBackward>) tensor([[ 1.1787, -0.1848, -0.0905, -1.9670, -1.5223, -0.4421]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6141, grad_fn=<NllLossBackward>) tensor([[ 1.1725, -0.1629, -0.1222, -1.9629, -1.4953, -0.4463]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6109, grad_fn=<NllLossBackward>) tensor([[ 1.1753, -0.1462, -0.1537, -1.9606, -1.4722, -0.4524]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9236, grad_fn=<NllLossBackward>) tensor([[ 1.1859, -0.1344, -0.1853, -1.9602, -1.4524, -0.4607]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8923, grad_fn=<NllLossBackward>) tensor([[ 1.1863, -0.1030, -0.2172, -1.9617, -1.4350, -0.4710]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6096, grad_fn=<NllLossBackward>) tensor([[ 1.1771, -0.0546, -0.2499, -1.9652, -1.4194, -0.4834]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2851, grad_fn=<NllLossBackward>) tensor([[ 1.1770, -0.0144, -0.2821, -1.9700, -1.4064, -0.4972]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6182, grad_fn=<NllLossBackward>) tensor([[ 1.1677,  0.0177, -0.3144, -1.9750, -1.3985, -0.4836]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7446, grad_fn=<NllLossBackward>) tensor([[ 1.1674,  0.0427, -0.3463, -1.9810, -1.3924, -0.4740]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7020, grad_fn=<NllLossBackward>) tensor([[ 1.1584,  0.0850, -0.3779, -1.9884, -1.3877, -0.4682]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6440, grad_fn=<NllLossBackward>) tensor([[ 1.1413,  0.1422, -0.4096, -1.9970, -1.3833, -0.4662]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6531, grad_fn=<NllLossBackward>) tensor([[ 1.1344,  0.1896, -0.4404, -2.0062, -1.3811, -0.4669]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6561, grad_fn=<NllLossBackward>) tensor([[ 1.1369,  0.2280, -0.4704, -2.0160, -1.3804, -0.4701]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2768, grad_fn=<NllLossBackward>) tensor([[ 1.1478,  0.2580, -0.4995, -2.0263, -1.3815, -0.4756]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3336, grad_fn=<NllLossBackward>) tensor([[ 1.1482,  0.2792, -0.5287, -2.0360, -1.3860, -0.4551]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2437, grad_fn=<NllLossBackward>) tensor([[ 1.1392,  0.2922, -0.5326, -2.0460, -1.3920, -0.4399]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.5013, grad_fn=<NllLossBackward>) tensor([[ 1.1218,  0.2979, -0.5390, -2.0553, -1.4011, -0.4013]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6969, grad_fn=<NllLossBackward>) tensor([[ 1.0977,  0.3214, -0.5472, -2.0655, -1.4097, -0.3697]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7085, grad_fn=<NllLossBackward>) tensor([[ 1.0850,  0.3377, -0.5566, -2.0760, -1.4190, -0.3441]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1193, grad_fn=<NllLossBackward>) tensor([[ 1.0825,  0.3472, -0.5671, -2.0870, -1.4287, -0.3240]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3731, grad_fn=<NllLossBackward>) tensor([[ 1.0711,  0.3493, -0.5796, -2.0974, -1.4412, -0.2819]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3565, grad_fn=<NllLossBackward>) tensor([[ 1.0523,  0.3449, -0.5685, -2.1082, -1.4543, -0.2479]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0007, grad_fn=<NllLossBackward>) tensor([[ 1.0265,  0.3344, -0.5368, -2.1195, -1.4675, -0.2215]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7742, grad_fn=<NllLossBackward>) tensor([[ 0.9948,  0.3188, -0.5114, -2.1302, -1.4835, -0.1744]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7870, grad_fn=<NllLossBackward>) tensor([[ 0.9757,  0.2995, -0.4907, -2.1415, -1.4989, -0.1358]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8648, grad_fn=<NllLossBackward>) tensor([[ 0.9683,  0.2773, -0.4742, -2.1530, -1.5142, -0.1049]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8024, grad_fn=<NllLossBackward>) tensor([[ 0.9531,  0.2510, -0.4629, -2.1643, -1.5316, -0.0549]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8051, grad_fn=<NllLossBackward>) tensor([[ 0.9494,  0.2226, -0.4550, -2.1758, -1.5485, -0.0140]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.5638, grad_fn=<NllLossBackward>) tensor([[ 0.9558,  0.1924, -0.4501, -2.1876, -1.5648,  0.0185]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.3373, grad_fn=<NllLossBackward>) tensor([[ 0.9542,  0.1842, -0.4483, -2.1997, -1.5796,  0.0433]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8097, grad_fn=<NllLossBackward>) tensor([[ 0.9445,  0.1724, -0.4493, -2.2119, -1.5550,  0.0589]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2061, grad_fn=<NllLossBackward>) tensor([[ 0.9457,  0.1573, -0.4524, -2.2242, -1.5339,  0.0684]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.6115, grad_fn=<NllLossBackward>) tensor([[ 0.9391,  0.1386, -0.4335, -2.2368, -1.5160,  0.0716]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.6767, grad_fn=<NllLossBackward>) tensor([[ 0.9258,  0.1410, -0.4192, -2.2497, -1.4999,  0.0696]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8362, grad_fn=<NllLossBackward>) tensor([[ 0.9056,  0.1376, -0.4101, -2.2621, -1.4891,  0.0891]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8426, grad_fn=<NllLossBackward>) tensor([[ 0.8975,  0.1303, -0.4042, -2.2745, -1.4805,  0.1019]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.6321, grad_fn=<NllLossBackward>) tensor([[ 0.9005,  0.1194, -0.4012, -2.2868, -1.4741,  0.1088]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8457, grad_fn=<NllLossBackward>) tensor([[ 0.8950,  0.1043, -0.4024, -2.2987, -1.4718,  0.1358]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1487, grad_fn=<NllLossBackward>) tensor([[ 0.9003,  0.0864, -0.4058, -2.3106, -1.4710,  0.1552]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8444, grad_fn=<NllLossBackward>) tensor([[ 0.8978,  0.0657, -0.3874, -2.3228, -1.4714,  0.1667]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1170, grad_fn=<NllLossBackward>) tensor([[ 0.9057,  0.0429, -0.3731, -2.3350, -1.4731,  0.1721]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8375, grad_fn=<NllLossBackward>) tensor([[ 0.9056,  0.0180, -0.3390, -2.3474, -1.4759,  0.1710]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7530, grad_fn=<NllLossBackward>) tensor([[ 0.9154, -0.0083, -0.3108, -2.3597, -1.4794,  0.1649]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0334, grad_fn=<NllLossBackward>) tensor([[ 0.9173, -0.0121, -0.2881, -2.3722, -1.4833,  0.1544]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8319, grad_fn=<NllLossBackward>) tensor([[ 0.9114, -0.0198, -0.2471, -2.3849, -1.4878,  0.1390]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8281, grad_fn=<NllLossBackward>) tensor([[ 0.9162, -0.0303, -0.2129, -2.3974, -1.4931,  0.1204]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7912, grad_fn=<NllLossBackward>) tensor([[ 0.9305, -0.0434, -0.1850, -2.4097, -1.4991,  0.0990]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7851, grad_fn=<NllLossBackward>) tensor([[ 0.9359, -0.0355, -0.1633, -2.4221, -1.5048,  0.0748]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8973, grad_fn=<NllLossBackward>) tensor([[ 0.9334, -0.0089, -0.1472, -2.4346, -1.5104,  0.0482]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8253, grad_fn=<NllLossBackward>) tensor([[ 0.9234,  0.0107, -0.1127, -2.4472, -1.5166,  0.0190]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8258, grad_fn=<NllLossBackward>) tensor([[ 0.9245,  0.0247, -0.0847, -2.4594, -1.5235, -0.0113]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7972, grad_fn=<NllLossBackward>) tensor([[ 0.9355,  0.0336, -0.0629, -2.4714, -1.5309, -0.0426]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.2973, grad_fn=<NllLossBackward>) tensor([[ 0.9371,  0.0367, -0.0482, -2.4828, -1.5408, -0.0494]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8235, grad_fn=<NllLossBackward>) tensor([[ 0.9303,  0.0357, -0.0390, -2.4944, -1.5121, -0.0615]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.2410, grad_fn=<NllLossBackward>) tensor([[ 0.9339,  0.0310, -0.0345, -2.5057, -1.4873, -0.0764]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8202, grad_fn=<NllLossBackward>) tensor([[ 0.9290,  0.0229, -0.0346, -2.5171, -1.4289, -0.0957]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7861, grad_fn=<NllLossBackward>) tensor([[ 0.9341,  0.0118, -0.0385, -2.5284, -1.3772, -0.1169]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8136, grad_fn=<NllLossBackward>) tensor([[ 0.9313, -0.0025, -0.0221, -2.5397, -1.3324, -0.1404]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.0384, grad_fn=<NllLossBackward>) tensor([[ 0.9383, -0.0193, -0.0112, -2.5507, -1.2935, -0.1652]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9343, grad_fn=<NllLossBackward>) tensor([[ 0.9361, -0.0381, -0.0057, -2.5620, -1.2229, -0.1934]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(4.3081, grad_fn=<NllLossBackward>) tensor([[ 0.9257, -0.0596, -0.0059, -2.5729, -1.1626, -0.1966]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8175, grad_fn=<NllLossBackward>) tensor([[ 0.9054, -0.0859, -0.0134, -2.5198, -1.1133, -0.2057]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8173, grad_fn=<NllLossBackward>) tensor([[ 0.8973, -0.1130, -0.0237, -2.4728, -1.0711, -0.2170]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "Epoch  1\n",
            "tensor(4.1411, grad_fn=<NllLossBackward>) tensor([[ 0.8997, -0.1410, -0.0372, -2.4313, -1.0353, -0.2306]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7549, grad_fn=<NllLossBackward>) tensor([[ 0.8910, -0.1730, -0.0566, -2.3362, -1.0088, -0.2490]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8114, grad_fn=<NllLossBackward>) tensor([[ 0.8753, -0.2060, -0.0545, -2.2519, -0.9873, -0.2700]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8082, grad_fn=<NllLossBackward>) tensor([[ 0.8709, -0.2389, -0.0565, -2.1769, -0.9707, -0.2921]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7983, grad_fn=<NllLossBackward>) tensor([[ 0.8767, -0.2719, -0.0623, -2.1104, -0.9585, -0.3152]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9792, grad_fn=<NllLossBackward>) tensor([[ 0.8920, -0.3047, -0.0711, -2.0517, -0.9505, -0.3390]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7743, grad_fn=<NllLossBackward>) tensor([[ 0.8982, -0.3137, -0.0831, -2.0004, -0.9449, -0.3637]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7603, grad_fn=<NllLossBackward>) tensor([[ 0.9132, -0.3248, -0.0977, -1.9557, -0.9426, -0.3890]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0920, grad_fn=<NllLossBackward>) tensor([[ 0.9362, -0.3377, -0.1146, -1.9170, -0.9434, -0.4145]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7302, grad_fn=<NllLossBackward>) tensor([[ 0.9487, -0.3527, -0.1346, -1.8833, -0.9483, -0.4139]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7140, grad_fn=<NllLossBackward>) tensor([[ 0.9696, -0.3689, -0.1557, -1.8546, -0.9559, -0.4158]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8695, grad_fn=<NllLossBackward>) tensor([[ 0.9972, -0.3863, -0.1785, -1.8305, -0.9650, -0.4203]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1010, grad_fn=<NllLossBackward>) tensor([[ 1.0136, -0.4052, -0.1784, -1.8108, -0.9758, -0.4281]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6777, grad_fn=<NllLossBackward>) tensor([[ 1.0197, -0.4012, -0.1819, -1.7953, -0.9868, -0.4381]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8903, grad_fn=<NllLossBackward>) tensor([[ 1.0342, -0.4002, -0.1881, -1.7832, -0.9993, -0.4496]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6656, grad_fn=<NllLossBackward>) tensor([[ 1.0384, -0.4024, -0.1734, -1.7744, -1.0128, -0.4638]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1158, grad_fn=<NllLossBackward>) tensor([[ 1.0508, -0.4069, -0.1634, -1.7683, -1.0274, -0.4789]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2058, grad_fn=<NllLossBackward>) tensor([[ 1.0531, -0.3902, -0.1581, -1.7651, -1.0418, -0.4955]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0864, grad_fn=<NllLossBackward>) tensor([[ 1.0462, -0.3782, -0.1578, -1.7635, -1.0584, -0.4866]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6722, grad_fn=<NllLossBackward>) tensor([[ 1.0316, -0.3468, -0.1610, -1.7643, -1.0748, -0.4813]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6759, grad_fn=<NllLossBackward>) tensor([[ 1.0270, -0.3213, -0.1672, -1.7669, -1.0917, -0.4790]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6737, grad_fn=<NllLossBackward>) tensor([[ 1.0316, -0.3012, -0.1761, -1.7712, -1.1091, -0.4795]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9966, grad_fn=<NllLossBackward>) tensor([[ 1.0444, -0.2859, -0.1874, -1.7769, -1.1269, -0.4824]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9649, grad_fn=<NllLossBackward>) tensor([[ 1.0471, -0.2521, -0.2014, -1.7842, -1.1439, -0.4881]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9297, grad_fn=<NllLossBackward>) tensor([[ 1.0410, -0.2019, -0.2173, -1.7929, -1.1603, -0.4962]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6833, grad_fn=<NllLossBackward>) tensor([[ 1.0267, -0.1603, -0.2112, -1.8027, -1.1771, -0.5070]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8371, grad_fn=<NllLossBackward>) tensor([[ 1.0224, -0.1262, -0.2089, -1.8133, -1.1941, -0.5192]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.9203, grad_fn=<NllLossBackward>) tensor([[ 1.0101, -0.0763, -0.2104, -1.8248, -1.2104, -0.5332]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7155, grad_fn=<NllLossBackward>) tensor([[ 0.9899, -0.0347, -0.2154, -1.8378, -1.1889, -0.5504]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7238, grad_fn=<NllLossBackward>) tensor([[ 9.8068e-01, -1.0305e-03, -2.2326e-01, -1.8514e+00, -1.1714e+00,\n",
            "         -5.6837e-01]], grad_fn=<AddmmBackward>)\n",
            "tensor(0.7253, grad_fn=<NllLossBackward>) tensor([[ 0.9814,  0.0254, -0.2335, -1.8655, -1.1578, -0.5868]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9575, grad_fn=<NllLossBackward>) tensor([[ 0.9911,  0.0450, -0.2459, -1.8800, -1.1476, -0.6057]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7222, grad_fn=<NllLossBackward>) tensor([[ 0.9916,  0.0582, -0.2363, -1.8948, -1.1407, -0.6258]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.8548, grad_fn=<NllLossBackward>) tensor([[ 1.0008,  0.0659, -0.2310, -1.9099, -1.1364, -0.6461]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9483, grad_fn=<NllLossBackward>) tensor([[ 1.0003,  0.0689, -0.2296, -1.9258, -1.0973, -0.6686]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7253, grad_fn=<NllLossBackward>) tensor([[ 0.9910,  0.0668, -0.2083, -1.9417, -1.0639, -0.6921]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.6566, grad_fn=<NllLossBackward>) tensor([[ 0.9920,  0.0607, -0.1919, -1.9577, -1.0366, -0.7150]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7324, grad_fn=<NllLossBackward>) tensor([[ 0.9845,  0.0738, -0.1810, -1.9733, -1.0130, -0.7383]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7325, grad_fn=<NllLossBackward>) tensor([[ 0.9870,  0.0813, -0.1743, -1.9890, -0.9945, -0.7610]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7263, grad_fn=<NllLossBackward>) tensor([[ 0.9981,  0.0838, -0.1720, -2.0048, -0.9803, -0.7833]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9049, grad_fn=<NllLossBackward>) tensor([[ 1.0176,  0.0818, -0.1729, -2.0205, -0.9706, -0.8049]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7098, grad_fn=<NllLossBackward>) tensor([[ 1.0265,  0.0752, -0.1535, -2.0361, -0.9642, -0.8273]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8825, grad_fn=<NllLossBackward>) tensor([[ 1.0433,  0.0650, -0.1395, -2.0516, -0.9611, -0.8491]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6969, grad_fn=<NllLossBackward>) tensor([[ 1.0500,  0.0512, -0.1064, -2.0668, -0.9610, -0.8712]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6886, grad_fn=<NllLossBackward>) tensor([[ 1.0647,  0.0347, -0.0804, -2.0820, -0.9635, -0.8928]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.6760, grad_fn=<NllLossBackward>) tensor([[ 1.0869,  0.0159, -0.0603, -2.0971, -0.9685, -0.9136]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6681, grad_fn=<NllLossBackward>) tensor([[ 1.0986, -0.0050, -0.0466, -2.1120, -0.9765, -0.9059]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6559, grad_fn=<NllLossBackward>) tensor([[ 1.1177, -0.0276, -0.0379, -2.1269, -0.9863, -0.9002]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6396, grad_fn=<NllLossBackward>) tensor([[ 1.1436, -0.0515, -0.0337, -2.1416, -0.9976, -0.8965]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.6897, grad_fn=<NllLossBackward>) tensor([[ 1.1751, -0.0766, -0.0336, -2.1562, -1.0099, -0.8945]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8396, grad_fn=<NllLossBackward>) tensor([[ 1.1950, -0.1025, -0.0374, -2.1705, -1.0246, -0.8666]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6025, grad_fn=<NllLossBackward>) tensor([[ 1.2031, -0.1298, -0.0210, -2.1843, -1.0399, -0.8443]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.6378, grad_fn=<NllLossBackward>) tensor([[ 1.2185, -0.1575, -0.0097, -2.1980, -1.0558, -0.8257]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5906, grad_fn=<NllLossBackward>) tensor([[ 1.2230, -0.1856, -0.0039, -2.2114, -1.0732, -0.7838]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5833, grad_fn=<NllLossBackward>) tensor([[ 1.2348, -0.2139, -0.0023, -2.2246, -1.0907, -0.7478]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5723, grad_fn=<NllLossBackward>) tensor([[ 1.2538, -0.2420, -0.0039, -2.2375, -1.1086, -0.7169]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.5276, grad_fn=<NllLossBackward>) tensor([[ 1.2786, -0.2701, -0.0090, -2.2502, -1.1264, -0.6908]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5510, grad_fn=<NllLossBackward>) tensor([[ 1.2910, -0.2982, -0.0178, -2.2625, -1.1454, -0.6432]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.4523, grad_fn=<NllLossBackward>) tensor([[ 1.3095, -0.3261, -0.0291, -2.2746, -1.1640, -0.6023]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.0367, grad_fn=<NllLossBackward>) tensor([[ 1.3162, -0.3540, -0.0433, -2.2860, -1.1838, -0.5419]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5385, grad_fn=<NllLossBackward>) tensor([[ 1.3115, -0.3817, -0.0602, -2.2980, -1.1665, -0.4918]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.0030, grad_fn=<NllLossBackward>) tensor([[ 1.3144, -0.4090, -0.0787, -2.3097, -1.1522, -0.4490]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9453, grad_fn=<NllLossBackward>) tensor([[ 1.3061, -0.4361, -0.0996, -2.3220, -1.1051, -0.4149]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5495, grad_fn=<NllLossBackward>) tensor([[ 1.2884, -0.4634, -0.0981, -2.3343, -1.0648, -0.3875]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.8641, grad_fn=<NllLossBackward>) tensor([[ 1.2797, -0.4901, -0.1001, -2.3463, -1.0300, -0.3653]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1755, grad_fn=<NllLossBackward>) tensor([[ 1.2613, -0.5167, -0.1059, -2.3589, -0.9650, -0.3499]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9293, grad_fn=<NllLossBackward>) tensor([[ 1.2349, -0.5430, -0.1149, -2.3711, -0.9098, -0.3133]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0855, grad_fn=<NllLossBackward>) tensor([[ 1.2008, -0.5697, -0.1034, -2.3834, -0.8623, -0.2843]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0227, grad_fn=<NllLossBackward>) tensor([[ 1.1608, -0.5961, -0.0966, -2.3954, -0.8234, -0.2357]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9423, grad_fn=<NllLossBackward>) tensor([[ 1.1151, -0.6225, -0.0949, -2.4072, -0.7920, -0.1704]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6924, grad_fn=<NllLossBackward>) tensor([[ 1.0649, -0.6488, -0.0975, -2.4187, -0.7679, -0.0903]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7202, grad_fn=<NllLossBackward>) tensor([[ 1.0287, -0.6744, -0.1032, -2.4298, -0.7487, -0.0217]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.4445, grad_fn=<NllLossBackward>) tensor([[ 1.0051, -0.6993, -0.1118, -2.4406, -0.7341,  0.0361]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7633, grad_fn=<NllLossBackward>) tensor([[ 0.9758, -0.7000, -0.1233, -2.4506, -0.7238,  0.0834]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7778, grad_fn=<NllLossBackward>) tensor([[ 0.9590, -0.7023, -0.1370, -2.4604, -0.7175,  0.1216]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7842, grad_fn=<NllLossBackward>) tensor([[ 0.9535, -0.7062, -0.1526, -2.4700, -0.7147,  0.1514]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.4561, grad_fn=<NllLossBackward>) tensor([[ 0.9582, -0.7115, -0.1700, -2.4794, -0.7150,  0.1734]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.4597, grad_fn=<NllLossBackward>) tensor([[ 0.9541, -0.7182, -0.1893, -2.4901, -0.6806,  0.1866]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7959, grad_fn=<NllLossBackward>) tensor([[ 0.9420, -0.7028, -0.2108, -2.5000, -0.6523,  0.1926]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.5443, grad_fn=<NllLossBackward>) tensor([[ 0.9410, -0.6906, -0.2330, -2.5097, -0.6301,  0.1932]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8046, grad_fn=<NllLossBackward>) tensor([[ 0.9321, -0.6822, -0.2569, -2.5191, -0.6144,  0.2135]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8046, grad_fn=<NllLossBackward>) tensor([[ 0.9339, -0.6764, -0.2814, -2.5283, -0.6036,  0.2268]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.4159, grad_fn=<NllLossBackward>) tensor([[ 0.9457, -0.6730, -0.3061, -2.5374, -0.5973,  0.2338]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(4.2892, grad_fn=<NllLossBackward>) tensor([[ 0.9485, -0.6485, -0.3312, -2.5458, -0.5950,  0.2343]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7963, grad_fn=<NllLossBackward>) tensor([[ 0.9398, -0.6306, -0.3603, -2.4926, -0.5996,  0.2266]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3487, grad_fn=<NllLossBackward>) tensor([[ 0.9419, -0.6164, -0.3889, -2.4455, -0.6072,  0.2147]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3081, grad_fn=<NllLossBackward>) tensor([[ 0.9354, -0.5829, -0.4180, -2.4035, -0.6168,  0.1980]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1628, grad_fn=<NllLossBackward>) tensor([[ 0.9219, -0.5320, -0.4467, -2.3664, -0.6284,  0.1775]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.5519, grad_fn=<NllLossBackward>) tensor([[ 0.9024, -0.4887, -0.4508, -2.3344, -0.6426,  0.1537]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8193, grad_fn=<NllLossBackward>) tensor([[ 0.8768, -0.4528, -0.4579, -2.3064, -0.6598,  0.1522]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.5443, grad_fn=<NllLossBackward>) tensor([[ 0.8637, -0.4230, -0.4666, -2.2823, -0.6786,  0.1459]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0842, grad_fn=<NllLossBackward>) tensor([[ 0.8442, -0.3994, -0.4778, -2.2613, -0.7000,  0.1600]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8587, grad_fn=<NllLossBackward>) tensor([[ 0.8192, -0.3581, -0.4907, -2.2435, -0.7214,  0.1673]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8672, grad_fn=<NllLossBackward>) tensor([[ 0.8073, -0.3236, -0.5043, -2.2287, -0.7442,  0.1689]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9694, grad_fn=<NllLossBackward>) tensor([[ 0.8067, -0.2956, -0.5189, -2.2166, -0.7677,  0.1651]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8724, grad_fn=<NllLossBackward>) tensor([[ 0.7992, -0.2507, -0.5345, -2.2070, -0.7908,  0.1563]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2230, grad_fn=<NllLossBackward>) tensor([[ 0.8025, -0.2135, -0.5509, -2.1997, -0.8146,  0.1432]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2142, grad_fn=<NllLossBackward>) tensor([[ 0.7986, -0.1835, -0.5441, -2.1950, -0.8390,  0.1257]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8778, grad_fn=<NllLossBackward>) tensor([[ 0.7883, -0.1602, -0.5166, -2.1927, -0.8640,  0.1047]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8757, grad_fn=<NllLossBackward>) tensor([[ 0.7894, -0.1427, -0.4941, -2.1919, -0.8893,  0.0807]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "Epoch  2\n",
            "tensor(2.1426, grad_fn=<NllLossBackward>) tensor([[ 0.8007, -0.1304, -0.4760, -2.1924, -0.9149,  0.0544]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.6402, grad_fn=<NllLossBackward>) tensor([[ 0.8036, -0.1234, -0.4392, -2.1947, -0.9405,  0.0256]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8659, grad_fn=<NllLossBackward>) tensor([[ 0.7988, -0.1212, -0.4094, -2.1975, -0.9675,  0.0198]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8616, grad_fn=<NllLossBackward>) tensor([[ 0.8042, -0.1230, -0.3854, -2.2013, -0.9941,  0.0099]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8501, grad_fn=<NllLossBackward>) tensor([[ 0.8195, -0.1281, -0.3662, -2.2059, -1.0206, -0.0033]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8115, grad_fn=<NllLossBackward>) tensor([[ 0.8428, -0.1363, -0.3520, -2.2113, -1.0463, -0.0198]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7182, grad_fn=<NllLossBackward>) tensor([[ 0.8568, -0.1245, -0.3419, -2.2175, -1.0710, -0.0387]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.7788, grad_fn=<NllLossBackward>) tensor([[ 0.8614, -0.1180, -0.3368, -2.2240, -1.0968, -0.0357]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0164, grad_fn=<NllLossBackward>) tensor([[ 0.8581, -0.1154, -0.3351, -2.2318, -1.0858, -0.0385]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8307, grad_fn=<NllLossBackward>) tensor([[ 0.8477, -0.1170, -0.3135, -2.2406, -1.0779, -0.0457]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9751, grad_fn=<NllLossBackward>) tensor([[ 0.8477, -0.1221, -0.2971, -2.2498, -1.0726, -0.0565]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8067, grad_fn=<NllLossBackward>) tensor([[ 0.8408, -0.1306, -0.2620, -2.2595, -1.0703, -0.0705]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8459, grad_fn=<NllLossBackward>) tensor([[ 0.8274, -0.1191, -0.2338, -2.2697, -1.0694, -0.0873]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8848, grad_fn=<NllLossBackward>) tensor([[ 0.8250, -0.1123, -0.2116, -2.2799, -1.0707, -0.1065]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7997, grad_fn=<NllLossBackward>) tensor([[ 0.8157, -0.1102, -0.1720, -2.2907, -1.0741, -0.1280]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8694, grad_fn=<NllLossBackward>) tensor([[ 0.7998, -0.1125, -0.1405, -2.3012, -1.0808, -0.1268]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7851, grad_fn=<NllLossBackward>) tensor([[ 0.7955, -0.1182, -0.1157, -2.3117, -1.0890, -0.1296]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8841, grad_fn=<NllLossBackward>) tensor([[ 0.7843, -0.1273, -0.0744, -2.3227, -1.0983, -0.1364]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8854, grad_fn=<NllLossBackward>) tensor([[ 0.7848, -0.1391, -0.0407, -2.3336, -1.1091, -0.1461]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8789, grad_fn=<NllLossBackward>) tensor([[ 0.7951, -0.1532, -0.0145, -2.3444, -1.1207, -0.1586]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8492, grad_fn=<NllLossBackward>) tensor([[ 0.8143, -0.1695,  0.0050, -2.3551, -1.1332, -0.1737]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8590, grad_fn=<NllLossBackward>) tensor([[ 0.8250, -0.1647,  0.0185, -2.3658, -1.1459, -0.1907]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8458, grad_fn=<NllLossBackward>) tensor([[ 0.8441, -0.1638,  0.0262, -2.3766, -1.1590, -0.2098]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8265, grad_fn=<NllLossBackward>) tensor([[ 0.8715, -0.1663,  0.0293, -2.3870, -1.1730, -0.2301]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9596, grad_fn=<NllLossBackward>) tensor([[ 0.9056, -0.1719,  0.0278, -2.3973, -1.1874, -0.2518]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9650, grad_fn=<NllLossBackward>) tensor([[ 0.9285, -0.1807,  0.0217, -2.4075, -1.2038, -0.2501]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7076, grad_fn=<NllLossBackward>) tensor([[ 0.9411, -0.1924,  0.0115, -2.4175, -1.2218, -0.2276]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9283, grad_fn=<NllLossBackward>) tensor([[ 0.9447, -0.2067,  0.0212, -2.4278, -1.2399, -0.2111]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7818, grad_fn=<NllLossBackward>) tensor([[ 0.9405, -0.1999,  0.0259, -2.4381, -1.2575, -0.1996]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7793, grad_fn=<NllLossBackward>) tensor([[ 0.9457, -0.1969,  0.0259, -2.4483, -1.2746, -0.1926]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7704, grad_fn=<NllLossBackward>) tensor([[ 0.9600, -0.1973,  0.0222, -2.4581, -1.2919, -0.1895]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7558, grad_fn=<NllLossBackward>) tensor([[ 0.9819, -0.2007,  0.0147, -2.4680, -1.3087, -0.1901]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.0722, grad_fn=<NllLossBackward>) tensor([[ 1.0105, -0.2068,  0.0041, -2.4776, -1.3252, -0.1941]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7233, grad_fn=<NllLossBackward>) tensor([[ 1.0283, -0.2152, -0.0095, -2.4879, -1.3060, -0.2024]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9842, grad_fn=<NllLossBackward>) tensor([[ 1.0527, -0.2258, -0.0257, -2.4980, -1.2897, -0.2135]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9890, grad_fn=<NllLossBackward>) tensor([[ 1.0668, -0.2155, -0.0441, -2.5079, -1.2763, -0.2265]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9732, grad_fn=<NllLossBackward>) tensor([[ 1.0705, -0.2100, -0.0652, -2.5178, -1.2671, -0.2175]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(4.2891, grad_fn=<NllLossBackward>) tensor([[ 1.0659, -0.1854, -0.0876, -2.5274, -1.2603, -0.2124]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9664, grad_fn=<NllLossBackward>) tensor([[ 1.0511, -0.1687, -0.1138, -2.4762, -1.2578, -0.2130]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7144, grad_fn=<NllLossBackward>) tensor([[ 1.0293, -0.1573, -0.1414, -2.4307, -1.2589, -0.1925]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.9991, grad_fn=<NllLossBackward>) tensor([[ 1.0182, -0.1504, -0.1696, -2.3905, -1.2614, -0.1775]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7286, grad_fn=<NllLossBackward>) tensor([[ 1.0000, -0.1472, -0.1985, -2.3559, -1.2301, -0.1689]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9511, grad_fn=<NllLossBackward>) tensor([[ 0.9924, -0.1476, -0.2278, -2.3257, -1.2033, -0.1646]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7388, grad_fn=<NllLossBackward>) tensor([[ 0.9774, -0.1518, -0.2342, -2.2998, -1.1812, -0.1646]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7398, grad_fn=<NllLossBackward>) tensor([[ 0.9727, -0.1588, -0.2431, -2.2776, -1.1628, -0.1681]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7345, grad_fn=<NllLossBackward>) tensor([[ 0.9775, -0.1684, -0.2541, -2.2585, -1.1481, -0.1747]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7237, grad_fn=<NllLossBackward>) tensor([[ 0.9905, -0.1803, -0.2671, -2.2424, -1.1363, -0.1843]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7081, grad_fn=<NllLossBackward>) tensor([[ 1.0112, -0.1941, -0.2816, -2.2288, -1.1279, -0.1961]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6885, grad_fn=<NllLossBackward>) tensor([[ 1.0384, -0.2097, -0.2977, -2.2177, -1.1220, -0.2101]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9638, grad_fn=<NllLossBackward>) tensor([[ 1.0713, -0.2268, -0.3150, -2.2088, -1.1184, -0.2260]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6515, grad_fn=<NllLossBackward>) tensor([[ 1.0923, -0.2226, -0.3337, -2.2023, -1.1162, -0.2439]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6338, grad_fn=<NllLossBackward>) tensor([[ 1.1197, -0.2217, -0.3531, -2.1974, -1.1164, -0.2629]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6134, grad_fn=<NllLossBackward>) tensor([[ 1.1521, -0.2239, -0.3733, -2.1942, -1.1182, -0.2831]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1740, grad_fn=<NllLossBackward>) tensor([[ 1.1891, -0.2288, -0.3941, -2.1924, -1.1215, -0.3042]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1159, grad_fn=<NllLossBackward>) tensor([[ 1.2129, -0.2366, -0.3923, -2.1923, -1.1265, -0.3266]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5695, grad_fn=<NllLossBackward>) tensor([[ 1.2249, -0.2469, -0.3934, -2.1928, -1.1344, -0.3251]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5590, grad_fn=<NllLossBackward>) tensor([[ 1.2433, -0.2589, -0.3966, -2.1941, -1.1434, -0.3261]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5457, grad_fn=<NllLossBackward>) tensor([[ 1.2670, -0.2725, -0.4020, -2.1965, -1.1529, -0.3298]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2346, grad_fn=<NllLossBackward>) tensor([[ 1.2953, -0.2874, -0.4094, -2.1999, -1.1628, -0.3360]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2276, grad_fn=<NllLossBackward>) tensor([[ 1.3110, -0.3039, -0.3951, -2.2041, -1.1741, -0.3444]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5195, grad_fn=<NllLossBackward>) tensor([[ 1.3151, -0.3218, -0.3618, -2.2091, -1.1861, -0.3550]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5142, grad_fn=<NllLossBackward>) tensor([[ 1.3255, -0.3405, -0.3343, -2.2147, -1.1984, -0.3671]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5060, grad_fn=<NllLossBackward>) tensor([[ 1.3418, -0.3597, -0.3120, -2.2208, -1.2109, -0.3805]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.4952, grad_fn=<NllLossBackward>) tensor([[ 1.3633, -0.3793, -0.2942, -2.2272, -1.2236, -0.3948]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.4823, grad_fn=<NllLossBackward>) tensor([[ 1.3894, -0.3993, -0.2807, -2.2341, -1.2365, -0.4101]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3066, grad_fn=<NllLossBackward>) tensor([[ 1.4192, -0.4196, -0.2713, -2.2414, -1.2492, -0.4263]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1614, grad_fn=<NllLossBackward>) tensor([[ 1.4357, -0.4170, -0.2654, -2.2490, -1.2620, -0.4432]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3603, grad_fn=<NllLossBackward>) tensor([[ 1.4398, -0.4174, -0.2399, -2.2570, -1.2753, -0.4612]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1166, grad_fn=<NllLossBackward>) tensor([[ 1.4326, -0.4203, -0.2202, -2.2647, -1.2898, -0.4554]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.1945, grad_fn=<NllLossBackward>) tensor([[ 1.4159, -0.4255, -0.1821, -2.2725, -1.3048, -0.4526]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.4876, grad_fn=<NllLossBackward>) tensor([[ 1.3898, -0.4324, -0.1514, -2.2815, -1.2841, -0.4542]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.4969, grad_fn=<NllLossBackward>) tensor([[ 1.3729, -0.4406, -0.1265, -2.2905, -1.2667, -0.4578]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3294, grad_fn=<NllLossBackward>) tensor([[ 1.3645, -0.4502, -0.1070, -2.2996, -1.2524, -0.4630]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3044, grad_fn=<NllLossBackward>) tensor([[ 1.3468, -0.4612, -0.0929, -2.3083, -1.2422, -0.4457]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3209, grad_fn=<NllLossBackward>) tensor([[ 1.3207, -0.4736, -0.0839, -2.3168, -1.2356, -0.4085]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2104, grad_fn=<NllLossBackward>) tensor([[ 1.2875, -0.4640, -0.0793, -2.3257, -1.2309, -0.3776]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.5683, grad_fn=<NllLossBackward>) tensor([[ 1.2482, -0.4579, -0.0788, -2.3342, -1.2299, -0.3282]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.2600, grad_fn=<NllLossBackward>) tensor([[ 1.2200, -0.4546, -0.0816, -2.3429, -1.2302, -0.2864]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8800, grad_fn=<NllLossBackward>) tensor([[ 1.1853, -0.4313, -0.0880, -2.3520, -1.2317, -0.2519]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6328, grad_fn=<NllLossBackward>) tensor([[ 1.1449, -0.4131, -0.0744, -2.3615, -1.2348, -0.2243]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.0073, grad_fn=<NllLossBackward>) tensor([[ 1.1164, -0.3991, -0.0656, -2.3709, -1.2390, -0.2025]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(4.1373, grad_fn=<NllLossBackward>) tensor([[ 1.0820, -0.3889, -0.0614, -2.3815, -1.2089, -0.1877]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6979, grad_fn=<NllLossBackward>) tensor([[ 1.0403, -0.3848, -0.0637, -2.3309, -1.1865, -0.1796]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7142, grad_fn=<NllLossBackward>) tensor([[ 1.0115, -0.3837, -0.0691, -2.2861, -1.1682, -0.1753]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7232, grad_fn=<NllLossBackward>) tensor([[ 0.9937, -0.3854, -0.0779, -2.2468, -1.1531, -0.1751]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1015, grad_fn=<NllLossBackward>) tensor([[ 0.9867, -0.3894, -0.0891, -2.2122, -1.1415, -0.1780]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7326, grad_fn=<NllLossBackward>) tensor([[ 0.9725, -0.3729, -0.1031, -2.1826, -1.1323, -0.1843]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0622, grad_fn=<NllLossBackward>) tensor([[ 0.9683, -0.3607, -0.1194, -2.1572, -1.1255, -0.1936]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9015, grad_fn=<NllLossBackward>) tensor([[ 0.9569, -0.3299, -0.1376, -2.1360, -1.1207, -0.2054]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7507, grad_fn=<NllLossBackward>) tensor([[ 0.9387, -0.3054, -0.1581, -2.1175, -1.1201, -0.1952]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8757, grad_fn=<NllLossBackward>) tensor([[ 0.9312, -0.2863, -0.1800, -2.1021, -1.1214, -0.1894]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7652, grad_fn=<NllLossBackward>) tensor([[ 0.9167, -0.2725, -0.2035, -2.0889, -1.1264, -0.1634]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7681, grad_fn=<NllLossBackward>) tensor([[ 0.9125, -0.2632, -0.2282, -2.0784, -1.1326, -0.1437]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9356, grad_fn=<NllLossBackward>) tensor([[ 0.9174, -0.2578, -0.2537, -2.0703, -1.1399, -0.1297]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.8298, grad_fn=<NllLossBackward>) tensor([[ 0.9143, -0.2564, -0.2567, -2.0647, -1.1485, -0.1212]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7735, grad_fn=<NllLossBackward>) tensor([[ 0.9039, -0.2580, -0.2625, -2.0624, -1.1227, -0.1189]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9387, grad_fn=<NllLossBackward>) tensor([[ 0.9034, -0.2625, -0.2711, -2.0617, -1.1010, -0.1207]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9205, grad_fn=<NllLossBackward>) tensor([[ 0.8956, -0.2468, -0.2818, -2.0630, -1.0828, -0.1261]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7884, grad_fn=<NllLossBackward>) tensor([[ 0.8814, -0.2133, -0.2946, -2.0660, -1.0675, -0.1350]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8554, grad_fn=<NllLossBackward>) tensor([[ 0.8778, -0.1863, -0.3090, -2.0701, -1.0560, -0.1466]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "Epoch  3\n",
            "tensor(1.8283, grad_fn=<NllLossBackward>) tensor([[ 0.8672, -0.1430, -0.3252, -2.0756, -1.0468, -0.1611]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.7069, grad_fn=<NllLossBackward>) tensor([[ 0.8504, -0.1080, -0.3432, -2.0812, -1.0424, -0.1534]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8317, grad_fn=<NllLossBackward>) tensor([[ 0.8275, -0.0800, -0.3626, -2.0889, -1.0052, -0.1519]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8408, grad_fn=<NllLossBackward>) tensor([[ 0.8166, -0.0585, -0.3828, -2.0970, -0.9742, -0.1541]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8419, grad_fn=<NllLossBackward>) tensor([[ 0.8160, -0.0431, -0.4040, -2.1057, -0.9483, -0.1600]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8358, grad_fn=<NllLossBackward>) tensor([[ 0.8249, -0.0332, -0.4260, -2.1148, -0.9276, -0.1689]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8232, grad_fn=<NllLossBackward>) tensor([[ 0.8425, -0.0283, -0.4484, -2.1243, -0.9116, -0.1805]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8673, grad_fn=<NllLossBackward>) tensor([[ 0.8673, -0.0279, -0.4715, -2.1342, -0.8994, -0.1948]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7949, grad_fn=<NllLossBackward>) tensor([[ 0.8825, -0.0319, -0.4952, -2.1439, -0.8925, -0.1868]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7791, grad_fn=<NllLossBackward>) tensor([[ 0.9052, -0.0395, -0.5191, -2.1539, -0.8890, -0.1831]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7585, grad_fn=<NllLossBackward>) tensor([[ 0.9349, -0.0501, -0.5428, -2.1640, -0.8887, -0.1831]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7342, grad_fn=<NllLossBackward>) tensor([[ 0.9702, -0.0635, -0.5668, -2.1744, -0.8906, -0.1868]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7967, grad_fn=<NllLossBackward>) tensor([[ 1.0104, -0.0793, -0.5907, -2.1850, -0.8947, -0.1936]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3424, grad_fn=<NllLossBackward>) tensor([[ 1.0390, -0.0745, -0.6144, -2.1959, -0.9003, -0.2031]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8084, grad_fn=<NllLossBackward>) tensor([[ 1.0563, -0.0741, -0.6143, -2.2072, -0.9080, -0.2153]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6744, grad_fn=<NllLossBackward>) tensor([[ 1.0640, -0.0547, -0.6162, -2.2186, -0.9171, -0.2296]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6659, grad_fn=<NllLossBackward>) tensor([[ 1.0789, -0.0409, -0.6202, -2.2301, -0.9274, -0.2457]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6531, grad_fn=<NllLossBackward>) tensor([[ 1.1003, -0.0320, -0.6258, -2.2415, -0.9389, -0.2633]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3972, grad_fn=<NllLossBackward>) tensor([[ 1.1275, -0.0277, -0.6330, -2.2528, -0.9516, -0.2822]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3883, grad_fn=<NllLossBackward>) tensor([[ 1.1431, -0.0279, -0.6182, -2.2644, -0.9654, -0.3026]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.3554, grad_fn=<NllLossBackward>) tensor([[ 1.1483, -0.0320, -0.5838, -2.2761, -0.9803, -0.3241]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1159, grad_fn=<NllLossBackward>) tensor([[ 1.1439, -0.0397, -0.5324, -2.2880, -0.9958, -0.3468]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6325, grad_fn=<NllLossBackward>) tensor([[ 1.1315, -0.0505, -0.4882, -2.2993, -1.0134, -0.3455]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1097, grad_fn=<NllLossBackward>) tensor([[ 1.1280, -0.0637, -0.4509, -2.3106, -1.0311, -0.3471]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6422, grad_fn=<NllLossBackward>) tensor([[ 1.1162, -0.0794, -0.4199, -2.3214, -1.0501, -0.3273]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6442, grad_fn=<NllLossBackward>) tensor([[ 1.1132, -0.0971, -0.3948, -2.3323, -1.0690, -0.3125]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6412, grad_fn=<NllLossBackward>) tensor([[ 1.1185, -0.1163, -0.3745, -2.3429, -1.0883, -0.3016]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9016, grad_fn=<NllLossBackward>) tensor([[ 1.1310, -0.1370, -0.3590, -2.3534, -1.1074, -0.2950]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9030, grad_fn=<NllLossBackward>) tensor([[ 1.1339, -0.1362, -0.3475, -2.3639, -1.1263, -0.2918]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.1064, grad_fn=<NllLossBackward>) tensor([[ 1.1279, -0.1164, -0.3400, -2.3745, -1.1446, -0.2921]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8650, grad_fn=<NllLossBackward>) tensor([[ 1.1138, -0.1023, -0.3133, -2.3853, -1.1629, -0.2958]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6648, grad_fn=<NllLossBackward>) tensor([[ 1.0929, -0.0706, -0.2919, -2.3959, -1.1811, -0.3019]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0317, grad_fn=<NllLossBackward>) tensor([[ 1.0818, -0.0456, -0.2757, -2.4066, -1.1988, -0.3106]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7790, grad_fn=<NllLossBackward>) tensor([[ 1.0636, -0.0270, -0.2412, -2.4173, -1.2168, -0.3216]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7389, grad_fn=<NllLossBackward>) tensor([[ 1.0393,  0.0083, -0.2131, -2.4278, -1.2345, -0.3344]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0911, grad_fn=<NllLossBackward>) tensor([[ 1.0095,  0.0582, -0.1911, -2.4384, -1.2515, -0.3490]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0764, grad_fn=<NllLossBackward>) tensor([[ 0.9749,  0.0989, -0.1748, -2.4487, -1.2698, -0.3407]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8922, grad_fn=<NllLossBackward>) tensor([[ 0.9360,  0.1310, -0.1638, -2.4589, -1.2890, -0.3121]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8273, grad_fn=<NllLossBackward>) tensor([[ 0.8934,  0.1551, -0.1347, -2.4693, -1.3080, -0.2898]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9894, grad_fn=<NllLossBackward>) tensor([[ 0.8643,  0.1722, -0.1121, -2.4795, -1.3267, -0.2728]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.5289, grad_fn=<NllLossBackward>) tensor([[ 0.8312,  0.1827, -0.0955, -2.4895, -1.3468, -0.2364]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.9130, grad_fn=<NllLossBackward>) tensor([[ 0.7946,  0.2095, -0.0844, -2.4996, -1.3657, -0.2072]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7845, grad_fn=<NllLossBackward>) tensor([[ 0.7717,  0.2289, -0.0780, -2.5093, -1.3845, -0.1840]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(4.2237, grad_fn=<NllLossBackward>) tensor([[ 0.7445,  0.2411, -0.0534, -2.5193, -1.4029, -0.1672]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.9854, grad_fn=<NllLossBackward>) tensor([[ 0.7121,  0.2452, -0.0369, -2.4692, -1.4226, -0.1571]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.9995, grad_fn=<NllLossBackward>) tensor([[ 0.6935,  0.2439, -0.0258, -2.4245, -1.4421, -0.1514]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.1510, grad_fn=<NllLossBackward>) tensor([[ 0.6869,  0.2376, -0.0199, -2.3852, -1.4608, -0.1501]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.0094, grad_fn=<NllLossBackward>) tensor([[ 0.6749,  0.2268, -0.0186, -2.3514, -1.4435, -0.1540]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(4.0020, grad_fn=<NllLossBackward>) tensor([[ 0.6744,  0.2121, -0.0214, -2.3218, -1.4293, -0.1613]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.7012, grad_fn=<NllLossBackward>) tensor([[ 0.6664,  0.1917, -0.0300, -2.2388, -1.4207, -0.1729]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.0094, grad_fn=<NllLossBackward>) tensor([[ 0.6529,  0.1679, -0.0193, -2.1654, -1.4144, -0.1877]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8604, grad_fn=<NllLossBackward>) tensor([[ 0.6511,  0.1416, -0.0139, -2.1003, -1.4104, -0.2046]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.5363, grad_fn=<NllLossBackward>) tensor([[ 0.6431,  0.1125, -0.0139, -2.0427, -1.4096, -0.1993]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.0127, grad_fn=<NllLossBackward>) tensor([[ 0.6305,  0.1041, -0.0179, -1.9920, -1.4103, -0.1979]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.6651, grad_fn=<NllLossBackward>) tensor([[ 0.6295,  0.0920, -0.0257, -1.9479, -1.4124, -0.2004]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8425, grad_fn=<NllLossBackward>) tensor([[ 0.6226,  0.0762, -0.0142, -1.9101, -1.4156, -0.2068]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.5744, grad_fn=<NllLossBackward>) tensor([[ 0.6104,  0.0569, -0.0087, -1.8771, -1.4215, -0.1920]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.0340, grad_fn=<NllLossBackward>) tensor([[ 0.5938,  0.0576, -0.0079, -1.8495, -1.4278, -0.1823]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.6374, grad_fn=<NllLossBackward>) tensor([[ 0.5895,  0.0537, -0.0114, -1.8265, -1.4347, -0.1774]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.6203, grad_fn=<NllLossBackward>) tensor([[ 0.5799,  0.0457,  0.0039, -1.8078, -1.4424, -0.1772]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.0572, grad_fn=<NllLossBackward>) tensor([[ 0.5654,  0.0338,  0.0359, -1.7931, -1.4505, -0.1813]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.5623, grad_fn=<NllLossBackward>) tensor([[ 0.5631,  0.0188,  0.0602, -1.7818, -1.4593, -0.1888]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.0674, grad_fn=<NllLossBackward>) tensor([[ 5.5545e-01,  7.7335e-04,  1.0004e-01, -1.7735e+00, -1.4687e+00,\n",
            "         -1.9975e-01]], grad_fn=<AddmmBackward>)\n",
            "tensor(1.4935, grad_fn=<NllLossBackward>) tensor([[ 0.5592, -0.0197,  0.1311, -1.7682, -1.4784, -0.2133]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.0700, grad_fn=<NllLossBackward>) tensor([[ 0.5571, -0.0424,  0.1767, -1.7652, -1.4887, -0.2296]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.0649, grad_fn=<NllLossBackward>) tensor([[ 0.5659, -0.0668,  0.2125, -1.7645, -1.4992, -0.2479]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.0511, grad_fn=<NllLossBackward>) tensor([[ 0.5847, -0.0927,  0.2395, -1.7658, -1.5103, -0.2677]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.0296, grad_fn=<NllLossBackward>) tensor([[ 0.6122, -0.1198,  0.2583, -1.7688, -1.5216, -0.2888]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.0014, grad_fn=<NllLossBackward>) tensor([[ 0.6474, -0.1478,  0.2697, -1.7736, -1.5329, -0.3111]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9914, grad_fn=<NllLossBackward>) tensor([[ 0.6896, -0.1765,  0.2745, -1.7795, -1.5449, -0.3341]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9971, grad_fn=<NllLossBackward>) tensor([[ 0.7215, -0.2062,  0.2727, -1.7863, -1.5582, -0.3333]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.9244, grad_fn=<NllLossBackward>) tensor([[ 0.7439, -0.2366,  0.2651, -1.7939, -1.5730, -0.3113]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9415, grad_fn=<NllLossBackward>) tensor([[ 0.7737, -0.2672,  0.2528, -1.8028, -1.5872, -0.2947]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9616, grad_fn=<NllLossBackward>) tensor([[ 0.7947, -0.2750,  0.2367, -1.8127, -1.6009, -0.2828]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8745, grad_fn=<NllLossBackward>) tensor([[ 0.8066, -0.2856,  0.2163, -1.8233, -1.6156, -0.2515]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8585, grad_fn=<NllLossBackward>) tensor([[ 0.8268, -0.2981,  0.1929, -1.8345, -1.6299, -0.2265]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.3355, grad_fn=<NllLossBackward>) tensor([[ 0.8544, -0.3122,  0.1670, -1.8461, -1.6440, -0.2073]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8882, grad_fn=<NllLossBackward>) tensor([[ 0.8721, -0.3273,  0.1389, -1.8599, -1.6216, -0.1948]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.8131, grad_fn=<NllLossBackward>) tensor([[ 0.8805, -0.3445,  0.1083, -1.8738, -1.6037, -0.1635]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7990, grad_fn=<NllLossBackward>) tensor([[ 0.8974, -0.3625,  0.0764, -1.8874, -1.5892, -0.1386]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0832, grad_fn=<NllLossBackward>) tensor([[ 0.9215, -0.3815,  0.0434, -1.9014, -1.5770, -0.1199]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7685, grad_fn=<NllLossBackward>) tensor([[ 0.9362, -0.3785,  0.0096, -1.9161, -1.5664, -0.1069]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0883, grad_fn=<NllLossBackward>) tensor([[ 0.9581, -0.3784, -0.0247, -1.9308, -1.5579, -0.0988]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.0712, grad_fn=<NllLossBackward>) tensor([[ 0.9706, -0.3584, -0.0593, -1.9462, -1.5506, -0.0954]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7389, grad_fn=<NllLossBackward>) tensor([[ 0.9746, -0.3208, -0.0940, -1.9621, -1.5445, -0.0961]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.2568, grad_fn=<NllLossBackward>) tensor([[ 0.9866, -0.2896, -0.1288, -1.9779, -1.5399, -0.1006]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.8256, grad_fn=<NllLossBackward>) tensor([[ 0.9898, -0.2640, -0.1633, -1.9947, -1.5018, -0.1094]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7294, grad_fn=<NllLossBackward>) tensor([[ 0.9845, -0.2447, -0.1984, -2.0108, -1.4702, -0.0974]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(1.9448, grad_fn=<NllLossBackward>) tensor([[ 0.9881, -0.2303, -0.2332, -2.0266, -1.4428, -0.0905]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7295, grad_fn=<NllLossBackward>) tensor([[ 0.9840, -0.1982, -0.2674, -2.0424, -1.4191, -0.0878]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7267, grad_fn=<NllLossBackward>) tensor([[ 0.9885, -0.1724, -0.3014, -2.0579, -1.3989, -0.0894]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(3.1019, grad_fn=<NllLossBackward>) tensor([[ 1.0011, -0.1524, -0.3347, -2.0729, -1.3824, -0.0944]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7152, grad_fn=<NllLossBackward>) tensor([[ 1.0045, -0.1376, -0.3674, -2.0886, -1.3342, -0.1038]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7069, grad_fn=<NllLossBackward>) tensor([[ 1.0157, -0.1277, -0.3997, -2.1040, -1.2922, -0.1162]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6940, grad_fn=<NllLossBackward>) tensor([[ 1.0339, -0.1221, -0.4315, -2.1188, -1.2560, -0.1310]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6773, grad_fn=<NllLossBackward>) tensor([[ 1.0583, -0.1206, -0.4627, -2.1333, -1.2251, -0.1480]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(2.9443, grad_fn=<NllLossBackward>) tensor([[ 1.0880, -0.1227, -0.4933, -2.1474, -1.1989, -0.1669]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6443, grad_fn=<NllLossBackward>) tensor([[ 1.1063, -0.1280, -0.5233, -2.1620, -1.1431, -0.1884]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6280, grad_fn=<NllLossBackward>) tensor([[ 1.1302, -0.1361, -0.5526, -2.1762, -1.0946, -0.2113]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.6092, grad_fn=<NllLossBackward>) tensor([[ 1.1592, -0.1468, -0.5813, -2.1900, -1.0529, -0.2351]],\n",
            "       grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG-x6L1_w_gR",
        "outputId": "7bd91ecb-ec39-46a3-bb5e-381aab8bbc29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "source": [
        "for step, batch in enumerate(test_loader):\n",
        "\n",
        "    for idx, sentence in enumerate(batch[0].T):\n",
        "        x = ' '.join(list(map(lambda x: id2word[x], sentence.tolist())))\n",
        "        print(x)\n",
        "        inputs = tokenizer.encode(x, return_tensors=\"pt\", add_special_tokens= True).to(device)\n",
        "        labels = torch.argmax(batch[1][idx,1:])\n",
        "\n",
        "        logits = model(inputs)[0]\n",
        "\n",
        "        print(logits[0].shape)\n",
        "\n",
        "        output_layer = torch.nn.Softmax(dim=1)\n",
        "        outputs = output_layer(logits)\n",
        "        output_class = torch.argmax(outputs)\n",
        "\n",
        "        print(output_class, labels)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hey expo tv today i'm reviewing the happening umm this was one of the worst movies i've ever seen and that's not even a lie i've i've stutter seen a lot of really bad movies i walked out of a couple movies and i would have walked out of this one if i hadn't been praying for m night to pull something at the very end umm mark wahlberg was terrible he will never get a job after this hopefully because he was so bad in this movie and m night wanted this to be campy and like a two hour long twilight zone and with even worse actors than they had back in the day umm they were actually really good in twilight zone this is this is stutter that was a insult to the twilight zone this is a terrible movie umm i know people are rooting for m night i know that you know he hasn't made a movie since lady in the water god forbid that movie too he's just on stutter he hit his peak at twenty three years old like this kid he he stutter doesn't know what he's doing any more the sixth sense was a beautiful movie it was a beautiful film i love that film uhh was pretty good uhh signs awesome uhh and then there came the village which was not great but it was still okay there was the twist there we counted on m night for that we were like oh it's the twist and then it came and you're like uhh but you know what that was okay mainly lady in the water which was just an atrocity and the happening was even worse it's people running away from wind yup for twenty four hours and that's it they're just running away from wind because it's carrying well there's actually no real stutter explanation really but they think the trees are trying to poison them because it makes the part of their brain some chemical part in their brain shut off umm so they all kill themselves so i think what m night really wanted to do was make a movie where he could just have people kill themselves in the most creative ways possible because that's all this was and it wasn't even that gory like it could have been so much more it really could have been but mark wahlberg just killed it he killed it it he was so unreal in this movie i mean even in one part an old lady's like you're gonna kill me in my sleep and he's like what no like come on have some emotion like don't sound like me {lg} sound like a man who's like uhh and i like her too but she just did a very poor job in this movie but like i said maybe he meant it to be really campy and really silly and if he did good job but even then don't go see it don't even see it as a satire it's not worth it it's a terrible film terrible m night has got to get back on track we need another signs we need another sixth sense we need another these were all great films these were amazing films he can get back into that i don't know what's wrong with him why is he doing this i'm really pissed off at this movie i was looking forward to this film all summer i was so psyched about it because i figured it was the comeback it's the first r rated movie it's gonna be grotesque it's gonna be twisted it's gonna be awesome no do not waste your money on this film don't even waste renting it on if it comes on cable see it then that's all i can say don't do not stutter see this movie there's nothing more i can say i give it one star because it's the lowest i can go but if i could go lower i would the happening don't see it it will waste your time and it will make you so sad for everyone involved especially m night because i don't know he must have a stroke or something between films because this is just so bad i don't know why he thought this would be a good idea m night get your act together and mark wahlberg uhh go back to marky mark that's what you gotta do but do not go back on film umm and i've got hope for you but don't trust m night again okay the happening do not see this film do not see this film\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(4)\n",
            "the mummy tomb of the dragon emperor i'm gonna just come out right and say it do not go see this movie it fails on every level possible they shouldn't have not even made the movie they probably even shouldn't have made the second one to be honest and uhh there's i can't really say one good thing about this movie umm the casting rachel weisz is not a return a big problem there big problem it really has an effect on the movie you don't love the characters as much cause the original actress that played brendan fraser's wife is not back brendan fraser just fails entirely he's not funny anymore he's getting older and the whole hero guy who tries to be funny with the one liners just just stutter stop just it's terrible the plot it has nothing to do with the first or second mummy anymore which is you know it's a good idea you know maybe get away from that but you don't care you really don't care about the tomb of the dragon emperor this dragon emperor guy you don't care umm the special effects are way too over the top and not only are they over the top but they are fake you can it's it's stutter insulting it is insulting the way they are umm and i'm not trying to be like mean about it like you need to do better but they do need to do better i mean the public is so used to special effects and computer effects now that we expect a lot more and we are being insulted by this movie everything about it the special effects you can tell are fake and they're just trying to put so much action on you and and stutter and special effects that like the movie and the plot just are completely lost and they're just trying to throw that to make up for it so really and it doesn't make up for it at all so i do not recommend seeing this movie it was in a way it was entertaining in a way but really it's not worth your money i i stutter think if you took your kids to go see it i think i think teenage audiences or them might enjoy it but really it's that great a movie i think it's stutter i think it's gonna fail umm on all levels really i don't think it's good at all really i mean just you gotta go see it you gotta know what i'm talking about it's just old the mummy franchise is dead or they need to it or something because brendan fraser's one liners in all this stuff it's just the public's not gonna take it anymore it's really boring it's old it's cheesy it it stutter was old five years ago seriously so just stop it umm i guess the good thing about the movie was it was it was stutter so it was it was stutter so like bad that it was entertaining cause like you kinda lose yourself in it in the fact that it's not real i mean obviously it's not gonna be real you know a mummy coming to life but like it's just so over the top that it's like somewhat entertaining you know what i mean like it's just so unbelievable that you just gotta sit back and kind of enjoy the ride while you go along with it so that's that's stutter one good thing i'd say about it other than that though i do not recommend this movie at all do not go see it really do not waste your money on it or if you're going to go see matinee or something cheaper seriously i just i think you're gonna get bored with it it's just too much too much action and it's it's stutter over the top it really is so don't go see it in my opinion it's not good i'm gonna give it two out of two out of stutter five stars [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(2)\n",
            "good morning hank it's tuesday as you can tell from the gray speckled walls behind me i am in an airport and as you can tell from my cold dead eyes it's in the morning also i forgot my camcorder so i'm filming this on my phone i have a bunch of things to tell you but i also wanna give you some travel tips so i'm gonna intersperse them thing one hank the paper towns movie they moved the release date it is now coming out on july that is the real date it will really come out on that day i think that's very good news because it means that jake and everybody will have time to actually finish the movie also there is going to be a trailer soon how soon i can't tell you but pretty soon ok travel tip in almost every airport in the united states there is a gate that is empty all the time in our case in indianapolis it's gate this is where i come to make because there are no people here if you're at an airport take the time to walk around and find the empty gate because there are plugs and chairs and freedom sadly that travel tip does not apply to people in los angeles new york chicago or atlanta thing two the looking for alaska movie so hank my first book looking for alaska came out years ago and the movie rights have been owned ever since by paramount but suddenly it seems like a looking for alaska movie might actually happen because the team has gotten back together the same screenwriters who wrote the fault in our stars and paper towns scott and mike weber are gonna be writing the script and the producers of both fault in our stars and paper towns have joined in as well so that is very exciting travel tip so this is the fancy airport lounge don't pay to go to the airport lounge it's never worth it you think there's gonna be like milk and honey and beds in there you know what they have i'll tell you exactly what's in there alcohol and cookies that's it that's what you will find in the airport lounge so unless you really love to drink or are cookie monster there is no way that it's worth it if you want to go to a quiet private place in the airport don't go to the lounge go to the chapel almost every airport has a chapel and they are always nice and pleasant and very quiet places hank i noticed most of my travel tips are like how do you get away from other travelers hank the biggest movie news of all is that i have seen the paper towns movie and it is great i'm just so psyched about it and so grateful to everyone who's working on it hank it's one thing to be faithful to a book's plot it's another to be faithful to the themes of a book and i really feel like the paper towns movie is about learning to imagine others i'm just so psyched about it no spoilers but i am psyched final travel tip hank when walking on the moving walkway be very careful not to step on a crack lest you break your mother's back ok hank i have to go board sorry about the video july paper towns movie i'll see you on friday [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(5)\n",
            "hi there i'm alexis and i'm doing a review on my sister's keeper this movie is playing out in theaters and it is very good i would give it a five star rating and umm basically the movie is thirteen it's one hour and forty six minutes umm there are very good actresses and actors in this movie and i was very impressed with the film uhh there's abigail breslin cameron diaz sofia and jason patrick and they all play very very good umm i was just really impressed with this movie and i recommend it to many people one thing that i should warn you about is the movie will probably make you cry throughout the whole film because it's about this little girl who is dying from leukemia and there are just umm the family it umm is just getting torn apart and there's just a lot of the drama going on and umm it's a wonderful storyline really really interesting and umm it will just keep you on the edge of your seat the whole movie it's very very good and i am so impressed with how it turned out there's also a bestselling book called my sister's keeper and that's why they have the movie and umm it's just so good so go watch it i mean i just want everyone to see it because it's just so serious you can see how umm you can watch a girl a little kid going through leukemia and it's just so sad but i mean it just shows you right there and i mean that stuff really gets to you and it was pretty serious so umm i think that many people should see it so that you can see how serious leukemia is and how much it affects the family and all that so yeah it just it opened my eyes to life and like opened my eyes to a different view on life and i know that it would open many views to other peoples and people umm different perspectives on things so umm it's definitely worth watching\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(0)\n",
            "hi so you're a homeowner and you're wondering how to build equity in your property no problem i'll be able to answer that my name is adriel torres and i'm the owner of now in order to build equity in your property you have to have time in the property where you're making your mortgage payment or you have to have money where you can just either pay it off and build instant equity or pay more to build faster equity now those are the traditional ways you have money or you have time you've been in it for a long time now one easy way is if you have any extra money at all whatever that may be you can send a payment by itself it doesn't have to be in the mortgage payment to the lender you have to make sure that that payment is to be used strictly for principal reduction what that means is when the lender receives this payment whether it's two hundred dollars three hundred four whatever the case may be they're going to apply it right to the principal so they're going to be taking it off right off what is owed and not from your regular mortgage payment where there is interest included in that payment so if you have any extra money go ahead and send it in and the faster you send in those payments the faster you're going to be able to build equity in your property again my name is adriel torres and i'm the owner of thank you very much [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(1)\n",
            "and then we're shooting an episode where we want it to be winter during summer because it's stupid that it's only winter during winter we're gonna ride sleds down a summer hill and we're going to have like a snow war everybody's going to bully me for having a stick here is the house where we're filming they're packing everything up yeah it's a hot mess oh wow we're going to go ride we're sledding down this hill and i'm like terrified because it's really steep and there's like a bunch of stones and i feel like i'm going to crack my butt also i'm wearing this like overalls it's like they have a personal grudge against my crotch because it's like so tight so this is the one that has soap water and tarp under it pretty terrifying [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(0)\n",
            "hi this is my movie review of the movie stutter license to wed starring mandy moore umm this is a movie i though was going to be pretty funny because the trailer looked pretty funny and it's got you know robin williams which i personally like and thought you know he's gonna bring some entertainment to this movie but i didn't really like this movie at all it was boring stutter the plot it was like it didn't have a plot it was just they are trying to you know create all these things to make you laugh but i don't think it was funny at all even the robotic baby part was supposed to be funny it didn't turn out to be funny in the film it looked funny in the clip for the trailer but didn't seem funny at all and umm you know mandy moore's character is kind of like you feel like her character's unrealistic because what type of girl would go that hard core to you know to get married in this church and it just seemed unrealistic and she gets all in a fit and it's just really really tiring and stressful movie and it wasn't and i feel like the characters were underdeveloped they're you don't like any of the characters you don't connect with any of the characters and it felt like the you know there's like just a little bit just way too typical of a boring film that tries really hard to be funny but it's just not funny at all and it's not it's not stutter witty it it's stutter not even romantic you know in a way that the way it's written it's like how can someone you know go that hard it's just like at the end when you're done with the movie you don't even believe in the original premise of the movie you don't believe that someone can actually you know believe in the priest and just wanted get married that badly in that church and be you know believe everything in the priest it's just kind of like the movie's stressful i don't know if it's the acting or the way the characters are characterized it's just really boring bland film that even the plot it didn't really have anything going on for that so umm for those who are interested i definitely do not recommend this movie at all i mean i mean stutter if you're really bored and if you're looking for something to do the last resort and if you wanna go to a movie theater pick any movie but this one because it seemed like a very interesting movie but it turns out to be just very bland\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(3)\n",
            "i can't i've got to go home the cat needs the cat doesn't need you to supernova and everybody who wants to go to the website with the wiggly thing where it going crazy it's does the sun ever run out of shine does the ocean ever run out of wet does matt smith ever run out of chin i don't think so no man the way that it works is that you use your kitchen your kitchen has a microwave in it you use your microwave to cook popcorn people eat popcorn and therefore it's a part of them so the internet is popcorn it's ok that one's really complicated pedro del valle theorizes the connection between cats and eternal september which is really interesting so that when the internet first became home to new users they would post pictures of their pets and then the rest is history so it is possible to and everyone else who says that the internet is not cats but is rather porn dented bells and adam weaver have really great responses to all over those comments we'll pause here for a second so you can read them i totally agree [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(0)\n",
            "uh when i became director of or or was nominated to be director of don said to me oughta have a conversation with the and this was what nineteen sixty eight n sixty nine i guess uh we had no relations with china at the time i uh don and i went into the oval office just the three of us the president don and myself and um nixon said uh you're a foreign service officer aren't you and i said mister he said uh where'd you and i said he said going nowhere place you oughta think about is china uh you go over and manage that damn he hated and after two years we'll send you to i walked out of the office saying he talking about we don't have relations with two years later almost to the day kissinger went on tv saying richard nixon was going to china so i'm living witness that nixon came into office uh thinking about uh reestablishing relations with china [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(0)\n",
            "hello expo tv umm today i'm reviewing the movie death race uhh this means two out of five uhh it really wasn't very good uhh it stars jason statham you might recognize him from the crank movies where he was like a getaway driver kind of drug runner anyways umm where do i start umm the movie takes place in two thousand twelve it's about a guy that goes to prison for a crime he didn't commit it's a like for profit prison where they have this pay creation that's these death races and they have convicts racing in like cars with all kinds of crazy stuff like missiles and and machine guns and they kill each other and if you win it's five races you get your freedom so that's kind of the premise of the movie umm honestly that sounds pretty stupid i don't even know why i saw it uhh but at least now you guys are warned don't see death race uhh i wouldn't even rent it it's really pretty bad uhh i mean there's a lot of action if you like senseless action death race is your movie but otherwise death race two out of five\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(1)\n",
            "the name my project is triangle dance marathon triangle dance marathon is a dance marathon that we organize at arena which is where the carolina hurricanes play and state basketball team play and it's a dance marathon that sponsors duke and north carolina children's hospitals all and all we raise a for the hospitals it's last year it was hours of dancing basketball eating photo booths we make cards for the kids and it was a really good time the club that i'm part of that organize that has about currently some students and then last year we had come out to the dance we encourage other people to volunteer by doing little fundraisers and pep rallies for dance marathon like every year have a huge pep rally at our school where we get the whole school in the gym and we give information about the dance marathon it makes me feel really good but it's not just me that feels good it's the kids at hospital and that's what's most important [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(0)\n",
            "disney but from all over the place what we hear in the radio so for those of us in the theatres watching frozen for the first time again we see this build up happening and we were probably subconsciously just expecting let it go but that doesn't happen instead probably somebody really smart a genius at disney they probably suggested let's build up the song so that it makes it seem as if there's going to be this big chorus just like all the other songs we're used to let's prime them for that but then delay it so that it does happen but the second time the chorus comes in and the first time let's do something completely magical something sweet something completely unexpected so with that in mind let's imagine going back to the first time we watched the movie kind of with this perspective in mind let's imagine experiencing these emotions expecting this big chorus but then instead seeing this [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(0)\n",
            "hip hop if there's one thing that storytelling teaches us about life it's that different objectives require different tactics that achieve different results cause and effect one thing always leads to another applying this theme to marketing reveals at least one universal truth at the beginning of any project make sure your entire team especially the final stakeholders and decision makers are all seeing the same objective as the end goal if you have different and conflicting objectives you're gonna look for different tactics to achieve different results and you're setting yourself up for incoherence let me give you a hypothetical let's imagine that we have a client together and their goal is to sell their product online to year old men on great that's a clear objective we lay out a day strategy we start creating daily content on for them sales start to go up and they can see that the conversion is coming from the audience on but then as we get towards the end of the schedule the end of the strategy someone remarks we don't like that the number of subscribers to our channel isn't as high as we'd like it to be or we don't like that there aren't more people coming from to our other social channels as we'd like there to be those notes provide an excellent opportunity to pivot towards new goals but they were never the target objective the target objective was to sell to year old men on but at the end not everyone's measuring success the same way when your team all has the same objective in mind and is measuring success the exact same way that becomes your north star it makes hard choices so much easier it allows you to assess so much easier going forward if you're aiming for year old men and the only people engaging with your content are year old women something's weird we've gotta pivot here this is what storytelling teaches us differing objectives require different tactics that result in different outcomes have your objectives clear up front\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(2)\n",
            "they want to find out they ask people that's fine you should give the correct answer then but in the buildup to an election what's happening is that you're getting called on the phone and being asked to give away your valuable personal information remember the uproar when starting messing with their privacy settings your personal information is valuable the polls collect that bundle it and sell it for massive profits so that politicians can lie to you better the whole reason that you get sort of etch politics that was the quote out of romney's campaign that between the primaries and the main election you just shake it up and start over with the new message politicians can do that because they have faith that the polls are accurate if the polls tell them the right lie then all they have to do is always give the right lie so what you should really start thinking about is making the polls work for you if you want to find out a politician's stance on a position make the polls inaccurate [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(0)\n",
            "hi i'm reviewing the release of uhh python umm if you've seen lake placid or anaconda or any of those this is basically just a a rip off of those and uhh since it's ripping of a couple of pretty poor movies it's probably no surprise that this one's even worse umm the first thing going against it is that there stars casper van who in my opinion has never been good in anything except for maybe starship troopers umm there's no special features on the disc uhh which may be a blessing because once the movies over you wouldn't want to see anymore anyway but uhh this movie is basically about an hour and a half of uhh poorly done special effects a computer generated snake chasing around uhh man and naked girls i know that sounds good but stutter it's not umm basically this movie is not good the is not good and uhh i have nothing good to say about it [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "torch.Size([6])\n",
            "tensor(0) tensor(1)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}